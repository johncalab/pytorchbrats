{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure that conversion went ok\n",
    "imgPath = os.path.join('ignore','data', 'num32train', 'BRATS_001.nii.gz.npy')\n",
    "img = np.load(imgPath)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(20,20))\n",
    "slices = [20,40]\n",
    "num_channels = img.shape[0]\n",
    "k = 1\n",
    "for slice in slices:\n",
    "    for j in range(num_channels):\n",
    "        plt.subplot(num_channels,num_channels,k)\n",
    "        plt.axis('off')\n",
    "        plt.imshow(img[j,:,:,slice])\n",
    "        k+=1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgPath = os.path.join('ignore','data', 'num32labels', 'BRATS_001.nii.gz.npy')\n",
    "img = np.load(imgPath)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.axis('off')\n",
    "plt.imshow(img[:,:,30])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architectures\n",
    "Here is an article with several architectures:\n",
    "https://medium.com/@arthur_ouaknine/review-of-deep-learning-algorithms-for-image-semantic-segmentation-509a600f7b57"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IoU Score\n",
    "# https://cdn-images-1.medium.com/max/1600/1*aXXnB2IEA7DalpGNr3X59g.png\n",
    "def iou_score(y_pred, y, SMOOTH=1e-6, rounding=False):\n",
    "    \"\"\"\n",
    "    aka Jaccard\n",
    "    expect: y_pred, y to be of SAME integer type\n",
    "    \n",
    "    y_pred is output of model\n",
    "        expect: y_pred.shape = (batch_len,D,D,S)\n",
    "        (no channels!)\n",
    "    y is truth value (labels)\n",
    "        expect: y.shape = (batch_len,D,D,S)\n",
    "    \n",
    "    returns: the mean across the batch of the iou scores\n",
    "    \"\"\"\n",
    "    # sanity check\n",
    "    assert y_pred.shape == y.shape\n",
    "    # to compute scores, we sum along all axes except for batch\n",
    "    axes = tuple([i for i in range(1,len(y.shape))])\n",
    "    batch_len = y.shape[0]\n",
    "    # if y_pred hasn't been rounded\n",
    "    if rounding:\n",
    "        y_pred = y_pred.round()\n",
    "    \n",
    "    intersection = (y_pred & y).sum(dim=axes).float()\n",
    "    union = (y_pred | y).sum(dim=axes).float()\n",
    "    # sanity check\n",
    "    assert intersection.shape == union.shape\n",
    "    assert union.shape == (batch_len,)\n",
    "    \n",
    "    iou = (intersection + SMOOTH) / (union + SMOOTH)\n",
    "    \n",
    "    return iou.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulate data\n",
    "def simulator(model,loss,batch_len=1,C=4,D=32,S=64):\n",
    "    # simulate input data\n",
    "    x = torch.randn(batch_len,C,D,D,S)\n",
    "    print(f\"Input has shape {tuple(x.shape)}.\")\n",
    "    # simulate output data\n",
    "    y = torch.randn(batch_len,D,D,S)\n",
    "    y = torch.sigmoid(y).round()\n",
    "    print(f\"Target has shape {tuple(y.shape)}.\")\n",
    "    # simulate prediction for training\n",
    "    y_pred = model(x, evaluating=False)\n",
    "    print(f\"Training output has shape {tuple(y_pred.shape)}.\")\n",
    "    # compute loss\n",
    "    l = loss(y_pred,y).item()\n",
    "    print(f\"Loss is {l}.\")\n",
    "    \n",
    "    # simulate prediction for training\n",
    "    y_pred = model(x, evaluating=True)\n",
    "    print(f\"Evaluation output has shape {tuple(y_pred.shape)}.\")\n",
    "    # convert to int type\n",
    "    # x.byte() is equivalent to x.to(dtype=torch.uint8)\n",
    "    # https://pytorch.org/docs/stable/tensors.html\n",
    "    y_pred = y_pred.byte()\n",
    "    y = y.byte()\n",
    "    score = iou_score(y_pred,y).item()\n",
    "    print(f\"IoU score is {score}.\")\n",
    "\n",
    "    # simulate segmentation comparison\n",
    "    y_pred = y_pred[0].cpu().detach().numpy()\n",
    "    y = y[0].cpu().detach().numpy()\n",
    "    \n",
    "    plt.figure(figsize=(10,10))\n",
    "    \n",
    "    plt.subplot(1,2,1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(y_pred[:,:,35])\n",
    "    \n",
    "    plt.subplot(1,2,2)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(y[:,:,35])\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pixelwise Logistic Regression (bad for your health)\n",
    "class PixLog(nn.Module):\n",
    "    def __init__(self,C=4,D=32,S=64):\n",
    "        super(PixLog,self).__init__()\n",
    "        \n",
    "        self.dimensions = (C,D,D,S)\n",
    "        self.fc = nn.Linear(in_features=C*D*D*S, out_features=D*D*S,bias=True)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x_in, evaluating=False):\n",
    "        batch_len = x_in.shape[0]\n",
    "        x = x_in.view(batch_len,-1)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        if evaluating:\n",
    "            x = self.sig(x)\n",
    "            x = x.round()\n",
    "        \n",
    "        dummy_dim = (-1,) + self.dimensions[1:]\n",
    "        x_out = x.view(dummy_dim)\n",
    "        return x_out\n",
    "\n",
    "# If you even try to initialize this, you're gonna have a bad time.\n",
    "# loss = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a simple encoderdecoder-type model\n",
    "class Crush(nn.Module):\n",
    "    def __init__(self,D=32,S=64,C=4,crush_size=32):\n",
    "        super(Crush,self).__init__()\n",
    "        \n",
    "        self.dimensions = (C,D,D,S)\n",
    "        self.in_features = int(D*D*S*C)\n",
    "        self.crush = crush_size\n",
    "        self.out_features = int(D*D*S)\n",
    "        self.enc = nn.Linear(in_features=self.in_features,out_features=self.crush,bias=True)\n",
    "        self.act = nn.ReLU()\n",
    "        self.dec = nn.Linear(in_features=self.crush,out_features=self.out_features)\n",
    "        self.sig = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x_in, evaluating=False):\n",
    "        batch_len = x_in.shape[0]\n",
    "        x = x_in.view(batch_len,-1)\n",
    "        x = self.enc(x)\n",
    "        x = self.act(x)\n",
    "        x = self.dec(x)\n",
    "        \n",
    "        if evaluating:\n",
    "            x = self.sig(x)\n",
    "            x = x.round()\n",
    "        \n",
    "        dummy_dim = (-1,) + self.dimensions[1:]\n",
    "        x_out = x.view(dummy_dim)\n",
    "        return x_out\n",
    "\n",
    "\n",
    "model = Crush()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "loss = nn.BCEWithLogitsLoss()\n",
    "# should be of the form loss(y_pred,y)\n",
    "simulator(model=model,loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a simple sequence of convolutional layers\n",
    "# neat animation explaining convolutions\n",
    "# https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md\n",
    "class ConvSeq(nn.Module):\n",
    "    def __init__(self,input_channels=4):\n",
    "        super(ConvSeq,self).__init__()\n",
    "        \n",
    "        self.input_channels = input_channels\n",
    "        \n",
    "        self.c1 = self.ConvLayer(in_channels=self.input_channels)\n",
    "        self.c2 = self.ConvLayer()\n",
    "        self.c3 = self.ConvLayer()\n",
    "        self.cfinal = self.ConvLayer(out_channels=1, relu=False)\n",
    "    \n",
    "    def forward(self, x_in, evaluating=False):\n",
    "        x = self.c1(x_in)\n",
    "        x = self.c2(x)\n",
    "        x = self.c3(x)\n",
    "        x_out = self.cfinal(x).squeeze(1)\n",
    "        \n",
    "        if evaluating:\n",
    "            x_out = torch.sigmoid(x_out)\n",
    "            x_out = x_out.round()\n",
    "            \n",
    "        return x_out\n",
    "    \n",
    "    def ConvLayer(self, in_channels=32, out_channels=32, kernel_size=3, stride=1, padding=1, bias=True, relu=True):\n",
    "        layer = nn.Sequential()\n",
    "        conv = nn.Conv3d(in_channels=in_channels,\n",
    "                         out_channels=out_channels,\n",
    "                         kernel_size=kernel_size,\n",
    "                         padding=padding,\n",
    "                         bias=bias)\n",
    "        layer.add_module('conv',conv)\n",
    "        if relu:\n",
    "            layer.add_module('relu',nn.ReLU())\n",
    "\n",
    "        return layer\n",
    "\n",
    "\n",
    "model = ConvSeq()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "loss = nn.BCEWithLogitsLoss()\n",
    "# should be of the form loss(y_pred,y)\n",
    "simulator(model=model,loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input has shape (1, 4, 32, 32, 64).\n",
      "Target has shape (1, 32, 32, 64).\n",
      "Training output has shape (1, 32, 32, 64).\n",
      "Loss is 0.7326396703720093.\n",
      "Evaluation output has shape (1, 32, 32, 64).\n",
      "IoU score is 0.24821293354034424.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# small 3d u-net with concatenating skip connection\n",
    "# original paper https://arxiv.org/abs/1505.04597\n",
    "class Small3dUcat(nn.Module):\n",
    "    def __init__(self,input_channels=4,num_filters=32):\n",
    "        super(Small3dUcat,self).__init__()\n",
    "        \n",
    "        # Structure:\n",
    "        # Conv,Conv,MaxPool,Conv,Conv,UnPool,Conv,Conv\n",
    "        \n",
    "        self.input_channels = input_channels\n",
    "        self.num_filters = num_filters\n",
    "        \n",
    "        self.c1 = self.ConvLayer(in_channels=self.input_channels,out_channels=num_filters)\n",
    "        self.c2 = self.ConvLayer(in_channels=num_filters,out_channels=num_filters)\n",
    "\n",
    "        self.c3 = self.ConvLayer(in_channels=num_filters,out_channels=num_filters)\n",
    "        self.c4 = self.ConvLayer(in_channels=num_filters,out_channels=num_filters)\n",
    "        \n",
    "        self.c5 = self.ConvLayer(in_channels=2*num_filters,out_channels=num_filters)\n",
    "        self.c6 = self.ConvLayer(in_channels=num_filters,out_channels=num_filters)\n",
    "        self.c7 = self.ConvLayer(in_channels=num_filters,out_channels=1)\n",
    "    \n",
    "    def forward(self, x_in, evaluating=False):\n",
    "        x1 = self.c1(x_in)\n",
    "        x1 = self.c2(x1)\n",
    "        \n",
    "        x2 = F.max_pool3d(x1,kernel_size=2)\n",
    "        x2 = self.c3(x2)\n",
    "        x2 = self.c4(x2)\n",
    "        \n",
    "        x2 = F.interpolate(x2, scale_factor=2)\n",
    "        # concatenate x1,x2\n",
    "        x = torch.cat([x1,x2],dim=1)\n",
    "        x = self.c5(x)\n",
    "        x = self.c6(x)\n",
    "        x = self.c7(x)\n",
    "        \n",
    "        x_out = x.squeeze(1)\n",
    "        \n",
    "        if evaluating:\n",
    "            x_out = torch.sigmoid(x_out)\n",
    "            x_out = x_out.round()\n",
    "\n",
    "        return x_out\n",
    "    \n",
    "    def ConvLayer(self, in_channels=32, out_channels=32, kernel_size=3,\n",
    "                  stride=1, padding=1, bias=True, relu=True, batchnorm=True):\n",
    "        layer = nn.Sequential()\n",
    "        conv = nn.Conv3d(in_channels=in_channels,\n",
    "                         out_channels=out_channels,\n",
    "                         kernel_size=kernel_size,\n",
    "                         padding=padding,\n",
    "                         bias=bias)\n",
    "        layer.add_module('conv',conv)\n",
    "        if relu:\n",
    "            layer.add_module('relu',nn.ReLU())\n",
    "        if batchnorm:\n",
    "            layer.add_module('batchnorm',nn.BatchNorm3d(out_channels))\n",
    "\n",
    "        return layer\n",
    "\n",
    "\n",
    "model = Small3dUcat()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "loss = nn.BCEWithLogitsLoss()\n",
    "# should be of the form loss(y_pred,y)\n",
    "simulator(model=model,loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input has shape (1, 4, 32, 32, 64).\n",
      "Target has shape (1, 32, 32, 64).\n",
      "Training output has shape (1, 32, 32, 64).\n",
      "Loss is 0.7621409893035889.\n",
      "Evaluation output has shape (1, 32, 32, 64).\n",
      "IoU score is 0.24466289579868317.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# small 3d u-net with addition skip connection\n",
    "# original paper https://arxiv.org/abs/1505.04597\n",
    "# a simple sequence of convolutional layers\n",
    "class Small3dUadd(nn.Module):\n",
    "    def __init__(self,input_channels=4,num_filters=32):\n",
    "        super(Small3dUadd,self).__init__()\n",
    "        \n",
    "        # Structure:\n",
    "        # Conv,Conv,MaxPool,Conv,Conv,UnPool,Conv,Conv\n",
    "        \n",
    "        self.input_channels = input_channels\n",
    "        self.num_filters = num_filters\n",
    "        \n",
    "        self.c1 = self.ConvLayer(in_channels=self.input_channels,out_channels=num_filters)\n",
    "        self.c2 = self.ConvLayer(in_channels=num_filters,out_channels=num_filters)\n",
    "\n",
    "        self.c3 = self.ConvLayer(in_channels=num_filters,out_channels=num_filters)\n",
    "        self.c4 = self.ConvLayer(in_channels=num_filters,out_channels=num_filters)\n",
    "        \n",
    "        self.c5 = self.ConvLayer(in_channels=num_filters,out_channels=num_filters)\n",
    "        self.c6 = self.ConvLayer(in_channels=num_filters,out_channels=num_filters)\n",
    "        self.c7 = self.ConvLayer(in_channels=num_filters,out_channels=1)\n",
    "    \n",
    "    def forward(self, x_in, evaluating=False):\n",
    "        x1 = self.c1(x_in)\n",
    "        x1 = self.c2(x1)\n",
    "        \n",
    "        x2 = F.max_pool3d(x1,kernel_size=2)\n",
    "        x2 = self.c3(x2)\n",
    "        x2 = self.c4(x2)\n",
    "        \n",
    "        x2 = F.interpolate(x2, scale_factor=2)\n",
    "        # add x1,x2\n",
    "        x = F.relu(x1+x2)\n",
    "        x = self.c5(x)\n",
    "        x = self.c6(x)\n",
    "        x = self.c7(x)\n",
    "        \n",
    "        x_out = x.squeeze(1)\n",
    "        \n",
    "        if evaluating:\n",
    "            x_out = torch.sigmoid(x_out)\n",
    "            x_out = x_out.round()\n",
    "\n",
    "        return x_out\n",
    "    \n",
    "    def ConvLayer(self, in_channels=32, out_channels=32, kernel_size=3,\n",
    "                  stride=1, padding=1, bias=True, relu=True, batchnorm=True):\n",
    "        layer = nn.Sequential()\n",
    "        conv = nn.Conv3d(in_channels=in_channels,\n",
    "                         out_channels=out_channels,\n",
    "                         kernel_size=kernel_size,\n",
    "                         padding=padding,\n",
    "                         bias=bias)\n",
    "        layer.add_module('conv',conv)\n",
    "        if relu:\n",
    "            layer.add_module('relu',nn.ReLU())\n",
    "        if batchnorm:\n",
    "            layer.add_module('batchnorm',nn.BatchNorm3d(out_channels))\n",
    "\n",
    "        return layer\n",
    "\n",
    "\n",
    "model = Small3dUadd()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "loss = nn.BCEWithLogitsLoss()\n",
    "# should be of the form loss(y_pred,y)\n",
    "simulator(model=model,loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paper discussing segmentation in medical imaging\n",
    "# https://arxiv.org/pdf/1701.03056.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1,2,3])\n",
    "y = torch.tensor([1,2,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x+y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x*y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Jaccard loss module\n",
    "# page 7: https://arxiv.org/pdf/1701.03056.pdf \n",
    "# do a big U-net like in that paper\n",
    "# do a 2d version of that\n",
    "# create dataset class\n",
    "# train!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
