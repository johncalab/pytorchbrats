{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure that conversion went ok\n",
    "imgPath = os.path.join('ignore','data', 'num32train', 'BRATS_001.nii.gz.npy')\n",
    "img = np.load(imgPath)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(20,20))\n",
    "slices = [20,40]\n",
    "num_channels = img.shape[0]\n",
    "k = 1\n",
    "for slice in slices:\n",
    "    for j in range(num_channels):\n",
    "        plt.subplot(num_channels,num_channels,k)\n",
    "        plt.axis('off')\n",
    "        plt.imshow(img[j,:,:,slice])\n",
    "        k+=1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgPath = os.path.join('ignore','data', 'num32labels', 'BRATS_001.nii.gz.npy')\n",
    "img = np.load(imgPath)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.axis('off')\n",
    "plt.imshow(img[:,:,30])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architectures\n",
    "Here is an article with several architectures:\n",
    "https://medium.com/@arthur_ouaknine/review-of-deep-learning-algorithms-for-image-semantic-segmentation-509a600f7b57"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IoU Score\n",
    "# https://cdn-images-1.medium.com/max/1600/1*aXXnB2IEA7DalpGNr3X59g.png\n",
    "def iou_score(y_pred, y, SMOOTH=1e-6, rounding=False):\n",
    "    \"\"\"\n",
    "    aka Jaccard\n",
    "    expect: y_pred, y to be of SAME integer type\n",
    "    \n",
    "    y_pred is output of model\n",
    "        expect: y_pred.shape = (batch_len,D,D,S)\n",
    "        (no channels!)\n",
    "    y is truth value (labels)\n",
    "        expect: y.shape = (batch_len,D,D,S)\n",
    "    \n",
    "    returns: the mean across the batch of the iou scores\n",
    "    \"\"\"\n",
    "    # sanity check\n",
    "    assert y_pred.shape == y.shape\n",
    "    # to compute scores, we sum along all axes except for batch\n",
    "    axes = tuple([i for i in range(1,len(y.shape))])\n",
    "    batch_len = y.shape[0]\n",
    "    # if y_pred hasn't been rounded\n",
    "    if rounding:\n",
    "        y_pred = y_pred.round()\n",
    "    \n",
    "    intersection = (y_pred & y).sum(dim=axes).float()\n",
    "    union = (y_pred | y).sum(dim=axes).float()\n",
    "    # sanity check\n",
    "    assert intersection.shape == union.shape\n",
    "    assert union.shape == (batch_len,)\n",
    "    \n",
    "    iou = 1 - (intersection + SMOOTH) / (union + SMOOTH)\n",
    "    \n",
    "    return iou.mean()\n",
    "\n",
    "def iou_loss(y_pred, y, SMOOTH=1e-6):\n",
    "    \"\"\"\n",
    "    essentially returns 1 - iou_score\n",
    "    but takes care of y_pred not being rounded\n",
    "    \"\"\"\n",
    "    assert y_pred.shape == y.shape\n",
    "    axes = tuple([i for i in range(1,len(y.shape))])\n",
    "    batch_len = y.shape[0]\n",
    "    \n",
    "    numerator = y*y_pred\n",
    "    numerator = numerator.sum(dim=axes)\n",
    "    a = y.sum(dim=axes)\n",
    "    b = y.sum(dim=axes)\n",
    "    denominator = a + b - numerator\n",
    "    quotient = (numerator + SMOOTH) / (denominator + SMOOTH)\n",
    "    return quotient.mean()\n",
    "\n",
    "class iou_module(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(iou_module,self).__init__()\n",
    "\n",
    "    def forward(self, y_pred, y):\n",
    "        loss = iou_loss(y_pred, y)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulate data\n",
    "def simulator(model,criterion,batch_len=1,C=4,D=32,S=64):\n",
    "    # simulate input data\n",
    "    x = torch.randn(batch_len,C,D,D,S)\n",
    "    print(f\"Input has shape {tuple(x.shape)}.\")\n",
    "    # simulate output data\n",
    "    y = torch.randn(batch_len,D,D,S)\n",
    "    y = torch.sigmoid(y).round()\n",
    "    print(f\"Target has shape {tuple(y.shape)}.\")\n",
    "    # simulate prediction for training\n",
    "    y_pred = model(x, evaluating=False)\n",
    "    print(f\"Training output has shape {tuple(y_pred.shape)}.\")\n",
    "    # compute loss\n",
    "    loss = criterion(y_pred,y)\n",
    "    print(f\"Loss is {loss.item()}.\")\n",
    "    \n",
    "    loss.backward()\n",
    "    print(\"Backward pass works.\")\n",
    "    \n",
    "    # simulate prediction for evaluating\n",
    "    y_pred = model(x, evaluating=True)\n",
    "    print(f\"Evaluation output has shape {tuple(y_pred.shape)}.\")\n",
    "    # convert to int type\n",
    "    # x.byte() is equivalent to x.to(dtype=torch.uint8)\n",
    "    # https://pytorch.org/docs/stable/tensors.html\n",
    "    y_pred = y_pred.byte()\n",
    "    y = y.byte()\n",
    "    score = iou_score(y_pred,y).item()\n",
    "    print(f\"IoU score is {score}.\")\n",
    "\n",
    "    # simulate segmentation comparison\n",
    "    y_pred = y_pred[0].cpu().detach().numpy()\n",
    "    y = y[0].cpu().detach().numpy()\n",
    "    \n",
    "    plt.figure(figsize=(10,10))\n",
    "    \n",
    "    plt.subplot(1,2,1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(y_pred[:,:,35])\n",
    "    \n",
    "    plt.subplot(1,2,2)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(y[:,:,35])\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pixelwise Logistic Regression (bad for your health)\n",
    "class PixLog(nn.Module):\n",
    "    def __init__(self,C=4,D=32,S=64):\n",
    "        super(PixLog,self).__init__()\n",
    "        \n",
    "        self.dimensions = (C,D,D,S)\n",
    "        self.fc = nn.Linear(in_features=C*D*D*S, out_features=D*D*S,bias=True)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x_in, evaluating=False):\n",
    "        batch_len = x_in.shape[0]\n",
    "        x = x_in.view(batch_len,-1)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        if evaluating:\n",
    "            x = self.sig(x)\n",
    "            x = x.round()\n",
    "        \n",
    "        dummy_dim = (-1,) + self.dimensions[1:]\n",
    "        x_out = x.view(dummy_dim)\n",
    "        return x_out\n",
    "\n",
    "# If you even try to initialize this, you're gonna have a bad time.\n",
    "# loss = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input has shape (1, 4, 32, 32, 64).\n",
      "Target has shape (1, 32, 32, 64).\n",
      "Training output has shape (1, 32, 32, 64).\n",
      "Loss is 0.6976760625839233.\n",
      "Backward pass works.\n",
      "Evaluation output has shape (1, 32, 32, 64).\n",
      "IoU score is 0.6700641512870789.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input has shape (1, 4, 32, 32, 64).\n",
      "Target has shape (1, 32, 32, 64).\n",
      "Training output has shape (1, 32, 32, 64).\n",
      "Loss is -0.0005064363940618932.\n",
      "Backward pass works.\n",
      "Evaluation output has shape (1, 32, 32, 64).\n",
      "IoU score is 0.6699619889259338.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# a simple encoderdecoder-type model\n",
    "class Crush(nn.Module):\n",
    "    def __init__(self,D=32,S=64,C=4,crush_size=32):\n",
    "        super(Crush,self).__init__()\n",
    "        \n",
    "        self.dimensions = (C,D,D,S)\n",
    "        self.in_features = int(D*D*S*C)\n",
    "        self.crush = crush_size\n",
    "        self.out_features = int(D*D*S)\n",
    "        self.enc = nn.Linear(in_features=self.in_features,out_features=self.crush,bias=True)\n",
    "        self.act = nn.ReLU()\n",
    "        self.dec = nn.Linear(in_features=self.crush,out_features=self.out_features)\n",
    "        self.sig = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x_in, evaluating=False):\n",
    "        batch_len = x_in.shape[0]\n",
    "        x = x_in.view(batch_len,-1)\n",
    "        x = self.enc(x)\n",
    "        x = self.act(x)\n",
    "        x = self.dec(x)\n",
    "        \n",
    "        if evaluating:\n",
    "            x = self.sig(x)\n",
    "            x = x.round()\n",
    "        \n",
    "        dummy_dim = (-1,) + self.dimensions[1:]\n",
    "        x_out = x.view(dummy_dim)\n",
    "        return x_out\n",
    "\n",
    "\n",
    "model = Crush()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "# loss = nn.BCEWithLogitsLoss()\n",
    "# should be of the form loss(y_pred,y)\n",
    "simulator(model=model,criterion=nn.BCEWithLogitsLoss())\n",
    "simulator(model=model,criterion=iou_module())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input has shape (1, 4, 32, 32, 64).\n",
      "Target has shape (1, 32, 32, 64).\n",
      "Training output has shape (1, 32, 32, 64).\n",
      "Loss is 0.008921965025365353.\n",
      "Backward pass works.\n",
      "Evaluation output has shape (1, 32, 32, 64).\n",
      "IoU score is 0.5597707033157349.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# a simple sequence of convolutional layers\n",
    "# neat animation explaining convolutions\n",
    "# https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md\n",
    "class ConvSeq(nn.Module):\n",
    "    def __init__(self,input_channels=4):\n",
    "        super(ConvSeq,self).__init__()\n",
    "        \n",
    "        self.input_channels = input_channels\n",
    "        \n",
    "        self.c1 = self.ConvLayer(in_channels=self.input_channels)\n",
    "        self.c2 = self.ConvLayer()\n",
    "        self.c3 = self.ConvLayer()\n",
    "        self.cfinal = self.ConvLayer(out_channels=1, relu=False)\n",
    "    \n",
    "    def forward(self, x_in, evaluating=False):\n",
    "        x = self.c1(x_in)\n",
    "        x = self.c2(x)\n",
    "        x = self.c3(x)\n",
    "        x_out = self.cfinal(x).squeeze(1)\n",
    "        \n",
    "        if evaluating:\n",
    "            x_out = torch.sigmoid(x_out)\n",
    "            x_out = x_out.round()\n",
    "            \n",
    "        return x_out\n",
    "    \n",
    "    def ConvLayer(self, in_channels=32, out_channels=32, kernel_size=3, stride=1, padding=1, bias=True, relu=True):\n",
    "        layer = nn.Sequential()\n",
    "        conv = nn.Conv3d(in_channels=in_channels,\n",
    "                         out_channels=out_channels,\n",
    "                         kernel_size=kernel_size,\n",
    "                         padding=padding,\n",
    "                         bias=bias)\n",
    "        layer.add_module('conv',conv)\n",
    "        if relu:\n",
    "            layer.add_module('relu',nn.ReLU())\n",
    "\n",
    "        return layer\n",
    "\n",
    "\n",
    "model = ConvSeq()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "# loss = nn.BCEWithLogitsLoss()\n",
    "# should be of the form loss(y_pred,y)\n",
    "criterion = iou_module()\n",
    "simulator(model=model,criterion=criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input has shape (1, 4, 32, 32, 64).\n",
      "Target has shape (1, 32, 32, 64).\n",
      "Training output has shape (1, 32, 32, 64).\n",
      "Loss is 0.7571125626564026.\n",
      "Backward pass works.\n",
      "Evaluation output has shape (1, 32, 32, 64).\n",
      "IoU score is 0.7498338222503662.\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjwAAAEMCAYAAADAnWyqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAALd0lEQVR4nO3dQW7bShYFUNvwIoKeZ96bMHoFXmVW0NAmep75R1Yh9fBPLFbyy8/v8eqcYQSSpSqqfEGAN8+32+0JACDZS/cAAACqCTwAQDyBBwCIJ/AAAPEEHgAgnsADAMR7Pfrw7eX98J31//71v62L/+df/946fmV3fJVW33019urju515/NX3XfV3v1x/PJde4Atdf31v7d3Y/Z1Ovvb0PWzy/l+t++9D9/lfvv38cA/zhAcAiCfwAADxBB4AIJ7AAwDEE3gAgHgCDwAQT+ABAOI93273ayp2OyzO3LNT3RNQff3pztyzU617bvTw/K27y6bS9L6o6T1B1aaPb0d1x5IeHgDgYQk8AEA8gQcAiCfwAADxBB4AIJ7AAwDEE3gAgHiHPTxvL++HHRbdPQ5n7sCodvYOh8njrx5bd//IvQ6LM9rt4dlV3QWzc69U76+7un9HK909P4+89np4AADuEHgAgHgCDwAQT+ABAOIJPABAPIEHAIgn8AAA8V6PPux+F39lepfAZNM7jKb3BHUyN4+jsmtlV3fH0Or46eNb2Tn/6tju71799+dy/fjfPeEBAOIJPABAPIEHAIgn8AAA8QQeACCewAMAxBN4AIB4hz081ab3RFRee7ru8Vd3XEy99mecX//UHJ19V9XX7u5iWen+HVc7+n67c1+9Nl33hic8AEA8gQcAiCfwAADxBB4AIJ7AAwDEE3gAgHgCDwAQb6uHZ3rPwo6zdzRU9zBUd8Wcef67+0l25+5y3Tr8VLq7Zs7cudTdRzX978vu2u98/+n37UrVHuYJDwAQT+ABAOIJPABAPIEHAIgn8AAA8QQeACCewAMAxNvq4Tn7u/o7ur/7ru6Oi+4umqPzT187ft/uWlbfp5X3eXcXy9n7qHbPv9t1tqP6vu/2T+fOEx4AIJ7AAwDEE3gAgHgCDwAQT+ABAOIJPABAPIEHAIj3fLvd7n54/fX9/oe/obunobLnYXpPwcr0jozuHqBHdrn+eO4ew2fZ3cNWdu+j6V0zRx59D+j+/jvj797/q718+/nhHuYJDwAQT+ABAOIJPABAPIEHAIgn8AAA8QQeACDe4Wvpby/vpa907pr82t708+/qfq27ev4qTX8V+ZFeS+/+HVcen14tsTJ9fJUm739PT/W/G6+lAwAPS+ABAOIJPABAPIEHAIgn8AAA8QQeACCewAMAxHs9+nB610nl9bv7OapVXz+9B+hIdccEX2dyz850Z99jpuvsmTtrh5MnPABAPIEHAIgn8AAA8QQeACCewAMAxBN4AIB4Ag8AEO+wh6f7XfyVyuvv9hR0d1B09wh1r22n7m6VM8/do+neQyt1dxitdP99q5yf3f15+h6yGv/l+vG/e8IDAMQTeACAeAIPABBP4AEA4gk8AEA8gQcAiCfwAADxnm+3290Pr7++3//wE3T2FPzO8Tum9+Tsmn79Hd3dJt33zsu3n89bFxjk7eW9dA+bvEd1/0ZXuufu7B1IOz081T073Wt3bw/zhAcAiCfwAADxBB4AIJ7AAwDEE3gAgHgCDwAQT+ABAOKV9vB0v4vf3ZPAOXX34Oyq6rA4o+ldYrvnr7x2t8lz+xnXX+ns0jl7T9zl+kMPDwDwmAQeACCewAMAxBN4AIB4Ag8AEE/gAQDiCTwAQLzXnYOrewI6e3q6u1i6O4Smj29l596r/m7T5+6RdHad/M71O3X3oFXP/e74q9eucw+unpvqv+33eMIDAMQTeACAeAIPABBP4AEA4gk8AEA8gQcAiCfwAADxnm+3290Pr7++3//wKbuHoFtnB9HvOPvaHI2/upule+5Xx798+/n8x4Ma6u3l/XAP29Xd07NzH6903+cr1eObvod2do1Nn/vL9ceHe5gnPABAPIEHAIgn8AAA8QQeACCewAMAxBN4AIB4Ag8AEO+wh2e3w+LMPQ7Te2S6TV/blcoOi85+jc+Q1MOz6hJb6e6S6bxXuu/T7uuvTL43pu/P1ePTwwMAPCyBBwCIJ/AAAPEEHgAgnsADAMQTeACAeAIPABDvsIdn1WGx+65+dwfFTo9Bd4/MytnHX+2RO5hW90ZSD89ul9iuzj6U6j2ge/9eSe/LOrPqtdHDAwA8LIEHAIgn8AAA8QQeACCewAMAxBN4AIB4Ag8AEO/16MPqd+V3eyAq+1K6Oyp2dXYY/c75q4/f0b12nd0tabq7Xqp/h5Xfb/pveHX+7p6c7j14sq618YQHAIgn8AAA8QQeACCewAMAxBN4AIB4Ag8AEE/gAQDiHfbwdPeRJEuf28kdS19x/krV/SeX6x8PaazuvqhqnV1kkzuEzmD6vdWp6r72hAcAiCfwAADxBB4AIJ7AAwDEE3gAgHgCDwAQT+ABAOId9vDsvgt/5q6Z3bHvdix0z1332u92UOyMf/rcP3I/x2frnuvOPbDzN/YZun8H3Xvk5Gt3n/8eT3gAgHgCDwAQT+ABAOIJPABAPIEHAIgn8AAA8QQeACDeYQ9PdRfNmZ19brq7ZKqPn+zs986ZdPdpVa9VZRdNd0fRSvfadK89f84THgAgnsADAMQTeACAeAIPABBP4AEA4gk8AEA8gQcAiHfYw9PdIdHZk9DdobDbcdE9/mqV9051/8j0/pBHokvlvt25qT6++j6uPv/knqOz73H3eMIDAMQTeACAeAIPABBP4AEA4gk8AEA8gQcAiCfwAADxnm+3290Pr7++3//wqb+jIrlDY3oPT3dHxa6j8Xf2P01wuf547h7DZ3l7eT/cw1bO3FnU3dfU/Tua3iXT/f0qr929Nvf2ME94AIB4Ag8AEE/gAQDiCTwAQDyBBwCIJ/AAAPEEHgAg3uvRh919IWfvM9nR2cHwGc7eA7Tj7GvH55l8L3Tfh91dXqvrJ//9mXxfPj3Vza0nPABAPIEHAIgn8AAA8QQeACCewAMAxBN4AIB4h6+ld7+W1/lf1E9/rbp7fNVrc2Znn7vLtfT0X6r6dzb51erp3736tfFd3Wu/cjS+9LX5p+f3hAcAiCfwAADxBB4AIJ7AAwDEE3gAgHgCDwAQT+ABAOId9vDsqu4C2O0j2Tm+uiulu6em+/orlV02u2vb3d/R3eMzSXdX2O7xnfdxteq+ql3T569S99pUra0nPABAPIEHAIgn8AAA8QQeACCewAMAxBN4AIB4Ag8AEK+0h2elusdg513+6g6gbtUdSbvX7+wxmt7dsnv+6v6SSarv8+q+kp3jq3+ju7p/Byur7z957aernpvL9eN/94QHAIgn8AAA8QQeACCewAMAxBN4AIB4Ag8AEE/gAQDiHfbwTO7JeXrq7crp7vfYPf+u5PGdvWOJv03veqnU/Rvt7vHZ7XI5+x69I7UDyBMeACCewAMAxBN4AIB4Ag8AEE/gAQDiCTwAQDyBBwCId9jDU91jsNL5rv/0noHJHQ4TVPZITO/fmH7vfqXpa7V7/qPjq++D6V1ku3b//u2ef8fksX2Gfzo+T3gAgHgCDwAQT+ABAOIJPABAPIEHAIgn8AAA8QQeACDeYQ/P2d/lr+5RqDz39P6P3bntXJvq85/9u1+upZf/UtO7XlYmd7Xszm33HtK99pXfb3r/VBdPeACAeAIPABBP4AEA4gk8AEA8gQcAiCfwAADxBB4AIN5hD0/1u/zVPQiV1+/uKejukEi2O7fT12b6+L5Sd2dS5fl3e3BWqvfA7p6ele75O7p+dUfSWXnCAwDEE3gAgHgCDwAQT+ABAOIJPABAPIEHAIgn8AAA8Q57eLo7Jrp7eiqdveeguwNjZef61d9ttwNjdf7dub1ctw4/lel71Ern9c/e09a9R61M7pE769p7wgMAxBN4AIB4Ag8AEE/gAQDiCTwAQDyBBwCIJ/AAAPGeb7db9xgAAEp5wgMAxBN4AIB4Ag8AEE/gAQDiCTwAQDyBBwCI93+Er9m0rlJuZQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# small 3d u-net with concatenating skip connection\n",
    "# original paper https://arxiv.org/abs/1505.04597\n",
    "class Small3dUcat(nn.Module):\n",
    "    def __init__(self,input_channels=4,num_filters=32):\n",
    "        super(Small3dUcat,self).__init__()\n",
    "        \n",
    "        # Structure:\n",
    "        # Conv,Conv,MaxPool,Conv,Conv,UnPool,Conv,Conv\n",
    "        \n",
    "        self.input_channels = input_channels\n",
    "        self.num_filters = num_filters\n",
    "        \n",
    "        self.c1 = self.ConvLayer(in_channels=self.input_channels,out_channels=num_filters)\n",
    "        self.c2 = self.ConvLayer(in_channels=num_filters,out_channels=num_filters)\n",
    "\n",
    "        self.c3 = self.ConvLayer(in_channels=num_filters,out_channels=num_filters)\n",
    "        self.c4 = self.ConvLayer(in_channels=num_filters,out_channels=num_filters)\n",
    "        \n",
    "        self.c5 = self.ConvLayer(in_channels=2*num_filters,out_channels=num_filters)\n",
    "        self.c6 = self.ConvLayer(in_channels=num_filters,out_channels=num_filters)\n",
    "        self.c7 = self.ConvLayer(in_channels=num_filters,out_channels=1)\n",
    "    \n",
    "    def forward(self, x_in, evaluating=False):\n",
    "        x1 = self.c1(x_in)\n",
    "        x1 = self.c2(x1)\n",
    "        \n",
    "        x2 = F.max_pool3d(x1,kernel_size=2)\n",
    "        x2 = self.c3(x2)\n",
    "        x2 = self.c4(x2)\n",
    "        \n",
    "        x2 = F.interpolate(x2, scale_factor=2)\n",
    "        # concatenate x1,x2\n",
    "        x = torch.cat([x1,x2],dim=1)\n",
    "        x = self.c5(x)\n",
    "        x = self.c6(x)\n",
    "        x = self.c7(x)\n",
    "        \n",
    "        x_out = x.squeeze(1)\n",
    "        \n",
    "        if evaluating:\n",
    "            x_out = torch.sigmoid(x_out)\n",
    "            x_out = x_out.round()\n",
    "\n",
    "        return x_out\n",
    "    \n",
    "    def ConvLayer(self, in_channels=32, out_channels=32, kernel_size=3,\n",
    "                  stride=1, padding=1, bias=True, relu=True, batchnorm=True):\n",
    "        layer = nn.Sequential()\n",
    "        conv = nn.Conv3d(in_channels=in_channels,\n",
    "                         out_channels=out_channels,\n",
    "                         kernel_size=kernel_size,\n",
    "                         padding=padding,\n",
    "                         bias=bias)\n",
    "        layer.add_module('conv',conv)\n",
    "        if relu:\n",
    "            layer.add_module('relu',nn.ReLU())\n",
    "        if batchnorm:\n",
    "            layer.add_module('batchnorm',nn.BatchNorm3d(out_channels))\n",
    "\n",
    "        return layer\n",
    "\n",
    "\n",
    "model = Small3dUcat()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "# should be of the form loss(y_pred,y)\n",
    "simulator(model=model,criterion=criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input has shape (1, 4, 32, 32, 64).\n",
      "Target has shape (1, 32, 32, 64).\n",
      "Training output has shape (1, 32, 32, 64).\n",
      "Loss is 0.00022692106722388417.\n",
      "Backward pass works.\n",
      "Evaluation output has shape (1, 32, 32, 64).\n",
      "IoU score is 0.7429132461547852.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# small 3d u-net with addition skip connection\n",
    "# original paper https://arxiv.org/abs/1505.04597\n",
    "class Small3dUadd(nn.Module):\n",
    "    def __init__(self,input_channels=4,num_filters=32):\n",
    "        super(Small3dUadd,self).__init__()\n",
    "        \n",
    "        # Structure:\n",
    "        # Conv,Conv,MaxPool,Conv,Conv,UnPool,Conv,Conv\n",
    "        \n",
    "        self.input_channels = input_channels\n",
    "        self.num_filters = num_filters\n",
    "        \n",
    "        self.c1 = self.ConvLayer(in_channels=self.input_channels,out_channels=num_filters)\n",
    "        self.c2 = self.ConvLayer(in_channels=num_filters,out_channels=num_filters)\n",
    "\n",
    "        self.c3 = self.ConvLayer(in_channels=num_filters,out_channels=num_filters)\n",
    "        self.c4 = self.ConvLayer(in_channels=num_filters,out_channels=num_filters)\n",
    "        \n",
    "        self.c5 = self.ConvLayer(in_channels=num_filters,out_channels=num_filters)\n",
    "        self.c6 = self.ConvLayer(in_channels=num_filters,out_channels=num_filters)\n",
    "        self.c7 = self.ConvLayer(in_channels=num_filters,out_channels=1)\n",
    "    \n",
    "    def forward(self, x_in, evaluating=False):\n",
    "        x1 = self.c1(x_in)\n",
    "        x1 = self.c2(x1)\n",
    "        \n",
    "        x2 = F.max_pool3d(x1,kernel_size=2)\n",
    "        x2 = self.c3(x2)\n",
    "        x2 = self.c4(x2)\n",
    "        \n",
    "        x2 = F.interpolate(x2, scale_factor=2)\n",
    "        # add x1,x2\n",
    "        x = F.relu(x1+x2)\n",
    "        x = self.c5(x)\n",
    "        x = self.c6(x)\n",
    "        x = self.c7(x)\n",
    "        \n",
    "        x_out = x.squeeze(1)\n",
    "        \n",
    "        if evaluating:\n",
    "            x_out = torch.sigmoid(x_out)\n",
    "            x_out = x_out.round()\n",
    "\n",
    "        return x_out\n",
    "    \n",
    "    def ConvLayer(self, in_channels=32, out_channels=32, kernel_size=3,\n",
    "                  stride=1, padding=1, bias=True, relu=True, batchnorm=True):\n",
    "        layer = nn.Sequential()\n",
    "        conv = nn.Conv3d(in_channels=in_channels,\n",
    "                         out_channels=out_channels,\n",
    "                         kernel_size=kernel_size,\n",
    "                         padding=padding,\n",
    "                         bias=bias)\n",
    "        layer.add_module('conv',conv)\n",
    "        if relu:\n",
    "            layer.add_module('relu',nn.ReLU())\n",
    "        if batchnorm:\n",
    "            layer.add_module('batchnorm',nn.BatchNorm3d(out_channels))\n",
    "\n",
    "        return layer\n",
    "\n",
    "\n",
    "model = Small3dUadd()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "# loss = nn.BCEWithLogitsLoss()\n",
    "# should be of the form loss(y_pred,y)\n",
    "criterion = iou_module()\n",
    "simulator(model=model,criterion=criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# page 7: https://arxiv.org/pdf/1701.03056.pdf \n",
    "# do a big U-net similar to the one in the paper\n",
    "# do a 2d version of that\n",
    "# create dataset class\n",
    "# train!\n",
    "# perhaps crop images before downsampling..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input has shape (1, 4, 32, 32, 64).\n",
      "Target has shape (1, 32, 32, 64).\n",
      "Training output has shape (1, 32, 32, 64).\n",
      "Loss is 0.22303548455238342.\n",
      "Backward pass works.\n",
      "Evaluation output has shape (1, 32, 32, 64).\n",
      "IoU score is 0.6268064975738525.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# bigger 3d U-net\n",
    "# similar to https://arxiv.org/pdf/1701.03056.pdf\n",
    "# downsampling is now a convolution\n",
    "# has prelu activations\n",
    "# has more skip connections\n",
    "# upsampling...\n",
    "def yes(x,i):\n",
    "    print(f\"x{i} has shape {x.shape}\")\n",
    "\n",
    "class Big3dU(nn.Module):\n",
    "    def __init__(self,input_channels=4):\n",
    "        super(Big3dU,self).__init__()\n",
    "        \n",
    "        self.c0 = self.ConvLayer(in_channels=4,out_channels=8,\n",
    "                                 kernel_size=3,stride=1,padding=1)\n",
    "\n",
    "        self.d1 = self.ConvLayer(in_channels=8,out_channels=16,\n",
    "                                 stride=2,kernel_size=2,padding=0)\n",
    "        self.c1 = self.ConvLayer(in_channels=16,out_channels=16,\n",
    "                                kernel_size=3,stride=1,padding=1)\n",
    "\n",
    "        self.d2 = self.ConvLayer(in_channels=16,out_channels=32,\n",
    "                                 stride=2,kernel_size=2,padding=0)\n",
    "        self.c2 = self.ConvLayer(in_channels=32,out_channels=32,\n",
    "                                kernel_size=3,stride=1,padding=1)\n",
    "\n",
    "        self.d3 = self.ConvLayer(in_channels=32,out_channels=64,\n",
    "                                 stride=2,kernel_size=2,padding=0)\n",
    "        self.c3 = self.ConvLayer(in_channels=64,out_channels=64,\n",
    "                                kernel_size=3,stride=1,padding=1)\n",
    "        \n",
    "        self.f1 = self.ConvLayer(in_channels=64,out_channels=32,\n",
    "                                kernel_size=1,stride=1,padding=0)\n",
    "        \n",
    "        self.u1 = self.ConvLayer(in_channels=32,out_channels=32,\n",
    "                                kernel_size=2,stride=2,padding=0,transpose=True)\n",
    "        self.c4 = self.ConvLayer(in_channels=64,out_channels=32,\n",
    "                                 kernel_size=3,stride=1,padding=1)\n",
    "        self.f2 = self.ConvLayer(in_channels=32,out_channels=16,\n",
    "                                kernel_size=1,stride=1,padding=0)\n",
    "        \n",
    "        self.u2 = self.ConvLayer(in_channels=16,out_channels=16,\n",
    "                                kernel_size=2,stride=2,padding=0,transpose=True)\n",
    "        self.c5 = self.ConvLayer(in_channels=32,out_channels=16,\n",
    "                                kernel_size=3,stride=1,padding=1)\n",
    "        self.f3 = self.ConvLayer(in_channels=16,out_channels=8,\n",
    "                                kernel_size=1,stride=1,padding=0)\n",
    "        \n",
    "        self.u3 = self.ConvLayer(in_channels=8,out_channels=8,\n",
    "                                kernel_size=2,stride=2,padding=0,transpose=True)\n",
    "        self.c6 = self.ConvLayer(in_channels=16,out_channels=16,\n",
    "                                kernel_size=3,stride=1,padding=1)\n",
    "        \n",
    "        self.f4 = self.ConvLayer(in_channels=32,out_channels=16,\n",
    "                                kernel_size=1,stride=1,padding=0)\n",
    "        self.f5 = self.ConvLayer(in_channels=16,out_channels=8,\n",
    "                                kernel_size=1,stride=1,padding=0)\n",
    "        self.f6 = self.ConvLayer(in_channels=16,out_channels=1,\n",
    "                                kernel_size=1,stride=1,padding=0)\n",
    "        \n",
    "        self.u4 = self.ConvLayer(in_channels=16,out_channels=8,\n",
    "                                kernel_size=2,stride=2,padding=0,transpose=True)\n",
    "        self.u5 = self.ConvLayer(in_channels=8,out_channels=1,\n",
    "                                kernel_size=2,stride=2,padding=0,transpose=True)\n",
    "        \n",
    "        self.act = nn.PReLU(num_parameters=1)\n",
    "\n",
    "        \n",
    "\n",
    "    def forward(self, x_in, evaluating=False):\n",
    "        x0 = self.c0(x_in)\n",
    "        x1 = self.d1(x0)\n",
    "        x2 = self.c1(x1)\n",
    "        x3 = self.d2(x2+x1)\n",
    "        x4 = self.c2(x3)\n",
    "        x5 = self.d3(x4+x3)\n",
    "        x6 = self.c3(x5)\n",
    "        x7 = self.f1(x6+x5)\n",
    "        x8 = self.u1(x7)\n",
    "        x9 = self.c4(torch.cat([x8,x4],dim=1))\n",
    "        x10 = self.f2(x9)\n",
    "        x11 = self.u2(x10)\n",
    "        x12 = self.c5(torch.cat([x11,x2],dim=1))\n",
    "        x13 = self.f3(x12)\n",
    "        x14 = self.u3(x13)\n",
    "        x15 = self.f4(x9)\n",
    "        x16 = self.u4(x15)\n",
    "        x17 = self.f5(x12)\n",
    "        # x18 is missing, the graph I sketched had a box I did not use\n",
    "        x19 = self.u5(x16+x17)\n",
    "        x20 = self.c6(torch.cat([x14,x0],dim=1))\n",
    "        x21 = self.f6(x20)\n",
    "        x22 = self.act(x21+x19)\n",
    "\n",
    "        x_out = x22.squeeze(1)\n",
    "        \n",
    "        if evaluating:\n",
    "            x_out = torch.sigmoid(x_out)\n",
    "            x_out = x_out.round()\n",
    "\n",
    "        return x_out\n",
    "\n",
    "    def ConvLayer(self,in_channels,out_channels,kernel_size,\n",
    "                  stride,padding,dilation=1,bias=True,prelu=True,batchnorm=True,transpose=False):\n",
    "        layer = nn.Sequential()\n",
    "        if transpose:\n",
    "            tconv = nn.ConvTranspose3d(in_channels=in_channels,\n",
    "                             out_channels=out_channels,\n",
    "                             kernel_size=kernel_size,\n",
    "                             stride=stride,\n",
    "                             padding=padding,\n",
    "                             dilation=dilation,\n",
    "                             bias=bias)\n",
    "            layer.add_module('tconv',tconv)\n",
    "        else:\n",
    "            conv = nn.Conv3d(in_channels=in_channels,\n",
    "                             out_channels=out_channels,\n",
    "                             kernel_size=kernel_size,\n",
    "                             stride=stride,\n",
    "                             padding=padding,\n",
    "                             dilation=dilation,\n",
    "                             bias=bias)\n",
    "            layer.add_module('conv',conv)\n",
    "\n",
    "        if batchnorm:\n",
    "            layer.add_module('batchnorm',nn.BatchNorm3d(num_features=out_channels))\n",
    "\n",
    "        if prelu:\n",
    "            layer.add_module('prelu',nn.PReLU(num_parameters=out_channels))\n",
    "\n",
    "        return layer\n",
    "\n",
    "model = Big3dU()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "# loss = nn.BCEWithLogitsLoss()\n",
    "# should be of the form loss(y_pred,y)\n",
    "criterion = iou_module()\n",
    "simulator(model=model,criterion=criterion)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
