{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure that conversion went ok\n",
    "imgPath = os.path.join('ignore','data', 'num32train', 'BRATS_001.nii.gz.npy')\n",
    "img = np.load(imgPath)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(20,20))\n",
    "slices = [20,40]\n",
    "num_channels = img.shape[0]\n",
    "k = 1\n",
    "for slice in slices:\n",
    "    for j in range(num_channels):\n",
    "        plt.subplot(num_channels,num_channels,k)\n",
    "        plt.axis('off')\n",
    "        plt.imshow(img[j,:,:,slice])\n",
    "        k+=1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgPath = os.path.join('ignore','data', 'num32labels', 'BRATS_001.nii.gz.npy')\n",
    "img = np.load(imgPath)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.axis('off')\n",
    "plt.imshow(img[:,:,30])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architectures\n",
    "Here is an article with several architectures:\n",
    "https://medium.com/@arthur_ouaknine/review-of-deep-learning-algorithms-for-image-semantic-segmentation-509a600f7b57"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IoU Score\n",
    "# https://cdn-images-1.medium.com/max/1600/1*aXXnB2IEA7DalpGNr3X59g.png\n",
    "def iou_score(y_pred, y, SMOOTH=1e-6, rounding=False):\n",
    "    \"\"\"\n",
    "    aka Jaccard\n",
    "    expect: y_pred, y to be of SAME integer type\n",
    "    \n",
    "    y_pred is output of model\n",
    "        expect: y_pred.shape = (batch_len,D,D,S)\n",
    "        (no channels!)\n",
    "    y is truth value (labels)\n",
    "        expect: y.shape = (batch_len,D,D,S)\n",
    "    \n",
    "    returns: the mean across the batch of the iou scores\n",
    "    \"\"\"\n",
    "    # sanity check\n",
    "    assert y_pred.shape == y.shape\n",
    "    # to compute scores, we sum along all axes except for batch\n",
    "    axes = tuple([i for i in range(1,len(y.shape))])\n",
    "    batch_len = y.shape[0]\n",
    "    # if y_pred hasn't been rounded\n",
    "    if rounding:\n",
    "        y_pred = y_pred.round()\n",
    "    \n",
    "    intersection = (y_pred & y).sum(dim=axes).float()\n",
    "    union = (y_pred | y).sum(dim=axes).float()\n",
    "    # sanity check\n",
    "    assert intersection.shape == union.shape\n",
    "    assert union.shape == (batch_len,)\n",
    "    \n",
    "    iou = 1 - (intersection + SMOOTH) / (union + SMOOTH)\n",
    "    \n",
    "    return iou.mean()\n",
    "\n",
    "def iou_loss(y_pred, y, SMOOTH=1e-6):\n",
    "    \"\"\"\n",
    "    essentially returns 1 - iou_score\n",
    "    but takes care of y_pred not being rounded\n",
    "    \"\"\"\n",
    "    assert y_pred.shape == y.shape\n",
    "    axes = tuple([i for i in range(1,len(y.shape))])\n",
    "    batch_len = y.shape[0]\n",
    "    \n",
    "    numerator = y*y_pred\n",
    "    numerator = numerator.sum(dim=axes)\n",
    "    a = y.sum(dim=axes)\n",
    "    b = y.sum(dim=axes)\n",
    "    denominator = a + b - numerator\n",
    "    quotient = (numerator + SMOOTH) / (denominator + SMOOTH)\n",
    "    return quotient.mean()\n",
    "\n",
    "class iou_module(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(iou_module,self).__init__()\n",
    "\n",
    "    def forward(self, y_pred, y):\n",
    "        loss = iou_loss(y_pred, y)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulate data\n",
    "def simulator(model,criterion,batch_len=1,C=4,D=32,S=64):\n",
    "    # simulate input data\n",
    "    x = torch.randn(batch_len,C,D,D,S)\n",
    "    print(f\"Input has shape {tuple(x.shape)}.\")\n",
    "    # simulate output data\n",
    "    y = torch.randn(batch_len,D,D,S)\n",
    "    y = torch.sigmoid(y).round()\n",
    "    print(f\"Target has shape {tuple(y.shape)}.\")\n",
    "    # simulate prediction for training\n",
    "    y_pred = model(x, evaluating=False)\n",
    "    print(f\"Training output has shape {tuple(y_pred.shape)}.\")\n",
    "    # compute loss\n",
    "    loss = criterion(y_pred,y)\n",
    "    print(f\"Loss is {loss.item()}.\")\n",
    "    \n",
    "    loss.backward()\n",
    "    print(\"Backward pass works.\")\n",
    "    \n",
    "    # simulate prediction for evaluating\n",
    "    y_pred = model(x, evaluating=True)\n",
    "    print(f\"Evaluation output has shape {tuple(y_pred.shape)}.\")\n",
    "    # convert to int type\n",
    "    # x.byte() is equivalent to x.to(dtype=torch.uint8)\n",
    "    # https://pytorch.org/docs/stable/tensors.html\n",
    "    y_pred = y_pred.byte()\n",
    "    y = y.byte()\n",
    "    score = iou_score(y_pred,y).item()\n",
    "    print(f\"IoU score is {score}.\")\n",
    "\n",
    "    # simulate segmentation comparison\n",
    "    y_pred = y_pred[0].cpu().detach().numpy()\n",
    "    y = y[0].cpu().detach().numpy()\n",
    "    \n",
    "    plt.figure(figsize=(10,10))\n",
    "    \n",
    "    plt.subplot(1,2,1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(y_pred[:,:,35])\n",
    "    \n",
    "    plt.subplot(1,2,2)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(y[:,:,35])\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pixelwise Logistic Regression (bad for your health)\n",
    "class PixLog(nn.Module):\n",
    "    def __init__(self,C=4,D=32,S=64):\n",
    "        super(PixLog,self).__init__()\n",
    "        \n",
    "        self.dimensions = (C,D,D,S)\n",
    "        self.fc = nn.Linear(in_features=C*D*D*S, out_features=D*D*S,bias=True)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x_in, evaluating=False):\n",
    "        batch_len = x_in.shape[0]\n",
    "        x = x_in.view(batch_len,-1)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        if evaluating:\n",
    "            x = self.sig(x)\n",
    "            x = x.round()\n",
    "        \n",
    "        dummy_dim = (-1,) + self.dimensions[1:]\n",
    "        x_out = x.view(dummy_dim)\n",
    "        return x_out\n",
    "\n",
    "# If you even try to initialize this, you're gonna have a bad time.\n",
    "# loss = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input has shape (1, 4, 32, 32, 64).\n",
      "Target has shape (1, 32, 32, 64).\n",
      "Training output has shape (1, 32, 32, 64).\n",
      "Loss is 0.706767737865448.\n",
      "Backward pass works.\n",
      "Evaluation output has shape (1, 32, 32, 64).\n",
      "IoU score is 0.6669915914535522.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input has shape (1, 4, 32, 32, 64).\n",
      "Target has shape (1, 32, 32, 64).\n",
      "Training output has shape (1, 32, 32, 64).\n",
      "Loss is 0.0008251722902059555.\n",
      "Backward pass works.\n",
      "Evaluation output has shape (1, 32, 32, 64).\n",
      "IoU score is 0.664172887802124.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# a simple encoderdecoder-type model\n",
    "class Crush(nn.Module):\n",
    "    def __init__(self,D=32,S=64,C=4,crush_size=32):\n",
    "        super(Crush,self).__init__()\n",
    "        \n",
    "        self.dimensions = (C,D,D,S)\n",
    "        self.in_features = int(D*D*S*C)\n",
    "        self.crush = crush_size\n",
    "        self.out_features = int(D*D*S)\n",
    "        self.enc = nn.Linear(in_features=self.in_features,out_features=self.crush,bias=True)\n",
    "        self.act = nn.ReLU()\n",
    "        self.dec = nn.Linear(in_features=self.crush,out_features=self.out_features)\n",
    "        self.sig = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x_in, evaluating=False):\n",
    "        batch_len = x_in.shape[0]\n",
    "        x = x_in.view(batch_len,-1)\n",
    "        x = self.enc(x)\n",
    "        x = self.act(x)\n",
    "        x = self.dec(x)\n",
    "        \n",
    "        if evaluating:\n",
    "            x = self.sig(x)\n",
    "            x = x.round()\n",
    "        \n",
    "        dummy_dim = (-1,) + self.dimensions[1:]\n",
    "        x_out = x.view(dummy_dim)\n",
    "        return x_out\n",
    "\n",
    "\n",
    "model = Crush()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "# loss = nn.BCEWithLogitsLoss()\n",
    "# should be of the form loss(y_pred,y)\n",
    "simulator(model=model,criterion=nn.BCEWithLogitsLoss())\n",
    "simulator(model=model,criterion=iou_module())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a simple sequence of convolutional layers\n",
    "# neat animation explaining convolutions\n",
    "# https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md\n",
    "class ConvSeq(nn.Module):\n",
    "    def __init__(self,input_channels=4):\n",
    "        super(ConvSeq,self).__init__()\n",
    "        \n",
    "        self.input_channels = input_channels\n",
    "        \n",
    "        self.c1 = self.ConvLayer(in_channels=self.input_channels)\n",
    "        self.c2 = self.ConvLayer()\n",
    "        self.c3 = self.ConvLayer()\n",
    "        self.cfinal = self.ConvLayer(out_channels=1, relu=False)\n",
    "    \n",
    "    def forward(self, x_in, evaluating=False):\n",
    "        x = self.c1(x_in)\n",
    "        x = self.c2(x)\n",
    "        x = self.c3(x)\n",
    "        x_out = self.cfinal(x).squeeze(1)\n",
    "        \n",
    "        if evaluating:\n",
    "            x_out = torch.sigmoid(x_out)\n",
    "            x_out = x_out.round()\n",
    "            \n",
    "        return x_out\n",
    "    \n",
    "    def ConvLayer(self, in_channels=32, out_channels=32, kernel_size=3, stride=1, padding=1, bias=True, relu=True):\n",
    "        layer = nn.Sequential()\n",
    "        conv = nn.Conv3d(in_channels=in_channels,\n",
    "                         out_channels=out_channels,\n",
    "                         kernel_size=kernel_size,\n",
    "                         padding=padding,\n",
    "                         bias=bias)\n",
    "        layer.add_module('conv',conv)\n",
    "        if relu:\n",
    "            layer.add_module('relu',nn.ReLU())\n",
    "\n",
    "        return layer\n",
    "\n",
    "\n",
    "model = ConvSeq()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "loss = nn.BCEWithLogitsLoss()\n",
    "# should be of the form loss(y_pred,y)\n",
    "simulator(model=model,loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input has shape (1, 4, 32, 32, 64).\n",
      "Target has shape (1, 32, 32, 64).\n",
      "Training output has shape (1, 32, 32, 64).\n",
      "Loss is 0.7326396703720093.\n",
      "Evaluation output has shape (1, 32, 32, 64).\n",
      "IoU score is 0.24821293354034424.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# small 3d u-net with concatenating skip connection\n",
    "# original paper https://arxiv.org/abs/1505.04597\n",
    "class Small3dUcat(nn.Module):\n",
    "    def __init__(self,input_channels=4,num_filters=32):\n",
    "        super(Small3dUcat,self).__init__()\n",
    "        \n",
    "        # Structure:\n",
    "        # Conv,Conv,MaxPool,Conv,Conv,UnPool,Conv,Conv\n",
    "        \n",
    "        self.input_channels = input_channels\n",
    "        self.num_filters = num_filters\n",
    "        \n",
    "        self.c1 = self.ConvLayer(in_channels=self.input_channels,out_channels=num_filters)\n",
    "        self.c2 = self.ConvLayer(in_channels=num_filters,out_channels=num_filters)\n",
    "\n",
    "        self.c3 = self.ConvLayer(in_channels=num_filters,out_channels=num_filters)\n",
    "        self.c4 = self.ConvLayer(in_channels=num_filters,out_channels=num_filters)\n",
    "        \n",
    "        self.c5 = self.ConvLayer(in_channels=2*num_filters,out_channels=num_filters)\n",
    "        self.c6 = self.ConvLayer(in_channels=num_filters,out_channels=num_filters)\n",
    "        self.c7 = self.ConvLayer(in_channels=num_filters,out_channels=1)\n",
    "    \n",
    "    def forward(self, x_in, evaluating=False):\n",
    "        x1 = self.c1(x_in)\n",
    "        x1 = self.c2(x1)\n",
    "        \n",
    "        x2 = F.max_pool3d(x1,kernel_size=2)\n",
    "        x2 = self.c3(x2)\n",
    "        x2 = self.c4(x2)\n",
    "        \n",
    "        x2 = F.interpolate(x2, scale_factor=2)\n",
    "        # concatenate x1,x2\n",
    "        x = torch.cat([x1,x2],dim=1)\n",
    "        x = self.c5(x)\n",
    "        x = self.c6(x)\n",
    "        x = self.c7(x)\n",
    "        \n",
    "        x_out = x.squeeze(1)\n",
    "        \n",
    "        if evaluating:\n",
    "            x_out = torch.sigmoid(x_out)\n",
    "            x_out = x_out.round()\n",
    "\n",
    "        return x_out\n",
    "    \n",
    "    def ConvLayer(self, in_channels=32, out_channels=32, kernel_size=3,\n",
    "                  stride=1, padding=1, bias=True, relu=True, batchnorm=True):\n",
    "        layer = nn.Sequential()\n",
    "        conv = nn.Conv3d(in_channels=in_channels,\n",
    "                         out_channels=out_channels,\n",
    "                         kernel_size=kernel_size,\n",
    "                         padding=padding,\n",
    "                         bias=bias)\n",
    "        layer.add_module('conv',conv)\n",
    "        if relu:\n",
    "            layer.add_module('relu',nn.ReLU())\n",
    "        if batchnorm:\n",
    "            layer.add_module('batchnorm',nn.BatchNorm3d(out_channels))\n",
    "\n",
    "        return layer\n",
    "\n",
    "\n",
    "model = Small3dUcat()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "loss = nn.BCEWithLogitsLoss()\n",
    "# should be of the form loss(y_pred,y)\n",
    "simulator(model=model,loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input has shape (1, 4, 32, 32, 64).\n",
      "Target has shape (1, 32, 32, 64).\n",
      "Training output has shape (1, 32, 32, 64).\n",
      "Loss is -0.001723131979815662.\n",
      "Evaluation output has shape (1, 32, 32, 64).\n",
      "IoU score is 0.2612839937210083.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# small 3d u-net with addition skip connection\n",
    "# original paper https://arxiv.org/abs/1505.04597\n",
    "# a simple sequence of convolutional layers\n",
    "class Small3dUadd(nn.Module):\n",
    "    def __init__(self,input_channels=4,num_filters=32):\n",
    "        super(Small3dUadd,self).__init__()\n",
    "        \n",
    "        # Structure:\n",
    "        # Conv,Conv,MaxPool,Conv,Conv,UnPool,Conv,Conv\n",
    "        \n",
    "        self.input_channels = input_channels\n",
    "        self.num_filters = num_filters\n",
    "        \n",
    "        self.c1 = self.ConvLayer(in_channels=self.input_channels,out_channels=num_filters)\n",
    "        self.c2 = self.ConvLayer(in_channels=num_filters,out_channels=num_filters)\n",
    "\n",
    "        self.c3 = self.ConvLayer(in_channels=num_filters,out_channels=num_filters)\n",
    "        self.c4 = self.ConvLayer(in_channels=num_filters,out_channels=num_filters)\n",
    "        \n",
    "        self.c5 = self.ConvLayer(in_channels=num_filters,out_channels=num_filters)\n",
    "        self.c6 = self.ConvLayer(in_channels=num_filters,out_channels=num_filters)\n",
    "        self.c7 = self.ConvLayer(in_channels=num_filters,out_channels=1)\n",
    "    \n",
    "    def forward(self, x_in, evaluating=False):\n",
    "        x1 = self.c1(x_in)\n",
    "        x1 = self.c2(x1)\n",
    "        \n",
    "        x2 = F.max_pool3d(x1,kernel_size=2)\n",
    "        x2 = self.c3(x2)\n",
    "        x2 = self.c4(x2)\n",
    "        \n",
    "        x2 = F.interpolate(x2, scale_factor=2)\n",
    "        # add x1,x2\n",
    "        x = F.relu(x1+x2)\n",
    "        x = self.c5(x)\n",
    "        x = self.c6(x)\n",
    "        x = self.c7(x)\n",
    "        \n",
    "        x_out = x.squeeze(1)\n",
    "        \n",
    "        if evaluating:\n",
    "            x_out = torch.sigmoid(x_out)\n",
    "            x_out = x_out.round()\n",
    "\n",
    "        return x_out\n",
    "    \n",
    "    def ConvLayer(self, in_channels=32, out_channels=32, kernel_size=3,\n",
    "                  stride=1, padding=1, bias=True, relu=True, batchnorm=True):\n",
    "        layer = nn.Sequential()\n",
    "        conv = nn.Conv3d(in_channels=in_channels,\n",
    "                         out_channels=out_channels,\n",
    "                         kernel_size=kernel_size,\n",
    "                         padding=padding,\n",
    "                         bias=bias)\n",
    "        layer.add_module('conv',conv)\n",
    "        if relu:\n",
    "            layer.add_module('relu',nn.ReLU())\n",
    "        if batchnorm:\n",
    "            layer.add_module('batchnorm',nn.BatchNorm3d(out_channels))\n",
    "\n",
    "        return layer\n",
    "\n",
    "\n",
    "model = Small3dUadd()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "# loss = nn.BCEWithLogitsLoss()\n",
    "loss = iou_loss\n",
    "# should be of the form loss(y_pred,y)\n",
    "simulator(model=model,loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'backward'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-4f1fe93a04fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# paper discussing segmentation in medical imaging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# https://arxiv.org/pdf/1701.03056.pdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'function' object has no attribute 'backward'"
     ]
    }
   ],
   "source": [
    "loss.backward()\n",
    "# paper discussing segmentation in medical imaging\n",
    "# https://arxiv.org/pdf/1701.03056.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1,2,3])\n",
    "y = torch.tensor([1,2,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x+y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x*y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Jaccard loss module\n",
    "# page 7: https://arxiv.org/pdf/1701.03056.pdf \n",
    "# do a big U-net like in that paper\n",
    "# do a 2d version of that\n",
    "# create dataset class\n",
    "# train!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
