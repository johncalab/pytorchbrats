{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure that conversion went ok\n",
    "imgPath = os.path.join('ignore','data', 'num32train', 'BRATS_001.nii.gz.npy')\n",
    "img = np.load(imgPath)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(20,20))\n",
    "slices = [20,40]\n",
    "num_channels = img.shape[0]\n",
    "k = 1\n",
    "for slice in slices:\n",
    "    for j in range(num_channels):\n",
    "        plt.subplot(num_channels,num_channels,k)\n",
    "        plt.axis('off')\n",
    "        plt.imshow(img[j,:,:,slice])\n",
    "        k+=1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgPath = os.path.join('ignore','data', 'num32labels', 'BRATS_001.nii.gz.npy')\n",
    "img = np.load(imgPath)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.axis('off')\n",
    "plt.imshow(img[:,:,30])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architectures\n",
    "Here is an article with several architectures:\n",
    "https://medium.com/@arthur_ouaknine/review-of-deep-learning-algorithms-for-image-semantic-segmentation-509a600f7b57"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IoU Score\n",
    "# https://cdn-images-1.medium.com/max/1600/1*aXXnB2IEA7DalpGNr3X59g.png\n",
    "def iou_score(y_pred, y, SMOOTH=1e-6, rounding=False):\n",
    "    \"\"\"\n",
    "    aka Jaccard\n",
    "    expect: y_pred, y to be of SAME integer type\n",
    "    \n",
    "    y_pred is output of model\n",
    "        expect: y_pred.shape = (batch_len,D,D,S)\n",
    "        (no channels!)\n",
    "    y is truth value (labels)\n",
    "        expect: y.shape = (batch_len,D,D,S)\n",
    "    \n",
    "    returns: the mean across the batch of the iou scores\n",
    "    \"\"\"\n",
    "    # sanity check\n",
    "    assert y_pred.shape == y.shape\n",
    "    # to compute scores, we sum along all axes except for batch\n",
    "    axes = tuple([i for i in range(1,len(y.shape))])\n",
    "    batch_len = y.shape[0]\n",
    "    # if y_pred hasn't been rounded\n",
    "    if rounding:\n",
    "        y_pred = y_pred.round()\n",
    "    \n",
    "    intersection = (y_pred & y).sum(dim=axes).float()\n",
    "    union = (y_pred | y).sum(dim=axes).float()\n",
    "    # sanity check\n",
    "    assert intersection.shape == union.shape\n",
    "    assert union.shape == (batch_len,)\n",
    "    \n",
    "    iou = 1 - (intersection + SMOOTH) / (union + SMOOTH)\n",
    "    \n",
    "    return iou.mean()\n",
    "\n",
    "def iou_loss(y_pred, y, SMOOTH=1e-6):\n",
    "    \"\"\"\n",
    "    essentially returns 1 - iou_score\n",
    "    but takes care of y_pred not being rounded\n",
    "    \"\"\"\n",
    "    assert y_pred.shape == y.shape\n",
    "    axes = tuple([i for i in range(1,len(y.shape))])\n",
    "    batch_len = y.shape[0]\n",
    "    \n",
    "    numerator = y*y_pred\n",
    "    numerator = numerator.sum(dim=axes)\n",
    "    a = y.sum(dim=axes)\n",
    "    b = y.sum(dim=axes)\n",
    "    denominator = a + b - numerator\n",
    "    quotient = (numerator + SMOOTH) / (denominator + SMOOTH)\n",
    "    return quotient.mean()\n",
    "\n",
    "class iou_module(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(iou_module,self).__init__()\n",
    "\n",
    "    def forward(self, y_pred, y):\n",
    "        loss = iou_loss(y_pred, y)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulate data\n",
    "def simulator(model,loss,batch_len=1,C=4,D=32,S=64):\n",
    "    # simulate input data\n",
    "    x = torch.randn(batch_len,C,D,D,S)\n",
    "    print(f\"Input has shape {tuple(x.shape)}.\")\n",
    "    # simulate output data\n",
    "    y = torch.randn(batch_len,D,D,S)\n",
    "    y = torch.sigmoid(y).round()\n",
    "    print(f\"Target has shape {tuple(y.shape)}.\")\n",
    "    # simulate prediction for training\n",
    "    y_pred = model(x, evaluating=False)\n",
    "    print(f\"Training output has shape {tuple(y_pred.shape)}.\")\n",
    "    # compute loss\n",
    "    l = loss(y_pred,y)\n",
    "    print(f\"Loss is {l.item()}.\")\n",
    "    \n",
    "    l.backward()\n",
    "    print(\"Backward pass works.\")\n",
    "    \n",
    "    # simulate prediction for evaluating\n",
    "    y_pred = model(x, evaluating=True)\n",
    "    print(f\"Evaluation output has shape {tuple(y_pred.shape)}.\")\n",
    "    # convert to int type\n",
    "    # x.byte() is equivalent to x.to(dtype=torch.uint8)\n",
    "    # https://pytorch.org/docs/stable/tensors.html\n",
    "    y_pred = y_pred.byte()\n",
    "    y = y.byte()\n",
    "    score = iou_score(y_pred,y).item()\n",
    "    print(f\"IoU score is {score}.\")\n",
    "\n",
    "    # simulate segmentation comparison\n",
    "    y_pred = y_pred[0].cpu().detach().numpy()\n",
    "    y = y[0].cpu().detach().numpy()\n",
    "    \n",
    "    plt.figure(figsize=(10,10))\n",
    "    \n",
    "    plt.subplot(1,2,1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(y_pred[:,:,35])\n",
    "    \n",
    "    plt.subplot(1,2,2)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(y[:,:,35])\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pixelwise Logistic Regression (bad for your health)\n",
    "class PixLog(nn.Module):\n",
    "    def __init__(self,C=4,D=32,S=64):\n",
    "        super(PixLog,self).__init__()\n",
    "        \n",
    "        self.dimensions = (C,D,D,S)\n",
    "        self.fc = nn.Linear(in_features=C*D*D*S, out_features=D*D*S,bias=True)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x_in, evaluating=False):\n",
    "        batch_len = x_in.shape[0]\n",
    "        x = x_in.view(batch_len,-1)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        if evaluating:\n",
    "            x = self.sig(x)\n",
    "            x = x.round()\n",
    "        \n",
    "        dummy_dim = (-1,) + self.dimensions[1:]\n",
    "        x_out = x.view(dummy_dim)\n",
    "        return x_out\n",
    "\n",
    "# If you even try to initialize this, you're gonna have a bad time.\n",
    "# loss = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input has shape (1, 4, 32, 32, 64).\n",
      "Target has shape (1, 32, 32, 64).\n",
      "Training output has shape (1, 32, 32, 64).\n",
      "Loss is -0.000548207841347903.\n",
      "Backward pass works.\n",
      "Evaluation output has shape (1, 32, 32, 64).\n",
      "IoU score is 0.6657758951187134.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# a simple encoderdecoder-type model\n",
    "class Crush(nn.Module):\n",
    "    def __init__(self,D=32,S=64,C=4,crush_size=32):\n",
    "        super(Crush,self).__init__()\n",
    "        \n",
    "        self.dimensions = (C,D,D,S)\n",
    "        self.in_features = int(D*D*S*C)\n",
    "        self.crush = crush_size\n",
    "        self.out_features = int(D*D*S)\n",
    "        self.enc = nn.Linear(in_features=self.in_features,out_features=self.crush,bias=True)\n",
    "        self.act = nn.ReLU()\n",
    "        self.dec = nn.Linear(in_features=self.crush,out_features=self.out_features)\n",
    "        self.sig = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x_in, evaluating=False):\n",
    "        batch_len = x_in.shape[0]\n",
    "        x = x_in.view(batch_len,-1)\n",
    "        x = self.enc(x)\n",
    "        x = self.act(x)\n",
    "        x = self.dec(x)\n",
    "        \n",
    "        if evaluating:\n",
    "            x = self.sig(x)\n",
    "            x = x.round()\n",
    "        \n",
    "        dummy_dim = (-1,) + self.dimensions[1:]\n",
    "        x_out = x.view(dummy_dim)\n",
    "        return x_out\n",
    "\n",
    "\n",
    "model = Crush()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "# loss = nn.BCEWithLogitsLoss()\n",
    "# should be of the form loss(y_pred,y)\n",
    "loss = iou_module()\n",
    "simulator(model=model,loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a simple sequence of convolutional layers\n",
    "# neat animation explaining convolutions\n",
    "# https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md\n",
    "class ConvSeq(nn.Module):\n",
    "    def __init__(self,input_channels=4):\n",
    "        super(ConvSeq,self).__init__()\n",
    "        \n",
    "        self.input_channels = input_channels\n",
    "        \n",
    "        self.c1 = self.ConvLayer(in_channels=self.input_channels)\n",
    "        self.c2 = self.ConvLayer()\n",
    "        self.c3 = self.ConvLayer()\n",
    "        self.cfinal = self.ConvLayer(out_channels=1, relu=False)\n",
    "    \n",
    "    def forward(self, x_in, evaluating=False):\n",
    "        x = self.c1(x_in)\n",
    "        x = self.c2(x)\n",
    "        x = self.c3(x)\n",
    "        x_out = self.cfinal(x).squeeze(1)\n",
    "        \n",
    "        if evaluating:\n",
    "            x_out = torch.sigmoid(x_out)\n",
    "            x_out = x_out.round()\n",
    "            \n",
    "        return x_out\n",
    "    \n",
    "    def ConvLayer(self, in_channels=32, out_channels=32, kernel_size=3, stride=1, padding=1, bias=True, relu=True):\n",
    "        layer = nn.Sequential()\n",
    "        conv = nn.Conv3d(in_channels=in_channels,\n",
    "                         out_channels=out_channels,\n",
    "                         kernel_size=kernel_size,\n",
    "                         padding=padding,\n",
    "                         bias=bias)\n",
    "        layer.add_module('conv',conv)\n",
    "        if relu:\n",
    "            layer.add_module('relu',nn.ReLU())\n",
    "\n",
    "        return layer\n",
    "\n",
    "\n",
    "model = ConvSeq()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "loss = nn.BCEWithLogitsLoss()\n",
    "# should be of the form loss(y_pred,y)\n",
    "simulator(model=model,loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input has shape (1, 4, 32, 32, 64).\n",
      "Target has shape (1, 32, 32, 64).\n",
      "Training output has shape (1, 32, 32, 64).\n",
      "Loss is 0.7326396703720093.\n",
      "Evaluation output has shape (1, 32, 32, 64).\n",
      "IoU score is 0.24821293354034424.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# small 3d u-net with concatenating skip connection\n",
    "# original paper https://arxiv.org/abs/1505.04597\n",
    "class Small3dUcat(nn.Module):\n",
    "    def __init__(self,input_channels=4,num_filters=32):\n",
    "        super(Small3dUcat,self).__init__()\n",
    "        \n",
    "        # Structure:\n",
    "        # Conv,Conv,MaxPool,Conv,Conv,UnPool,Conv,Conv\n",
    "        \n",
    "        self.input_channels = input_channels\n",
    "        self.num_filters = num_filters\n",
    "        \n",
    "        self.c1 = self.ConvLayer(in_channels=self.input_channels,out_channels=num_filters)\n",
    "        self.c2 = self.ConvLayer(in_channels=num_filters,out_channels=num_filters)\n",
    "\n",
    "        self.c3 = self.ConvLayer(in_channels=num_filters,out_channels=num_filters)\n",
    "        self.c4 = self.ConvLayer(in_channels=num_filters,out_channels=num_filters)\n",
    "        \n",
    "        self.c5 = self.ConvLayer(in_channels=2*num_filters,out_channels=num_filters)\n",
    "        self.c6 = self.ConvLayer(in_channels=num_filters,out_channels=num_filters)\n",
    "        self.c7 = self.ConvLayer(in_channels=num_filters,out_channels=1)\n",
    "    \n",
    "    def forward(self, x_in, evaluating=False):\n",
    "        x1 = self.c1(x_in)\n",
    "        x1 = self.c2(x1)\n",
    "        \n",
    "        x2 = F.max_pool3d(x1,kernel_size=2)\n",
    "        x2 = self.c3(x2)\n",
    "        x2 = self.c4(x2)\n",
    "        \n",
    "        x2 = F.interpolate(x2, scale_factor=2)\n",
    "        # concatenate x1,x2\n",
    "        x = torch.cat([x1,x2],dim=1)\n",
    "        x = self.c5(x)\n",
    "        x = self.c6(x)\n",
    "        x = self.c7(x)\n",
    "        \n",
    "        x_out = x.squeeze(1)\n",
    "        \n",
    "        if evaluating:\n",
    "            x_out = torch.sigmoid(x_out)\n",
    "            x_out = x_out.round()\n",
    "\n",
    "        return x_out\n",
    "    \n",
    "    def ConvLayer(self, in_channels=32, out_channels=32, kernel_size=3,\n",
    "                  stride=1, padding=1, bias=True, relu=True, batchnorm=True):\n",
    "        layer = nn.Sequential()\n",
    "        conv = nn.Conv3d(in_channels=in_channels,\n",
    "                         out_channels=out_channels,\n",
    "                         kernel_size=kernel_size,\n",
    "                         padding=padding,\n",
    "                         bias=bias)\n",
    "        layer.add_module('conv',conv)\n",
    "        if relu:\n",
    "            layer.add_module('relu',nn.ReLU())\n",
    "        if batchnorm:\n",
    "            layer.add_module('batchnorm',nn.BatchNorm3d(out_channels))\n",
    "\n",
    "        return layer\n",
    "\n",
    "\n",
    "model = Small3dUcat()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "loss = nn.BCEWithLogitsLoss()\n",
    "# should be of the form loss(y_pred,y)\n",
    "simulator(model=model,loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input has shape (1, 4, 32, 32, 64).\n",
      "Target has shape (1, 32, 32, 64).\n",
      "Training output has shape (1, 32, 32, 64).\n",
      "Loss is -0.001723131979815662.\n",
      "Evaluation output has shape (1, 32, 32, 64).\n",
      "IoU score is 0.2612839937210083.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# small 3d u-net with addition skip connection\n",
    "# original paper https://arxiv.org/abs/1505.04597\n",
    "# a simple sequence of convolutional layers\n",
    "class Small3dUadd(nn.Module):\n",
    "    def __init__(self,input_channels=4,num_filters=32):\n",
    "        super(Small3dUadd,self).__init__()\n",
    "        \n",
    "        # Structure:\n",
    "        # Conv,Conv,MaxPool,Conv,Conv,UnPool,Conv,Conv\n",
    "        \n",
    "        self.input_channels = input_channels\n",
    "        self.num_filters = num_filters\n",
    "        \n",
    "        self.c1 = self.ConvLayer(in_channels=self.input_channels,out_channels=num_filters)\n",
    "        self.c2 = self.ConvLayer(in_channels=num_filters,out_channels=num_filters)\n",
    "\n",
    "        self.c3 = self.ConvLayer(in_channels=num_filters,out_channels=num_filters)\n",
    "        self.c4 = self.ConvLayer(in_channels=num_filters,out_channels=num_filters)\n",
    "        \n",
    "        self.c5 = self.ConvLayer(in_channels=num_filters,out_channels=num_filters)\n",
    "        self.c6 = self.ConvLayer(in_channels=num_filters,out_channels=num_filters)\n",
    "        self.c7 = self.ConvLayer(in_channels=num_filters,out_channels=1)\n",
    "    \n",
    "    def forward(self, x_in, evaluating=False):\n",
    "        x1 = self.c1(x_in)\n",
    "        x1 = self.c2(x1)\n",
    "        \n",
    "        x2 = F.max_pool3d(x1,kernel_size=2)\n",
    "        x2 = self.c3(x2)\n",
    "        x2 = self.c4(x2)\n",
    "        \n",
    "        x2 = F.interpolate(x2, scale_factor=2)\n",
    "        # add x1,x2\n",
    "        x = F.relu(x1+x2)\n",
    "        x = self.c5(x)\n",
    "        x = self.c6(x)\n",
    "        x = self.c7(x)\n",
    "        \n",
    "        x_out = x.squeeze(1)\n",
    "        \n",
    "        if evaluating:\n",
    "            x_out = torch.sigmoid(x_out)\n",
    "            x_out = x_out.round()\n",
    "\n",
    "        return x_out\n",
    "    \n",
    "    def ConvLayer(self, in_channels=32, out_channels=32, kernel_size=3,\n",
    "                  stride=1, padding=1, bias=True, relu=True, batchnorm=True):\n",
    "        layer = nn.Sequential()\n",
    "        conv = nn.Conv3d(in_channels=in_channels,\n",
    "                         out_channels=out_channels,\n",
    "                         kernel_size=kernel_size,\n",
    "                         padding=padding,\n",
    "                         bias=bias)\n",
    "        layer.add_module('conv',conv)\n",
    "        if relu:\n",
    "            layer.add_module('relu',nn.ReLU())\n",
    "        if batchnorm:\n",
    "            layer.add_module('batchnorm',nn.BatchNorm3d(out_channels))\n",
    "\n",
    "        return layer\n",
    "\n",
    "\n",
    "model = Small3dUadd()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "# loss = nn.BCEWithLogitsLoss()\n",
    "loss = iou_loss\n",
    "# should be of the form loss(y_pred,y)\n",
    "simulator(model=model,loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'backward'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-4f1fe93a04fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# paper discussing segmentation in medical imaging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# https://arxiv.org/pdf/1701.03056.pdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'function' object has no attribute 'backward'"
     ]
    }
   ],
   "source": [
    "loss.backward()\n",
    "# paper discussing segmentation in medical imaging\n",
    "# https://arxiv.org/pdf/1701.03056.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1,2,3])\n",
    "y = torch.tensor([1,2,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x+y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x*y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Jaccard loss module\n",
    "# page 7: https://arxiv.org/pdf/1701.03056.pdf \n",
    "# do a big U-net like in that paper\n",
    "# do a 2d version of that\n",
    "# create dataset class\n",
    "# train!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
