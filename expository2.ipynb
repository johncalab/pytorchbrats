{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure that conversion went ok\n",
    "imgPath = os.path.join('ignore','data', 'num32train', 'BRATS_001.nii.gz.npy')\n",
    "img = np.load(imgPath)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(20,20))\n",
    "slices = [20,40]\n",
    "num_channels = img.shape[0]\n",
    "k = 1\n",
    "for slice in slices:\n",
    "    for j in range(num_channels):\n",
    "        plt.subplot(num_channels,num_channels,k)\n",
    "        plt.axis('off')\n",
    "        plt.imshow(img[j,:,:,slice])\n",
    "        k+=1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgPath = os.path.join('ignore','data', 'num32labels', 'BRATS_001.nii.gz.npy')\n",
    "img = np.load(imgPath)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.axis('off')\n",
    "plt.imshow(img[:,:,30])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architectures\n",
    "Here is an article with several architectures:\n",
    "https://medium.com/@arthur_ouaknine/review-of-deep-learning-algorithms-for-image-semantic-segmentation-509a600f7b57"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IoU Score\n",
    "# https://cdn-images-1.medium.com/max/1600/1*aXXnB2IEA7DalpGNr3X59g.png\n",
    "def iou_score(y_pred, y, SMOOTH=1e-6, rounding=False):\n",
    "    \"\"\"\n",
    "    expect: y_pred, y to be of SAME integer type\n",
    "    \n",
    "    y_pred is output of model\n",
    "        expect: y_pred.shape = (batch_len,D,D,S)\n",
    "        (no channels!)\n",
    "    y is truth value (labels)\n",
    "        expect: y.shape = (batch_len,D,D,S)\n",
    "    \n",
    "    returns: the mean across the batch of the iou scores\n",
    "    \"\"\"\n",
    "    # sanity check\n",
    "    assert y_pred.shape == y.shape\n",
    "    # to compute scores, we sum along all axes except for batch\n",
    "    axes = tuple([i for i in range(1,len(y.shape))])\n",
    "    batch_len = y.shape[0]\n",
    "    # if y_pred hasn't been rounded\n",
    "    if rounding:\n",
    "        y_pred = y_pred.round()\n",
    "    \n",
    "    intersection = (y_pred & y).sum(dim=axes).float()\n",
    "    union = (y_pred | y).sum(dim=axes).float()\n",
    "    # sanity check\n",
    "    assert intersection.shape == union.shape\n",
    "    assert union.shape == (batch_len,)\n",
    "    \n",
    "    iou = (intersection + SMOOTH) / (union + SMOOTH)\n",
    "    \n",
    "    return iou.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulate data\n",
    "def simulator(model,loss,batch_len=1,C=4,D=32,S=64):\n",
    "    # simulate input data\n",
    "    x = torch.randn(batch_len,C,D,D,S)\n",
    "    print(f\"Input has shape {tuple(x.shape)}.\")\n",
    "    # simulate output data\n",
    "    y = torch.randn(batch_len,D,D,S)\n",
    "    y = torch.sigmoid(y).round()\n",
    "    print(f\"Target has shape {tuple(y.shape)}.\")\n",
    "    # simulate prediction for training\n",
    "    y_pred = model(x, evaluating=False)\n",
    "    print(f\"Training output has shape {tuple(y_pred.shape)}.\")\n",
    "    # compute loss\n",
    "    l = loss(y_pred,y).item()\n",
    "    print(f\"Loss is {l}.\")\n",
    "    \n",
    "    # simulate prediction for training\n",
    "    y_pred = model(x, evaluating=True)\n",
    "    print(f\"Evaluation output has shape {tuple(y_pred.shape)}.\")\n",
    "    # convert to int type\n",
    "    # x.byte() is equivalent to x.to(dtype=torch.uint8)\n",
    "    # https://pytorch.org/docs/stable/tensors.html\n",
    "    y_pred = y_pred.byte()\n",
    "    y = y.byte()\n",
    "    score = iou_score(y_pred,y).item()\n",
    "    print(f\"IoU score is {score}.\")\n",
    "\n",
    "    # simulate segmentation comparison\n",
    "    y_pred = y_pred[0].cpu().detach().numpy()\n",
    "    y = y[0].cpu().detach().numpy()\n",
    "    \n",
    "    plt.figure(figsize=(10,10))\n",
    "    \n",
    "    plt.subplot(1,2,1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(y_pred[:,:,35])\n",
    "    \n",
    "    plt.subplot(1,2,2)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(y[:,:,35])\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pixelwise Logistic Regression (bad for your health)\n",
    "class PixLog(nn.Module):\n",
    "    def __init__(self,C=4,D=32,S=64):\n",
    "        super(PixLog,self).__init__()\n",
    "        \n",
    "        self.dimensions = (C,D,D,S)\n",
    "        self.fc = nn.Linear(in_features=C*D*D*S, out_features=D*D*S,bias=True)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x_in, evaluating=False):\n",
    "        batch_len = x_in.shape[0]\n",
    "        x = x_in.view(batch_len,-1)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        if evaluating:\n",
    "            x = self.sig(x)\n",
    "            x = x.round()\n",
    "        \n",
    "        dummy_dim = (-1,) + self.dimensions[1:]\n",
    "        x_out = x.view(dummy_dim)\n",
    "        return x_out\n",
    "\n",
    "# If you even try to initialize this, you're gonna have a bad time.\n",
    "# loss = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input has shape (1, 4, 32, 32, 64).\n",
      "Target has shape (1, 32, 32, 64).\n",
      "Training output has shape (1, 32, 32, 64).\n",
      "Loss is 0.7022231817245483.\n",
      "Evaluation output has shape (1, 32, 32, 64).\n",
      "IoU score is 0.3346334397792816.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# a simple encoderdecoder-type model\n",
    "class Crush(nn.Module):\n",
    "    def __init__(self,D=32,S=64,C=4,crush_size=32):\n",
    "        super(Crush,self).__init__()\n",
    "        \n",
    "        self.dimensions = (C,D,D,S)\n",
    "        self.in_features = int(D*D*S*C)\n",
    "        self.crush = crush_size\n",
    "        self.out_features = int(D*D*S)\n",
    "        self.enc = nn.Linear(in_features=self.in_features,out_features=self.crush,bias=True)\n",
    "        self.act = nn.ReLU()\n",
    "        self.dec = nn.Linear(in_features=self.crush,out_features=self.out_features)\n",
    "        self.sig = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x_in, evaluating=False):\n",
    "        batch_len = x_in.shape[0]\n",
    "        x = x_in.view(batch_len,-1)\n",
    "        x = self.enc(x)\n",
    "        x = self.act(x)\n",
    "        x = self.dec(x)\n",
    "        \n",
    "        if evaluating:\n",
    "            x = self.sig(x)\n",
    "            x = x.round()\n",
    "        \n",
    "        dummy_dim = (-1,) + self.dimensions[1:]\n",
    "        x_out = x.view(dummy_dim)\n",
    "        return x_out\n",
    "\n",
    "\n",
    "model = Crush()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "loss = nn.BCEWithLogitsLoss()\n",
    "# should be of the form loss(y_pred,y)\n",
    "simulator(model=model,loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input has shape (1, 4, 32, 32, 64).\n",
      "Target has shape (1, 32, 32, 64).\n",
      "Training output has shape (1, 32, 32, 64).\n",
      "Loss is 0.693128228187561.\n",
      "Evaluation output has shape (1, 32, 32, 64).\n",
      "IoU score is 0.27150726318359375.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# a simple sequence of convolutional layers\n",
    "# neat animation explaining convolutions\n",
    "# https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md\n",
    "class ConvSeq(nn.Module):\n",
    "    def __init__(self,input_channels=4):\n",
    "        super(ConvSeq,self).__init__()\n",
    "        \n",
    "        self.input_channels = input_channels\n",
    "        \n",
    "        self.c1 = self.ConvLayer(in_channels=self.input_channels)\n",
    "        self.c2 = self.ConvLayer()\n",
    "        self.c3 = self.ConvLayer()\n",
    "        self.cfinal = self.ConvLayer(out_channels=1, relu=False)\n",
    "    \n",
    "    def forward(self, x_in, evaluating=False):\n",
    "        x = self.c1(x_in)\n",
    "        x = self.c2(x)\n",
    "        x = self.c3(x)\n",
    "        x_out = self.cfinal(x).squeeze(1)\n",
    "        \n",
    "        if evaluating:\n",
    "            x_out = torch.sigmoid(x_out)\n",
    "            x_out = x_out.round()\n",
    "            \n",
    "        return x_out\n",
    "    \n",
    "    def ConvLayer(self, in_channels=32, out_channels=32, kernel_size=3, stride=1, padding=1, bias=True, relu=True):\n",
    "        layer = nn.Sequential()\n",
    "        conv = nn.Conv3d(in_channels=in_channels,\n",
    "                         out_channels=out_channels,\n",
    "                         kernel_size=kernel_size,\n",
    "                         padding=padding,\n",
    "                         bias=bias)\n",
    "        layer.add_module('conv',conv)\n",
    "        if relu:\n",
    "            layer.add_module('relu',nn.ReLU())\n",
    "\n",
    "        return layer\n",
    "\n",
    "\n",
    "model = ConvSeq()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "loss = nn.BCEWithLogitsLoss()\n",
    "# should be of the form loss(y_pred,y)\n",
    "simulator(model=model,loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# small 3d u-net\n",
    "# original paper https://arxiv.org/abs/1505.04597\n",
    "# a simple sequence of convolutional layers\n",
    "class Small3dU(nn.Module):\n",
    "    def __init__(self,input_channels=4):\n",
    "        super(Small3dU,self).__init__()\n",
    "        \n",
    "        # Structure:\n",
    "        # Conv,Conv,MaxPool,Conv,Conv,UnPool,Conv,Conv\n",
    "        \n",
    "        self.input_channels = input_channels\n",
    "        \n",
    "        self.c1 = self.ConvLayer(in_channels=self.input_channels,out_channels=16)\n",
    "        self.c2 = self.ConvLayer(in_channels=16,out_channels=16)\n",
    "\n",
    "        self.c3 = self.ConvLayer(in_channels=16,out_channels=32)\n",
    "        self.c4 = self.ConvLayer(in_channels=32,out_channels=32)\n",
    "        \n",
    "        self.c5 = self.ConvLayer(in_channels=48,out_channels=32)\n",
    "        self.c6 = self.ConvLayer(in_channels=32,out_channels=16)\n",
    "        self.c7 = self.ConvLayer(in_channels=16,out_channels=1)\n",
    "    \n",
    "    def forward(self, x_in, evaluating=False):\n",
    "        x1 = self.c1(x_in)\n",
    "        x1 = self.c2(x1)\n",
    "        \n",
    "        x2 = F.max_pool3d(x1,kernel_size=2)\n",
    "        x2 = self.c3(x2)\n",
    "        x2 = self.c4(x2)\n",
    "        \n",
    "        x2 = F.interpolate(x2, scale_factor=2)\n",
    "        # concatenate x1 and x2 along channel axis\n",
    "        x = torch.cat([x1,x2],dim=1)\n",
    "        x = self.c5(x)\n",
    "        x = self.c6(x)\n",
    "        x = self.c7(x)\n",
    "        \n",
    "        x_out = x.squeeze(1)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def ConvLayer(self, in_channels=32, out_channels=32, kernel_size=3,\n",
    "                  stride=1, padding=1, bias=True, relu=True, batchnorm=True):\n",
    "        layer = nn.Sequential()\n",
    "        conv = nn.Conv3d(in_channels=in_channels,\n",
    "                         out_channels=out_channels,\n",
    "                         kernel_size=kernel_size,\n",
    "                         padding=padding,\n",
    "                         bias=bias)\n",
    "        layer.add_module('conv',conv)\n",
    "        if relu:\n",
    "            layer.add_module('relu',nn.ReLU())\n",
    "        if batchnorm:\n",
    "            layer.add_module('batchnorm',nn.BatchNorm3d(out_channels))\n",
    "\n",
    "        return layer\n",
    "\n",
    "\n",
    "model = Small3dU()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "loss = nn.BCEWithLogitsLoss()\n",
    "# should be of the form loss(y_pred,y)\n",
    "simulator(model=model,loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paper discussing segmentation in medical imaging\n",
    "# https://arxiv.org/pdf/1701.03056.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
