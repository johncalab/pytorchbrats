{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "Linear regression can be applied in a multitude of settings. Here we start from the abstract theory, and move on to the ideal case of iid samples with gaussian noise.\n",
    "## A basic fact\n",
    ">Let $X$ be a random vector, then $\\mu_X = E[X]$ satisfies the following minimization problem:\n",
    "$$\\mu_X=\\argmin_v E[||X - v ||^2].$$\n",
    "\n",
    ">More generally, let $X \\in \\R^a, Y \\in \\R^b$ be random vectors, then $\\mu_{Y|X} = E[Y|X]$ satisfies the following minimiztion problem:\n",
    "$$\\mu_{Y|X}=\\argmin_{g\\colon \\R^a \\to \\R^b} E[|| Y - g(X)||^2]$$\n",
    "\n",
    "i.e. the closest approximation (wrt the $L_2$-norm) of $Y$ as a function $X$ is $\\mu_{Y|X}.$ For this reason $\\mu_{Y|X}$ is also called the *least mean square* (LMS) estimator of $Y$ given $X$. We prove these two simple facts at the end.\n",
    "\n",
    "## Theory\n",
    "We have two random variables $X,Y$. We wish to express $Y$ as a linear function of $X$.\n",
    "In other words, we wish to find $\\beta_0, \\beta_1 \\in \\R$ for which $\\beta_0 + \\beta_1 X$ best approximates $Y$. As before, by 'best approximates' we mean in square norm.\n",
    "In other words, we want to solve the following minimazion problem\n",
    "$$(\\beta_0^*, \\beta_1^*) = \\argmin_{\\beta_0, \\beta_1} E[(Y - \\beta_0 - \\beta_1 X)^2]$$\n",
    "To make life easier afterwards, we complicate a little bit now. Let's call $\\beta = (\\beta_0, \\beta_1)$, let's relabel $X$ as $X^1$, and define $X = (1,X^1)$. We are trying to minimize $Q(\\beta) = E[(Y - \\beta^\\top X)^2] = E[(Y - X^\\top \\beta)^2]$. By taking partials with respect to $\\beta$, we find the Jacobian\n",
    "$$D_\\beta Q = E[ 2(Y - X^\\top \\beta)^\\top (-X^\\top) ]$$\n",
    "and the Hessian \n",
    "$${\\mathrm{Hess}_\\beta} Q=E[2XX^\\top].$$\n",
    "The matrix $E[X X^\\top]$ (which is almost the covariance matrix of $X$) is positive semi-definite. Hence the function $Q$ is convex everywhere (but not necessarily strictly convex, although the latter is guaranteed if $X^1$ has positive variance). The critical point of $Q$ is then given by\n",
    "$$0\\stackrel \\downarrow = D_w \\iff E[XY]=E[XX^\\top]w$$\n",
    "With a few tedious computations one can show\n",
    "$$w_1^* =\\frac  {\\text{Cov}(X^1,Y)} { \\text{Var}(X^1) }$$\n",
    "$$w_0^* = E[Y] - w_1 E[X^1].$$\n",
    "The importance of this fact comes from the Law of Large Numbers: replacing expectations with averages we immediately find estimators for $w$!\n",
    "\n",
    "## Multivariate Case\n",
    "We might as well deal with multivariable case. We start with a random vector $(X^1,\\ldots,X^p) \\in \\R^p$, a random variable $Y \\in \\R$, and we wish to find the best linear relation between $X$ and $Y$. In other words, we need $w_0,\\ldots,w_p$ such that $Y \\approx w_0 + w_1 X^1 + \\cdots w_p X^p$.\n",
    "As before, we write $w = (w_0,\\ldots,w_p)$, $X = (1,X^1,\\ldots,X^p)$. The problem translates to finding\n",
    "$$\\argmin_w E[(Y - w^\\top X)^2] = \\argmin_w E[(Y - X^\\top w)^2].$$ Just as before, by taking partials with respect to $w$, we see this problem is convex and that the minimum $w^*$ satisfies\n",
    "$$E[XY]=E[XX^\\top]w^*.$$\n",
    "\n",
    "If $E[X X^\\top]$ is invertible, we obtain a unique solution given by\n",
    "$$w^* = E[X X^\\top]^{-1} E[XY].$$\n",
    "\n",
    "## Estimators\n",
    "Suppose now we don't have direct access to our model $(X_0,Y_0)$ with $X_0 = (1,X_0^1,\\ldots,X_0^p) \\in \\R^{p+1}$, $Y_0 \\in \\R$. Assume instead to have access to (the realizations of) $n$ iid copies $(X_1,Y_1), \\ldots, (X_n,Y_n)$ of $(X_0,Y_0)$. To estimate $w^*$, we simply replace expectations with averages. We need some notation for that. The *design matrix* is traditionally defined as\n",
    "$$\\mathbb X = \n",
    "  \\begin{pmatrix}\n",
    "    X_1^\\top \\\\ \\vdots \\\\ X_n^\\top\n",
    "  \\end{pmatrix}=\n",
    "  \\begin{pmatrix}\n",
    "    X_1^1 & X_1^2 & \\cdots & X_1^p \\\\\n",
    "    X_2^1 & X_2^2 & \\cdots & X_2^p \\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\vdots \\\\\n",
    "    X_n^1 & X_n^2 & \\cdots & X_n^p\n",
    "  \\end{pmatrix}\n",
    "$$\n",
    "Write also $Y = (Y_1,\\ldots,Y_n)$. We have\n",
    "$$\\sum_{i=1}^n X_i Y_i = \\mathbb X^\\top Y \\quad\\quad\\quad \\sum_{i=1}^n X_i X_i^\\top = \\mathbb X^\\top \\mathbb X$$\n",
    "We then define our linear regression estimator to be any $\\hat w$ satisfying\n",
    "$$\\mathbb X^\\top Y = \\mathbb X^\\top \\mathbb X \\hat w.$$\n",
    "\n",
    "Our linear regression estimator will be any $\\hat w$ satisfying\n",
    "Suppose now to have iid copies $(X_1,Y_1), \\ldots, (X_n,Y_n)$ of a random vector $(X_0,Y_0)$ with $X_0, Y_0 \\in \\R$.\n",
    "We know that the best approximation of $Y_0$ as a linear (well, affine) function of $X$ is given by $\n",
    "As before, we wish to find the best way to li\n",
    "\n",
    "## Proofs of basic facts stated at the beginning\n",
    "Write $Q(v) = E[||X - v ||^2]$. Take derivative with respect to $v$:\n",
    "$$D_v Q = E [D_v || X - v ||^2] = -2 E[(X - v)^\\top].$$\n",
    "The Hessian is\n",
    "$$\\text{Hess}_v Q = D_v (D_v Q)^\\top = 2 I$$\n",
    "which means it is a convex function, hence the unique critical point will be the global minimum.\n",
    "$$0 \\stackrel ! = D_v Q \\iff v = E[X].$$\n",
    "\n",
    "For the more general statement note that, given $X=x$, we know $E[||Y - g(x)||^2 | X=x]$ is minimized for $g(x) = E[Y|X=x]$.\n",
    "We know $E[||Y - g(X)||^2] = E[E[||Y-g(X)||^2|X]]$, hence $g(X) = E[Y|X]$ is the minimizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
