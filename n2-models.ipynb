{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "todo:\n",
    "* update score to include score function\n",
    "* do a standard U-net\n",
    "* create dataset class\n",
    "* train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models\n",
    "I just ran `convert_to_np.py`. Let's make sure everything went OK first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intersection over Union\n",
    "![](extra/iou.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class iou_module(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(iou_module,self).__init__()\n",
    "\n",
    "    def forward(self, y_pred, y):\n",
    "        loss = iou_loss(y_pred, y)\n",
    "        return loss\n",
    "    \n",
    "def iou_score(y_pred, y, SMOOTH=1e-6, rounding=False):\n",
    "    \"\"\"\n",
    "    aka Jaccard\n",
    "    expect: y_pred, y to be of SAME integer type\n",
    "    \n",
    "    y_pred is output of model\n",
    "        expect: y_pred.shape = (batch_len,D,D,S)\n",
    "        (no channels!)\n",
    "    y is truth value (labels)\n",
    "        expect: y.shape = (batch_len,D,D,S)\n",
    "    \n",
    "    returns: the mean across the batch of the iou scores\n",
    "    \"\"\"\n",
    "    # sanity check\n",
    "    assert y_pred.shape == y.shape\n",
    "    # to compute scores, we sum along all axes except for batch\n",
    "    axes = tuple([i for i in range(1,len(y.shape))])\n",
    "    batch_len = y.shape[0]\n",
    "    # if y_pred hasn't been rounded\n",
    "    if rounding:\n",
    "        y_pred = y_pred.round()\n",
    "    \n",
    "    intersection = (y_pred & y).sum(dim=axes).float()\n",
    "    union = (y_pred | y).sum(dim=axes).float()\n",
    "    # sanity check\n",
    "    assert intersection.shape == union.shape\n",
    "    assert union.shape == (batch_len,)\n",
    "    \n",
    "    iou = 1 - (intersection + SMOOTH) / (union + SMOOTH)\n",
    "    \n",
    "    return iou.mean()\n",
    "\n",
    "def iou_loss(y_pred, y, SMOOTH=1e-6):\n",
    "    \"\"\"\n",
    "    essentially returns 1 - iou_score\n",
    "    but takes care of y_pred not being rounded\n",
    "    \"\"\"\n",
    "    assert y_pred.shape == y.shape\n",
    "    axes = tuple([i for i in range(1,len(y.shape))])\n",
    "    batch_len = y.shape[0]\n",
    "    \n",
    "    numerator = y*y_pred\n",
    "    numerator = numerator.sum(dim=axes)\n",
    "    a = y.sum(dim=axes)\n",
    "    b = y.sum(dim=axes)\n",
    "    denominator = a + b - numerator\n",
    "    quotient = (numerator + SMOOTH) / (denominator + SMOOTH)\n",
    "    return quotient.mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulator(model,criterion,score_fun,batch_len=1,C=4,D=32,S=64):\n",
    "    \"\"\"\n",
    "    Creates source data x of shape (batch_len,C,D,D,S).\n",
    "    Creates target data y of shape (batch_len,D,D,S).\n",
    "    Creates prediction data y_pred using model(x).\n",
    "    Computes loss using criterion(y_pred,y).\n",
    "        NOTE: the order of arguments may matter.\n",
    "    \"\"\"\n",
    "    # simulate input data\n",
    "    x = torch.randn(batch_len,C,D,D,S)\n",
    "    print(f\"Input has shape {tuple(x.shape)}.\")\n",
    "    # simulate output data\n",
    "    y = torch.randn(batch_len,D,D,S)\n",
    "    y = torch.sigmoid(y).round()\n",
    "    print(f\"Target has shape {tuple(y.shape)}.\")\n",
    "    # simulate prediction for training\n",
    "    y_pred = model(x, evaluating=False)\n",
    "    print(f\"Training output has shape {tuple(y_pred.shape)}.\")\n",
    "    # compute loss\n",
    "    loss = criterion(y_pred,y)\n",
    "    print(f\"Loss is {loss.item()}.\")\n",
    "    \n",
    "    loss.backward()\n",
    "    print(\"Backward pass works.\")\n",
    "    \n",
    "    # simulate prediction for evaluating\n",
    "    y_pred = model(x, evaluating=True)\n",
    "    print(f\"Evaluation output has shape {tuple(y_pred.shape)}.\")\n",
    "    # convert to int type\n",
    "    # x.byte() is equivalent to x.to(dtype=torch.uint8)\n",
    "    # https://pytorch.org/docs/stable/tensors.html\n",
    "    y_pred = y_pred.byte()\n",
    "    y = y.byte()\n",
    "    score = score_fun(y_pred,y).item()\n",
    "    print(f\"Score is {score}.\")\n",
    "\n",
    "    # simulate segmentation comparison\n",
    "    y_pred = y_pred[0].cpu().detach().numpy()\n",
    "    y = y[0].cpu().detach().numpy()\n",
    "    \n",
    "    plt.figure(figsize=(10,10))\n",
    "    \n",
    "    plt.subplot(1,2,1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(y_pred[:,:,35])\n",
    "    \n",
    "    plt.subplot(1,2,2)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(y[:,:,35])\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pixel-by-pixel logistic regression (bad for your health)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pixelwise Logistic Regression (don't run)\n",
    "class PixLog(nn.Module):\n",
    "    def __init__(self,C=4,D=32,S=64):\n",
    "        super(PixLog,self).__init__()\n",
    "        \n",
    "        self.dimensions = (C,D,D,S)\n",
    "        self.fc = nn.Linear(in_features=C*D*D*S, out_features=D*D*S,bias=True)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x_in, evaluating=False):\n",
    "        batch_len = x_in.shape[0]\n",
    "        x = x_in.view(batch_len,-1)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        if evaluating:\n",
    "            x = self.sig(x)\n",
    "            x = x.round()\n",
    "        \n",
    "        dummy_dim = (-1,) + self.dimensions[1:]\n",
    "        x_out = x.view(dummy_dim)\n",
    "        return x_out\n",
    "\n",
    "# If you even try to initialize this, you're gonna have a bad time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive encoder-decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input has shape (1, 4, 32, 32, 64).\n",
      "Target has shape (1, 32, 32, 64).\n",
      "Training output has shape (1, 32, 32, 64).\n",
      "Loss is 0.6992213129997253.\n",
      "Backward pass works.\n",
      "Evaluation output has shape (1, 32, 32, 64).\n",
      "Score is 0.666673481464386.\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjwAAAEMCAYAAADAnWyqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAALrklEQVR4nO3czW3kyhUFYHVDQQy8n72TEByBolQERifh/eyNiaLp5duoSXlu33eLR9+3FMEfscjqgwJ4Ltu2vQAAJLtOXwAAQDeBBwCIJ/AAAPEEHgAgnsADAMQTeACAeK97G9+u77vfrP/7v//ZPfi//vHP3e1H+x/pPv7kuY+Of2T63lavv2rlsZ8e2yPXH78urSf4G03PYZNjPTk/fuX8R6bnmO77c2RyDp1+L6r73+4fn85hVngAgHgCDwAQT+ABAOIJPABAPIEHAIgn8AAA8QQeACDebg9P1XRHReVb/+menekOiCOr9zB0qo7NdP/J6s/WM60+VlV7Y736tU+/49NdaVWdHU2d557c3woPABBP4AEA4gk8AEA8gQcAiCfwAADxBB4AIJ7AAwDE2+3hme7Rqapcf3ePzOodGdMme3qSO4S+m9XnqKrOLpb053j6+s/eA1Qx1RVmhQcAiCfwAADxBB4AIJ7AAwDEE3gAgHgCDwAQT+ABAOLt9vBMf+ff/a1+Zf/VO4qmx+7I6vevU/e1Hd27o+23+zOvZm3dnUtVk+fvPvfZ7+3Kc2z12qfH5sifzmFWeACAeAIPABBP4AEA4gk8AEA8gQcAiCfwAADxBB4AIN5uD8+0M3e1dF/76j0JVd39KJX7P90PUnX2Z+P/Mf0eHkkei+k5avX3qHp/OnvkqlbtOLLCAwDEE3gAgHgCDwAQT+ABAOIJPABAPIEHAIgn8AAA8Uo9PNPf8h+pfOvf3QNTvXcr99R85fhVK/eXnL3/43YvHT7KVF/IM47fPcdU9++eo6Z/n848R02PXRcrPABAPIEHAIgn8AAA8QQeACCewAMAxBN4AIB4Ag8AEO+ybdvDjfffPx9vfOnvIVi9z2TP2e9N1XTPz2RHxupjc+R2/7hMX8OzvF3fd+ewbt3veeX4010qq/fkTN+fipWv7eXl7+gS+3wOs8IDAMQTeACAeAIPABBP4AEA4gk8AEA8gQcAiCfwAADxXis7T3dMdO6/ekfEtO7rm+yRqI7N6u/Fd3L2vqvJnp2q1Xtuzjy2R1bvsJvqcLLCAwDEE3gAgHgCDwAQT+ABAOIJPABAPIEHAIi3+1n61Kdjqxx/T/cnn6t/VnhkunKgYvqzcp6nu2Jg+tPmPd3XfuZ78/Iyf/1nrlU5Mv1s3e6f/90KDwAQT+ABAOIJPABAPIEHAIgn8AAA8QQeACCewAMAxLts2/Zw4/33z8cbv+DsPQx7ztwz8xXd/9+ZdY/tfIfFx6V0AQt5u76X5rAjqz8LlXMfWX0OO7vK/T3771P1/Ncfvz6dw6zwAADxBB4AIJ7AAwDEE3gAgHgCDwAQT+ABAOIJPABAvNfKztNdK9Uugcr1V3sIpnsQpsfuyJk7PL5zR9HZTD9n3Z1LnVafg6bfw+mumorprrB6l9jnf7fCAwDEE3gAgHgCDwAQT+ABAOIJPABAPIEHAIgn8AAA8XZ7eLq/te/umqmcf7qfY9r0/9/9bKws+X9bzXSfyJFKl9j0/JveN9V9fyv3v3rvp5+NrjnQCg8AEE/gAQDiCTwAQDyBBwCIJ/AAAPEEHgAgnsADAMTb7eGp6u5hmOxq6f7fqh0P1eN3mz7/nul7u3o3TJLqe3bmeaB7jqmev2r696e6f+X+dHcEnbWjzwoPABBP4AEA4gk8AEA8gQcAiCfwAADxBB4AIJ7AAwDEa+3hSTbVI/Cs8x/pvv7ujoxO3WPH80z3kazc8zPZY/YMq/cETT9bk+dedX63wgMAxBN4AIB4Ag8AEE/gAQDiCTwAQDyBBwCIJ/AAAPFae3i6exw6uwCmOyqqPQard8V09zhUjj/dMTTd/5Fk9T6qI5XjT/fATPfkTHeJrfwbsfJz+4z9H7HCAwDEE3gAgHgCDwAQT+ABAOIJPABAPIEHAIgn8AAA8S7btj3c+HZ9f7zxZe2ega/ovP7Vr326o+LIdE/P5Lmnx+Z2/7iUTrCQ7jms+z1f2XSf03Tf1cpWn6O635tHc5gVHgAgnsADAMQTeACAeAIPABBP4AEA4gk8AEA8gQcAiPfaefCzd9F0mu6A6D5/tYfhzP0mZ++n4i/TfSOTXWGr90UdmZ5jj3S+x6v3tE2NjRUeACCewAMAxBN4AIB4Ag8AEE/gAQDiCTwAQDyBBwCI19rDUzXZAzHdIXH2/o/p869sup9Kj89fpvtGOvuojs7dPb9OP+dV3c/Gkcr9rV77dI9P1723wgMAxBN4AIB4Ag8AEE/gAQDiCTwAQDyBBwCIJ/AAAPFae3hW74qp7L96z87qPTbTPT8V0/0bfN1kD85Xzt+p+x3pnoO659gj3XPwmZ+N6bH9U1Z4AIB4Ag8AEE/gAQDiCTwAQDyBBwCIJ/AAAPEEHgAgXmsPz5HpvpG9LoDuDoWz9yBMj930+fecvf8kSXffVbczd7WsPsd0H3/y2Zv+fTgy9Vxb4QEA4gk8AEA8gQcAiCfwAADxBB4AIJ7AAwDEE3gAgHi7PTzVnoDpLoDOLp3q/z59b6Z1j21lfKavjeepvmerdxpVusSmO4pWf0+670/n8afHpvv37U+Pb4UHAIgn8AAA8QQeACCewAMAxBN4AIB4Ag8AEE/gAQDi7fbwVL/l7+5RWLmLpbunoLsfZLJD4hkq51+1Q+JZ+38n089p91it/Cys3uVStXpPz8q6f19v98//boUHAIgn8AAA8QQeACCewAMAxBN4AIB4Ag8AEE/gAQDi7fbwHJnuAejsWZjukTl7B8P0/Vn12M+wejfMmXTfy8keoKNjd89x0z07Z+9Y6rT6/941dlZ4AIB4Ag8AEE/gAQDiCTwAQDyBBwCIJ/AAAPEEHgAgXqmHZ/UehMrx0zskjkyP3ZHO8Znu4ZnuMLrdW0+/lNX7RFae46bfk+l72/0bUbm+7ud6euz/lBUeACCewAMAxBN4AIB4Ag8AEE/gAQDiCTwAQLzLtm0PN95//3y88eX8n253f7pX4ZPRdT9br577yPT/fv3x61K6gIW8Xd9357DV6xcqpueAI2e/99PVIHumK1+mx/bRHGaFBwCIJ/AAAPEEHgAgnsADAMQTeACAeAIPABBP4AEA4u328KR3WOxd39l7CKaPP90l03nudLf7R0wPz1GX2JHpLrDK/iv3xCSY/g2onLvb9G+/Hh4A4NsSeACAeAIPABBP4AEA4gk8AEA8gQcAiCfwAADxXis7T39rP33+Tqv37ByZ7NH5isr/t3rH0Or3fiWr36vKe9r9HE13FB3pniPP3MF05uf6K273z/9uhQcAiCfwAADxBB4AIJ7AAwDEE3gAgHgCDwAQT+ABAOJdtm17uPHt+v544wKmuwQ6dXc4VHXf+8keiel+kOmxvd0/Lq0n+BtV57DpsaiY7sHptvK9f3mZ7+npND0HHrn++PXpHGaFBwCIJ/AAAPEEHgAgnsADAMQTeACAeAIPABBP4AEA4r3ubVz9W/vOHoPpa6uev7vDobp/9/9/5o6R6ffmO5nuqul8j6bfgekusDP33FTPf/axrR7/dv/871Z4AIB4Ag8AEE/gAQDiCTwAQDyBBwCIJ/AAAPEEHgAg3mXbtocb367vjzcuYLpDo2L62s5+/s4eiNU7gLqPf/3x61I6wELuv3/uzmFnH6vprpg93e9w+th09shNdyQdqY7NoznMCg8AEE/gAQDiCTwAQDyBBwCIJ/AAAPEEHgAgnsADAMRr7eFZvcdgUvL/9gxn7umZ7kapXt/t/vFteniqJrtWjvafvraq6d+PI909cJXrn+7BmZ6/H81hVngAgHgCDwAQT+ABAOIJPABAPIEHAIgn8AAA8QQeACDea2Xn6Z6GI50dFqv3EEzf++meoMr9nb72I93PXpLpLpYjne/52btYjkyP7fQcfeb3eOr3yQoPABBP4AEA4gk8AEA8gQcAiCfwAADxBB4AIJ7AAwDEu2zbNn0NAACtrPAAAPEEHgAgnsADAMQTeACAeAIPABBP4AEA4v0PTH9mZKm/plUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input has shape (1, 4, 32, 32, 64).\n",
      "Target has shape (1, 32, 32, 64).\n",
      "Training output has shape (1, 32, 32, 64).\n",
      "Loss is 0.0005863960832357407.\n",
      "Backward pass works.\n",
      "Evaluation output has shape (1, 32, 32, 64).\n",
      "Score is 0.6664770245552063.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# a simple encoderdecoder-type model\n",
    "class Crush(nn.Module):\n",
    "    def __init__(self,D=32,S=64,C=4,crush_size=32):\n",
    "        super(Crush,self).__init__()\n",
    "        \n",
    "        self.dimensions = (C,D,D,S)\n",
    "        self.in_features = int(D*D*S*C)\n",
    "        self.crush = crush_size\n",
    "        self.out_features = int(D*D*S)\n",
    "        self.enc = nn.Linear(in_features=self.in_features,out_features=self.crush,bias=True)\n",
    "        self.act = nn.ReLU()\n",
    "        self.dec = nn.Linear(in_features=self.crush,out_features=self.out_features)\n",
    "        self.sig = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x_in, evaluating=False):\n",
    "        batch_len = x_in.shape[0]\n",
    "        x = x_in.view(batch_len,-1)\n",
    "        x = self.enc(x)\n",
    "        x = self.act(x)\n",
    "        x = self.dec(x)\n",
    "        \n",
    "        if evaluating:\n",
    "            x = self.sig(x)\n",
    "            x = x.round()\n",
    "        \n",
    "        dummy_dim = (-1,) + self.dimensions[1:]\n",
    "        x_out = x.view(dummy_dim)\n",
    "        return x_out\n",
    "\n",
    "\n",
    "model = Crush()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "simulator(model=model,criterion=nn.BCEWithLogitsLoss(),score_fun=iou_score)\n",
    "simulator(model=model,criterion=iou_module(),score_fun=iou_score)\n",
    "# for loss = nn.BCEWithLogitsLoss()\n",
    "# should be of the form loss(y_pred,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple Sequential CNN\n",
    "![](extra/conv.gif)  \n",
    "Image from:\n",
    "https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input has shape (1, 4, 32, 32, 64).\n",
      "Target has shape (1, 32, 32, 64).\n",
      "Training output has shape (1, 32, 32, 64).\n",
      "Loss is 0.008604518137872219.\n",
      "Backward pass works.\n",
      "Evaluation output has shape (1, 32, 32, 64).\n",
      "Score is 0.5750231742858887.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# a simple sequence of convolutional layers\n",
    "class ConvSeq(nn.Module):\n",
    "    def __init__(self,input_channels=4):\n",
    "        super(ConvSeq,self).__init__()\n",
    "        \n",
    "        self.input_channels = input_channels\n",
    "        \n",
    "        self.c1 = self.ConvLayer(in_channels=self.input_channels)\n",
    "        self.c2 = self.ConvLayer()\n",
    "        self.c3 = self.ConvLayer()\n",
    "        self.cfinal = self.ConvLayer(out_channels=1, relu=False)\n",
    "    \n",
    "    def forward(self, x_in, evaluating=False):\n",
    "        x = self.c1(x_in)\n",
    "        x = self.c2(x)\n",
    "        x = self.c3(x)\n",
    "        x_out = self.cfinal(x).squeeze(1)\n",
    "        \n",
    "        if evaluating:\n",
    "            x_out = torch.sigmoid(x_out)\n",
    "            x_out = x_out.round()\n",
    "            \n",
    "        return x_out\n",
    "    \n",
    "    def ConvLayer(self, in_channels=32, out_channels=32, kernel_size=3, stride=1, padding=1, bias=True, relu=True):\n",
    "        layer = nn.Sequential()\n",
    "        conv = nn.Conv3d(in_channels=in_channels,\n",
    "                         out_channels=out_channels,\n",
    "                         kernel_size=kernel_size,\n",
    "                         padding=padding,\n",
    "                         bias=bias)\n",
    "        layer.add_module('conv',conv)\n",
    "        if relu:\n",
    "            layer.add_module('relu',nn.ReLU())\n",
    "\n",
    "        return layer\n",
    "\n",
    "\n",
    "model = ConvSeq()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "criterion = iou_module()\n",
    "simulator(model=model,criterion=criterion,score_fun=iou_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Small 3d U-net\n",
    "* Original U-net paper https://arxiv.org/abs/1505.04597\n",
    "* The skip connection concatenates tensors channel-wise.\n",
    "* Downsampling is MaxPool.  \n",
    "![](extra/maxpool.webp)\n",
    "* Upsampling is nearest neighbor. https://pytorch.org/docs/master/nn.functional.html#interpolate  \n",
    "![](extra/upsample.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input has shape (1, 4, 32, 32, 64).\n",
      "Target has shape (1, 32, 32, 64).\n",
      "Training output has shape (1, 32, 32, 64).\n",
      "Loss is 0.00027482715086080134.\n",
      "Backward pass works.\n",
      "Evaluation output has shape (1, 32, 32, 64).\n",
      "Score is 0.7494961023330688.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# small 3d u-net with concatenating skip connection\n",
    "class Small3dUcat(nn.Module):\n",
    "    def __init__(self,input_channels=4,num_filters=32):\n",
    "        super(Small3dUcat,self).__init__()\n",
    "        \n",
    "        # Structure:\n",
    "        # Conv,Conv,MaxPool,Conv,Conv,UnPool,Conv,Conv\n",
    "        \n",
    "        self.input_channels = input_channels\n",
    "        self.num_filters = num_filters\n",
    "        \n",
    "        self.c1 = self.ConvLayer(in_channels=self.input_channels,out_channels=num_filters)\n",
    "        self.c2 = self.ConvLayer(in_channels=num_filters,out_channels=num_filters)\n",
    "\n",
    "        self.c3 = self.ConvLayer(in_channels=num_filters,out_channels=num_filters)\n",
    "        self.c4 = self.ConvLayer(in_channels=num_filters,out_channels=num_filters)\n",
    "        \n",
    "        self.c5 = self.ConvLayer(in_channels=2*num_filters,out_channels=num_filters)\n",
    "        self.c6 = self.ConvLayer(in_channels=num_filters,out_channels=num_filters)\n",
    "        self.c7 = self.ConvLayer(in_channels=num_filters,out_channels=1)\n",
    "    \n",
    "    def forward(self, x_in, evaluating=False):\n",
    "        x1 = self.c1(x_in)\n",
    "        x1 = self.c2(x1)\n",
    "        \n",
    "        x2 = F.max_pool3d(x1,kernel_size=2)\n",
    "        x2 = self.c3(x2)\n",
    "        x2 = self.c4(x2)\n",
    "        \n",
    "        x2 = F.interpolate(x2, scale_factor=2)\n",
    "        # concatenate x1,x2\n",
    "        x = torch.cat([x1,x2],dim=1)\n",
    "        x = self.c5(x)\n",
    "        x = self.c6(x)\n",
    "        x = self.c7(x)\n",
    "        \n",
    "        x_out = x.squeeze(1)\n",
    "        x_out = torch.sigmoid(x_out)\n",
    "\n",
    "        return x_out\n",
    "    \n",
    "    def ConvLayer(self, in_channels=32, out_channels=32, kernel_size=3,\n",
    "                  stride=1, padding=1, bias=True, relu=True, batchnorm=True):\n",
    "        layer = nn.Sequential()\n",
    "        conv = nn.Conv3d(in_channels=in_channels,\n",
    "                         out_channels=out_channels,\n",
    "                         kernel_size=kernel_size,\n",
    "                         padding=padding,\n",
    "                         bias=bias)\n",
    "        layer.add_module('conv',conv)\n",
    "        if relu:\n",
    "            layer.add_module('relu',nn.ReLU())\n",
    "        if batchnorm:\n",
    "            layer.add_module('batchnorm',nn.BatchNorm3d(out_channels))\n",
    "\n",
    "        return layer\n",
    "\n",
    "\n",
    "model = Small3dUcat()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "criterion = iou_module()\n",
    "simulator(model=model,criterion=criterion,score_fun=iou_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Small 3d U-net again\n",
    "* Same as above except skip connection adds tensors instead of concatenating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input has shape (1, 4, 32, 32, 64).\n",
      "Target has shape (1, 32, 32, 64).\n",
      "Training output has shape (1, 32, 32, 64).\n",
      "Loss is 0.0007296002586372197.\n",
      "Backward pass works.\n",
      "Evaluation output has shape (1, 32, 32, 64).\n",
      "Score is 0.7510488629341125.\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjwAAAEMCAYAAADAnWyqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAALiUlEQVR4nO3dsY3jShYF0JHQQTTWH3+TaPwIOsqOYKEk1m9/MVGI31xHYulP6c0rXp1jDiGyWCSrLwjwzmnbth8AAMnO3QMAAKgm8AAA8QQeACCewAMAxBN4AIB4Ag8AEO9tb+P118/db9b/+te/d3f+n//9d3f76Pcj1fufMTu2lc9tBeanzuX6deoew7McfQ0bSb7PZ+dmZPbaz+5/1t74qu/r6t/POr9/31zDvOEBAOIJPABAPIEHAIgn8AAA8QQeACCewAMAxBN4AIB4uz083R0Ps9/qd3a1dPd7zFq9Z2Fk5vid/RmPHH/1fpCVdJ/ryteqemwrn/uPH/3PYef8pM/tPd7wAADxBB4AIJ7AAwDEE3gAgHgCDwAQT+ABAOIJPABAvN0enpHqb+1nuwJG++/uinllq/f47Jm9b4/eDZOkeg3q7jvZU72+dh9/9f2vbPU1YjT3l+vtf/eGBwCIJ/AAAPEEHgAgnsADAMQTeACAeAIPABBP4AEA4k318CT3EIxU98is3jHR/fuR5A6N6rm912FxREe/DyrHf/S5Galeo2bNHr+zg6n63qi6Nt7wAADxBB4AIJ7AAwDEE3gAgHgCDwAQT+ABAOIJPABAvNO2bXc3Xn/9vL/xAd1dKp09BbMdDt09Nq+s+tqt7nL9OnWP4Vk+zp+7a1h3n1Rnz85IdU9N9XPSvUZW9/jsOfoaNDt35/fvm2uYNzwAQDyBBwCIJ/AAAPEEHgAgnsADAMQTeACAeAIPABBvt4dn1GExUt2D0NkD0d2/Ud3x0N2hRJ+kHp7ZLrFZnc/xkdfXR46ffn4rW/3a3lvDvOEBAOIJPABAPIEHAIgn8AAA8QQeACCewAMAxBN4AIB4b5U77/5Wf/T7ve2jfc+eW3cPTffxZ3X2/KzeDzJy9Gv/J1WuMc/Yf2WX2NF1n9/K136k+r7uOndveACAeAIPABBP4AEA4gk8AEA8gQcAiCfwAADxBB4AIN5uD8/R+0ZmdHYgPHL8kdWvzZHvrdX7pfTs/F91j061mfFXd4lRa/bendl35bEf+X3V8b3hAQDiCTwAQDyBBwCIJ/AAAPEEHgAgnsADAMTb/Sw9/bPFmfOr/jS4e+5X/+x+ZT4bz7F6RcDe/ld/xl693qHy+NXnftQ1zBseACCewAMAxBN4AIB4Ag8AEE/gAQDiCTwAQDyBBwCId9q27e7Gj/Pn/Y0PqO45qNx/139f/6z9rzy3z1DZMdJ9brNm5+b8/n160lDaVa9hI9XPYeWxZ63eAzRSvQavPD/dHUSz7q1h3vAAAPEEHgAgnsADAMQTeACAeAIPABBP4AEA4gk8AEC8t+4BdNrrApjtIZjtYKjueBj9vrtDYnZ8IzPj7+5mqd7/5fqkgQTofo5X7hLrfka7u8q6Vd4b3XNX9ffHGx4AIJ7AAwDEE3gAgHgCDwAQT+ABAOIJPABAPIEHAIh32rbt7saP8+f9jT/6u1hmzYz/6B0Os6qvfaeV77tHzPfwfJ2eNJR2118/d9ew6r6pV++KmdHdd9V97SuPPWv1uTm/f99cw7zhAQDiCTwAQDyBBwCIJ/AAAPEEHgAgnsADAMQTeACAeG8zP+7uOZjtsJgZf3pHxKzq8ztyP0n3tX8ls3OdfK1mn7Hq3490rzGj36/cVdM999X7v1xv/7s3PABAPIEHAIgn8AAA8QQeACCewAMAxBN4AIB4Ag8AEG+qh2ekuoOiuifiyKo7MEZW7h+pVn3fJXfDPFv3uXZ2xVT3nFXr7npJ/vtV3TG0ageTNzwAQDyBBwCIJ/AAAPEEHgAgnsADAMQTeACAeAIPABBvt4enu8emswei+9xndfUcPKq7I6Tz+lWPffbevVz/8ZAOq3ouj3yfdnYI/Qnd17ZT9bmv2vPmDQ8AEE/gAQDiCTwAQDyBBwCIJ/AAAPEEHgAgnsADAMTb7eGZ7Uno7nLp7nlYWfe1GanuR5lR3TGR3P+RZuU1qHv9615jRsfvnp/KLrLqc1+9v+oeb3gAgHgCDwAQT+ABAOIJPABAPIEHAIgn8AAA8QQeACDeadu2uxs/zp/3Nx5AZV/JbI9BdU8Nuaq7Xy7Xr9PUDhZy9DVsZKaLZdWulEePP7JyR9IjZsbffW265/7eGuYNDwAQT+ABAOIJPABAPIEHAIgn8AAA8QQeACCewAMAxHvb29jdFTP7LX9lj0R1z8Gs7vGt3tNwZObmebqfk5HZNa5S9zPefe1W7jHSoXSbNzwAQDyBBwCIJ/AAAPEEHgAgnsADAMQTeACAeAIPABBvt4enW3XPw8p9JqNzq+6AmP396uPb+313/xTrWLVP5JH9V/eUVd/nq69RnvPfN7vG/u7vveEBAOIJPABAPIEHAIgn8AAA8QQeACCewAMAxBN4AIB4uz083R0UK3exzB571uy16R7/yvPT3S8yov/jcZ09OI/sv3J83T06IyuvAc/QOf/df7+6r+3levvfveEBAOIJPABAPIEHAIgn8AAA8QQeACCewAMAxBN4AIB4uz08s47eFTOjut+jWnc/ycq6z02Pz+Ne+T7u7AD6E1buQHpE5fhX7rh7RNXfP294AIB4Ag8AEE/gAQDiCTwAQDyBBwCIJ/AAAPEEHgAgXmkPT7XKnoTuDoruHoPu81+556F7bkZWH99KqvuuVu87qTx2dY9ad0/OyMo9cq/a0+MNDwAQT+ABAOIJPABAPIEHAIgn8AAA8QQeACCewAMAxCvt4enskHh11T0H1dcu+d7o7rDgcd1dKpXHT7+PVu+SGak8fvW5Vf9eDw8AwB0CDwAQT+ABAOIJPABAPIEHAIgn8AAA8QQeACDebg/PbMdEd8/DkXsMujsguvtHRirnp/u+Hem+N3hc9Towc+zu9b1zbh75/dHXwD3VHUSrzq03PABAPIEHAIgn8AAA8QQeACCewAMAxBN4AIB4p23b7m68/vp5f+MfcPRPu2d0fzI6q/qTzcrPGrs/l+2+7y/Xr9PUABbycf4sXcO615juT6MrHfXT50ePP9K5hlWrnvvz+/fNNcwbHgAgnsADAMQTeACAeAIPABBP4AEA4gk8AEA8gQcAiPe2t7G6x6C6Z6Gzi+DIY3+G7g6MkaPP757kc/unqntyVu6K6R5b9xpQfe1nx9/d1zVj9b9vl+vtf/eGBwCIJ/AAAPEEHgAgnsADAMQTeACAeAIPABBP4AEA4p22bbu78eP8eX/jj/wehz3VHQ5H71Lpvjf4fZfr16l7DM9SvYaNVK9Re+N79a6r1a9t5fG7536k+t48v3/fXMO84QEA4gk8AEA8gQcAiCfwAADxBB4AIJ7AAwDEE3gAgHhTPTzVunsO9nT3xKzesdHZP1Kt+76svvdeqYdnVvd9PnP87p601Z+D7ud8RvfYu/8+6eEBAF6WwAMAxBN4AIB4Ag8AEE/gAQDiCTwAQDyBBwCI99Y9gD2dXTajY3f3FHR3aIxU77+y56H62s/q7j/heVa+Ft1rTPfcdK9hM8fv/vtUbXR+l+vtf/eGBwCIJ/AAAPEEHgAgnsADAMQTeACAeAIPABBP4AEA4p22bbu78eP8eX8ju9I7KroducNi9Wt3uX6dusfwLKuvYdV9VzPHPnpXV/X+q3Ve21nVxx/t//z+fXMN84YHAIgn8AAA8QQeACCewAMAxBN4AIB4Ag8AEE/gAQDivXUP4FXNdkB09nesYHb8neevo2kdq8/lkfukuntuqo9fPb8z4z/6GjI7vsv19r97wwMAxBN4AIB4Ag8AEE/gAQDiCTwAQDyBBwCIJ/AAAPH08BTp7sCY1d0VU33+lf0l1f0k3R0Zr6T6Wq18Lbuf0e7nqLtHaOSVu8R+99p4wwMAxBN4AIB4Ag8AEE/gAQDiCTwAQDyBBwCIJ/AAAPFO27Z1jwEAoJQ3PABAPIEHAIgn8AAA8QQeACCewAMAxBN4AIB4fwNkvbMgHHXZhwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# small 3d u-net with addition skip connection\n",
    "class Small3dUadd(nn.Module):\n",
    "    def __init__(self,input_channels=4,num_filters=32):\n",
    "        super(Small3dUadd,self).__init__()\n",
    "        \n",
    "        # Structure:\n",
    "        # Conv,Conv,MaxPool,Conv,Conv,UnPool,Conv,Conv\n",
    "        \n",
    "        self.input_channels = input_channels\n",
    "        self.num_filters = num_filters\n",
    "        \n",
    "        self.c1 = self.ConvLayer(in_channels=self.input_channels,out_channels=num_filters)\n",
    "        self.c2 = self.ConvLayer(in_channels=num_filters,out_channels=num_filters)\n",
    "\n",
    "        self.c3 = self.ConvLayer(in_channels=num_filters,out_channels=num_filters)\n",
    "        self.c4 = self.ConvLayer(in_channels=num_filters,out_channels=num_filters)\n",
    "        \n",
    "        self.c5 = self.ConvLayer(in_channels=num_filters,out_channels=num_filters)\n",
    "        self.c6 = self.ConvLayer(in_channels=num_filters,out_channels=num_filters)\n",
    "        self.c7 = self.ConvLayer(in_channels=num_filters,out_channels=1)\n",
    "    \n",
    "    def forward(self, x_in, evaluating=False):\n",
    "        x1 = self.c1(x_in)\n",
    "        x1 = self.c2(x1)\n",
    "        \n",
    "        x2 = F.max_pool3d(x1,kernel_size=2)\n",
    "        x2 = self.c3(x2)\n",
    "        x2 = self.c4(x2)\n",
    "        \n",
    "        x2 = F.interpolate(x2, scale_factor=2)\n",
    "        # add x1,x2\n",
    "        x = F.relu(x1+x2)\n",
    "        x = self.c5(x)\n",
    "        x = self.c6(x)\n",
    "        x = self.c7(x)\n",
    "        \n",
    "        x_out = x.squeeze(1)\n",
    "        x_out = torch.sigmoid(x_out)\n",
    "\n",
    "        return x_out\n",
    "    \n",
    "    def ConvLayer(self, in_channels=32, out_channels=32, kernel_size=3,\n",
    "                  stride=1, padding=1, bias=True, relu=True, batchnorm=True):\n",
    "        layer = nn.Sequential()\n",
    "        conv = nn.Conv3d(in_channels=in_channels,\n",
    "                         out_channels=out_channels,\n",
    "                         kernel_size=kernel_size,\n",
    "                         padding=padding,\n",
    "                         bias=bias)\n",
    "        layer.add_module('conv',conv)\n",
    "        if relu:\n",
    "            layer.add_module('relu',nn.ReLU())\n",
    "        if batchnorm:\n",
    "            layer.add_module('batchnorm',nn.BatchNorm3d(out_channels))\n",
    "\n",
    "        return layer\n",
    "\n",
    "\n",
    "model = Small3dUadd()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "criterion = iou_module()\n",
    "simulator(model=model,criterion=criterion,score_fun=iou_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A proper U-net.\n",
    "![Alt text](suca.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a proper U-net\n",
    "# include picture of architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Double U-net  \n",
    "![](extra/uu.png)\n",
    "* Taken from https://arxiv.org/pdf/1701.03056.pdf (with minor differences)\n",
    "* Downsampling is Conv with 2x2 kernel, stride=2.\n",
    "* Has PReLU activations.\n",
    "* Upsampling is now the transpose of downsampling.  \n",
    "![](extra/tconv.gif)\n",
    "* Uses 1d convolutions to reduce channels.   \n",
    "![](extra/1dconv.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input has shape (1, 4, 32, 32, 64).\n",
      "Target has shape (1, 32, 32, 64).\n",
      "Training output has shape (1, 32, 32, 64).\n",
      "Loss is 0.15647375583648682.\n",
      "Backward pass works.\n",
      "Evaluation output has shape (1, 32, 32, 64).\n",
      "Score is 0.6155891418457031.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 3d UU-net\n",
    "class Big3dU(nn.Module):\n",
    "    def __init__(self,input_channels=4):\n",
    "        super(Big3dU,self).__init__()\n",
    "        \n",
    "        self.c0 = self.ConvLayer(in_channels=4,out_channels=8,\n",
    "                                 kernel_size=3,stride=1,padding=1)\n",
    "\n",
    "        self.d1 = self.ConvLayer(in_channels=8,out_channels=16,\n",
    "                                 stride=2,kernel_size=2,padding=0)\n",
    "        self.c1 = self.ConvLayer(in_channels=16,out_channels=16,\n",
    "                                kernel_size=3,stride=1,padding=1)\n",
    "\n",
    "        self.d2 = self.ConvLayer(in_channels=16,out_channels=32,\n",
    "                                 stride=2,kernel_size=2,padding=0)\n",
    "        self.c2 = self.ConvLayer(in_channels=32,out_channels=32,\n",
    "                                kernel_size=3,stride=1,padding=1)\n",
    "\n",
    "        self.d3 = self.ConvLayer(in_channels=32,out_channels=64,\n",
    "                                 stride=2,kernel_size=2,padding=0)\n",
    "        self.c3 = self.ConvLayer(in_channels=64,out_channels=64,\n",
    "                                kernel_size=3,stride=1,padding=1)\n",
    "        \n",
    "        self.f1 = self.ConvLayer(in_channels=64,out_channels=32,\n",
    "                                kernel_size=1,stride=1,padding=0)\n",
    "        \n",
    "        self.u1 = self.ConvLayer(in_channels=32,out_channels=32,\n",
    "                                kernel_size=2,stride=2,padding=0,transpose=True)\n",
    "        self.c4 = self.ConvLayer(in_channels=64,out_channels=32,\n",
    "                                 kernel_size=3,stride=1,padding=1)\n",
    "        self.f2 = self.ConvLayer(in_channels=32,out_channels=16,\n",
    "                                kernel_size=1,stride=1,padding=0)\n",
    "        \n",
    "        self.u2 = self.ConvLayer(in_channels=16,out_channels=16,\n",
    "                                kernel_size=2,stride=2,padding=0,transpose=True)\n",
    "        self.c5 = self.ConvLayer(in_channels=32,out_channels=16,\n",
    "                                kernel_size=3,stride=1,padding=1)\n",
    "        self.f3 = self.ConvLayer(in_channels=16,out_channels=8,\n",
    "                                kernel_size=1,stride=1,padding=0)\n",
    "        \n",
    "        self.u3 = self.ConvLayer(in_channels=8,out_channels=8,\n",
    "                                kernel_size=2,stride=2,padding=0,transpose=True)\n",
    "        self.c6 = self.ConvLayer(in_channels=16,out_channels=16,\n",
    "                                kernel_size=3,stride=1,padding=1)\n",
    "        \n",
    "        self.f4 = self.ConvLayer(in_channels=32,out_channels=16,\n",
    "                                kernel_size=1,stride=1,padding=0)\n",
    "        self.f5 = self.ConvLayer(in_channels=16,out_channels=8,\n",
    "                                kernel_size=1,stride=1,padding=0)\n",
    "        self.f6 = self.ConvLayer(in_channels=16,out_channels=1,\n",
    "                                kernel_size=1,stride=1,padding=0)\n",
    "        \n",
    "        self.u4 = self.ConvLayer(in_channels=16,out_channels=8,\n",
    "                                kernel_size=2,stride=2,padding=0,transpose=True)\n",
    "        self.u5 = self.ConvLayer(in_channels=8,out_channels=1,\n",
    "                                kernel_size=2,stride=2,padding=0,transpose=True)\n",
    "        \n",
    "        self.act = nn.PReLU(num_parameters=1)  \n",
    "\n",
    "    def forward(self, x_in, evaluating=False):\n",
    "        x0 = self.c0(x_in)\n",
    "        x1 = self.d1(x0)\n",
    "        x2 = self.c1(x1)\n",
    "        x3 = self.d2(x2+x1)\n",
    "        x4 = self.c2(x3)\n",
    "        x5 = self.d3(x4+x3)\n",
    "        x6 = self.c3(x5)\n",
    "        x7 = self.f1(x6+x5)\n",
    "        x8 = self.u1(x7)\n",
    "        x9 = self.c4(torch.cat([x8,x4],dim=1))\n",
    "        x10 = self.f2(x9)\n",
    "        x11 = self.u2(x10)\n",
    "        x12 = self.c5(torch.cat([x11,x2],dim=1))\n",
    "        x13 = self.f3(x12)\n",
    "        x14 = self.u3(x13)\n",
    "        x15 = self.f4(x9)\n",
    "        x16 = self.u4(x15)\n",
    "        x17 = self.f5(x12)\n",
    "        # x18 is missing, the graph I sketched had a box I did not use\n",
    "        x19 = self.u5(x16+x17)\n",
    "        x20 = self.c6(torch.cat([x14,x0],dim=1))\n",
    "        x21 = self.f6(x20)\n",
    "        x22 = self.act(x21+x19)\n",
    "\n",
    "        x_out = x22.squeeze(1)\n",
    "        x_out = torch.sigmoid(x_out)\n",
    "\n",
    "        return x_out\n",
    "\n",
    "    def ConvLayer(self,in_channels,out_channels,kernel_size,\n",
    "                  stride,padding,dilation=1,bias=True,prelu=True,batchnorm=True,transpose=False):\n",
    "        layer = nn.Sequential()\n",
    "        if transpose:\n",
    "            tconv = nn.ConvTranspose3d(in_channels=in_channels,\n",
    "                             out_channels=out_channels,\n",
    "                             kernel_size=kernel_size,\n",
    "                             stride=stride,\n",
    "                             padding=padding,\n",
    "                             dilation=dilation,\n",
    "                             bias=bias)\n",
    "            layer.add_module('tconv',tconv)\n",
    "        else:\n",
    "            conv = nn.Conv3d(in_channels=in_channels,\n",
    "                             out_channels=out_channels,\n",
    "                             kernel_size=kernel_size,\n",
    "                             stride=stride,\n",
    "                             padding=padding,\n",
    "                             dilation=dilation,\n",
    "                             bias=bias)\n",
    "            layer.add_module('conv',conv)\n",
    "\n",
    "        if batchnorm:\n",
    "            layer.add_module('batchnorm',nn.BatchNorm3d(num_features=out_channels))\n",
    "\n",
    "        if prelu:\n",
    "            layer.add_module('prelu',nn.PReLU(num_parameters=out_channels))\n",
    "\n",
    "        return layer\n",
    "\n",
    "model = Big3dU()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "criterion = iou_module()\n",
    "simulator(model=model,criterion=criterion,score_fun=iou_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
