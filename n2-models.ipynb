{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models\n",
    "I just ran `convert_to_np.py`. Let's make sure everything went OK first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intersection over Union\n",
    "![](extra/iou.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class iou_module(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(iou_module,self).__init__()\n",
    "\n",
    "    def forward(self, y_pred, y):\n",
    "        loss = iou_loss(y_pred, y)\n",
    "        return loss\n",
    "    \n",
    "def iou_score(y_pred, y, SMOOTH=1e-6, rounding=False):\n",
    "    \"\"\"\n",
    "    aka Jaccard\n",
    "    expect: y_pred, y to be of SAME integer type\n",
    "    \n",
    "    y_pred is output of model\n",
    "        expect: y_pred.shape = (batch_len,D,D,S)\n",
    "        (no channels!)\n",
    "    y is truth value (labels)\n",
    "        expect: y.shape = (batch_len,D,D,S)\n",
    "    \n",
    "    returns: the mean across the batch of the iou scores\n",
    "    \"\"\"\n",
    "    # sanity check\n",
    "    assert y_pred.shape == y.shape\n",
    "    # to compute scores, we sum along all axes except for batch\n",
    "    axes = tuple([i for i in range(1,len(y.shape))])\n",
    "    batch_len = y.shape[0]\n",
    "    # if y_pred hasn't been rounded\n",
    "    if rounding:\n",
    "        y_pred = y_pred.round()\n",
    "    \n",
    "    intersection = (y_pred & y).sum(dim=axes).float()\n",
    "    union = (y_pred | y).sum(dim=axes).float()\n",
    "    # sanity check\n",
    "    assert intersection.shape == union.shape\n",
    "    assert union.shape == (batch_len,)\n",
    "    \n",
    "    iou = 1 - (intersection + SMOOTH) / (union + SMOOTH)\n",
    "    \n",
    "    return iou.mean()\n",
    "\n",
    "def iou_loss(y_pred, y, SMOOTH=1e-6):\n",
    "    \"\"\"\n",
    "    essentially returns 1 - iou_score\n",
    "    but takes care of y_pred not being rounded\n",
    "    \"\"\"\n",
    "    assert y_pred.shape == y.shape\n",
    "    axes = tuple([i for i in range(1,len(y.shape))])\n",
    "    batch_len = y.shape[0]\n",
    "    \n",
    "    numerator = y*y_pred\n",
    "    numerator = numerator.sum(dim=axes)\n",
    "    a = y.sum(dim=axes)\n",
    "    b = y.sum(dim=axes)\n",
    "    denominator = a + b - numerator\n",
    "    quotient = (numerator + SMOOTH) / (denominator + SMOOTH)\n",
    "    return quotient.mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulator(model,criterion,score_fun,batch_len=1,C=4,D=32,S=64):\n",
    "    \"\"\"\n",
    "    Creates source data x of shape (batch_len,C,D,D,S).\n",
    "    Creates target data y of shape (batch_len,D,D,S).\n",
    "    Creates prediction data y_pred using model(x).\n",
    "    Computes loss using criterion(y_pred,y).\n",
    "        NOTE: the order of arguments may matter.\n",
    "    \"\"\"\n",
    "    # simulate input data\n",
    "    x = torch.randn(batch_len,C,D,D,S)\n",
    "    print(f\"Input has shape {tuple(x.shape)}.\")\n",
    "    # simulate output data\n",
    "    y = torch.randn(batch_len,D,D,S)\n",
    "    y = torch.sigmoid(y).round()\n",
    "    print(f\"Target has shape {tuple(y.shape)}.\")\n",
    "    # simulate prediction for training\n",
    "    y_pred = model(x, evaluating=False)\n",
    "    print(f\"Training output has shape {tuple(y_pred.shape)}.\")\n",
    "    # compute loss\n",
    "    loss = criterion(y_pred,y)\n",
    "    print(f\"Loss is {loss.item()}.\")\n",
    "    \n",
    "    loss.backward()\n",
    "    print(\"Backward pass works.\")\n",
    "    \n",
    "    # simulate prediction for evaluating\n",
    "    y_pred = model(x, evaluating=True)\n",
    "    print(f\"Evaluation output has shape {tuple(y_pred.shape)}.\")\n",
    "    # convert to int type\n",
    "    # x.byte() is equivalent to x.to(dtype=torch.uint8)\n",
    "    # https://pytorch.org/docs/stable/tensors.html\n",
    "    y_pred = y_pred.byte()\n",
    "    y = y.byte()\n",
    "    score = score_fun(y_pred,y).item()\n",
    "    print(f\"Score is {score}.\")\n",
    "\n",
    "    # simulate segmentation comparison\n",
    "    y_pred = y_pred[0].cpu().detach().numpy()\n",
    "    y = y[0].cpu().detach().numpy()\n",
    "    \n",
    "    plt.figure(figsize=(10,10))\n",
    "    \n",
    "    plt.subplot(1,2,1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(y_pred[:,:,35])\n",
    "    \n",
    "    plt.subplot(1,2,2)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(y[:,:,35])\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pixel-by-pixel logistic regression (bad for your health)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pixelwise Logistic Regression (don't run)\n",
    "class PixLog(nn.Module):\n",
    "    def __init__(self,C=4,D=32,S=64):\n",
    "        super(PixLog,self).__init__()\n",
    "        \n",
    "        self.dimensions = (C,D,D,S)\n",
    "        self.fc = nn.Linear(in_features=C*D*D*S, out_features=D*D*S,bias=True)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x_in, evaluating=False):\n",
    "        batch_len = x_in.shape[0]\n",
    "        x = x_in.view(batch_len,-1)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        if evaluating:\n",
    "            x = self.sig(x)\n",
    "            x = x.round()\n",
    "        \n",
    "        dummy_dim = (-1,) + self.dimensions[1:]\n",
    "        x_out = x.view(dummy_dim)\n",
    "        return x_out\n",
    "\n",
    "# If you even try to initialize this, you're gonna have a bad time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive encoder-decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input has shape (1, 4, 32, 32, 64).\n",
      "Target has shape (1, 32, 32, 64).\n",
      "Training output has shape (1, 32, 32, 64).\n",
      "Loss is 0.7039296627044678.\n",
      "Backward pass works.\n",
      "Evaluation output has shape (1, 32, 32, 64).\n",
      "Score is 0.6646970510482788.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input has shape (1, 4, 32, 32, 64).\n",
      "Target has shape (1, 32, 32, 64).\n",
      "Training output has shape (1, 32, 32, 64).\n",
      "Loss is -0.00013447196397464722.\n",
      "Backward pass works.\n",
      "Evaluation output has shape (1, 32, 32, 64).\n",
      "Score is 0.6695092916488647.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# a simple encoderdecoder-type model\n",
    "class Crush(nn.Module):\n",
    "    def __init__(self,D=32,S=64,C=4,crush_size=32):\n",
    "        super(Crush,self).__init__()\n",
    "        \n",
    "        self.dimensions = (C,D,D,S)\n",
    "        self.in_features = int(D*D*S*C)\n",
    "        self.crush = crush_size\n",
    "        self.out_features = int(D*D*S)\n",
    "        self.enc = nn.Linear(in_features=self.in_features,out_features=self.crush,bias=True)\n",
    "        self.act = nn.ReLU()\n",
    "        self.dec = nn.Linear(in_features=self.crush,out_features=self.out_features)\n",
    "        self.sig = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x_in, evaluating=False):\n",
    "        batch_len = x_in.shape[0]\n",
    "        x = x_in.view(batch_len,-1)\n",
    "        x = self.enc(x)\n",
    "        x = self.act(x)\n",
    "        x = self.dec(x)\n",
    "        \n",
    "        if evaluating:\n",
    "            x = self.sig(x)\n",
    "            x = x.round()\n",
    "        \n",
    "        dummy_dim = (-1,) + self.dimensions[1:]\n",
    "        x_out = x.view(dummy_dim)\n",
    "        return x_out\n",
    "\n",
    "\n",
    "model = Crush()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "simulator(model=model,criterion=nn.BCEWithLogitsLoss(),score_fun=iou_score)\n",
    "simulator(model=model,criterion=iou_module(),score_fun=iou_score)\n",
    "# for loss = nn.BCEWithLogitsLoss()\n",
    "# should be of the form loss(y_pred,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple Sequential CNN\n",
    "* 4 Convolution layers  \n",
    "![](extra/conv.gif)  \n",
    "Image from:\n",
    "https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md\n",
    "* ReLU activation\n",
    "* Batch normalization  \n",
    "![](extra/batchnorm.png)  \n",
    "Image from:\n",
    "https://medium.com/luminovo/a-refresher-on-batch-re-normalization-5e0a1e902960"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input has shape (1, 4, 32, 32, 64).\n",
      "Target has shape (1, 32, 32, 64).\n",
      "Training output has shape (1, 32, 32, 64).\n",
      "Loss is 0.008604518137872219.\n",
      "Backward pass works.\n",
      "Evaluation output has shape (1, 32, 32, 64).\n",
      "Score is 0.5750231742858887.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# a simple sequence of convolutional layers\n",
    "class ConvSeq(nn.Module):\n",
    "    def __init__(self,input_channels=4):\n",
    "        super(ConvSeq,self).__init__()\n",
    "        \n",
    "        self.input_channels = input_channels\n",
    "        \n",
    "        self.c1 = self.ConvLayer(in_channels=self.input_channels)\n",
    "        self.c2 = self.ConvLayer()\n",
    "        self.c3 = self.ConvLayer()\n",
    "        self.cfinal = self.ConvLayer(out_channels=1, relu=False)\n",
    "    \n",
    "    def forward(self, x_in, evaluating=False):\n",
    "        x = self.c1(x_in)\n",
    "        x = self.c2(x)\n",
    "        x = self.c3(x)\n",
    "        x_out = self.cfinal(x).squeeze(1)\n",
    "        \n",
    "        if evaluating:\n",
    "            x_out = torch.sigmoid(x_out)\n",
    "            x_out = x_out.round()\n",
    "            \n",
    "        return x_out\n",
    "    \n",
    "    def ConvLayer(self, in_channels=32, out_channels=32, kernel_size=3, stride=1, padding=1, bias=True, relu=True):\n",
    "        layer = nn.Sequential()\n",
    "        conv = nn.Conv3d(in_channels=in_channels,\n",
    "                         out_channels=out_channels,\n",
    "                         kernel_size=kernel_size,\n",
    "                         padding=padding,\n",
    "                         bias=bias)\n",
    "        layer.add_module('conv',conv)\n",
    "        if relu:\n",
    "            layer.add_module('relu',nn.ReLU())\n",
    "\n",
    "        return layer\n",
    "\n",
    "\n",
    "model = ConvSeq()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "criterion = iou_module()\n",
    "simulator(model=model,criterion=criterion,score_fun=iou_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shallow 3d U-net\n",
    "* Original U-net paper https://arxiv.org/abs/1505.04597\n",
    "* The skip connection concatenates tensors channel-wise.\n",
    "* Downsampling is MaxPool.  \n",
    "![](extra/maxpool.webp)\n",
    "* Upsampling is nearest neighbor. https://pytorch.org/docs/master/nn.functional.html#interpolate  \n",
    "![](extra/upsample.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input has shape (1, 4, 32, 32, 64).\n",
      "Target has shape (1, 32, 32, 64).\n",
      "Training output has shape (1, 32, 32, 64).\n",
      "Loss is 0.00027482715086080134.\n",
      "Backward pass works.\n",
      "Evaluation output has shape (1, 32, 32, 64).\n",
      "Score is 0.7494961023330688.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# small 3d u-net with concatenating skip connection\n",
    "class Small3dUcat(nn.Module):\n",
    "    def __init__(self,input_channels=4,num_filters=32):\n",
    "        super(Small3dUcat,self).__init__()\n",
    "        \n",
    "        # Structure:\n",
    "        # Conv,Conv,MaxPool,Conv,Conv,UnPool,Conv,Conv\n",
    "        \n",
    "        self.input_channels = input_channels\n",
    "        self.num_filters = num_filters\n",
    "        \n",
    "        self.c1 = self.ConvLayer(in_channels=self.input_channels,out_channels=num_filters)\n",
    "        self.c2 = self.ConvLayer(in_channels=num_filters,out_channels=num_filters)\n",
    "\n",
    "        self.c3 = self.ConvLayer(in_channels=num_filters,out_channels=num_filters)\n",
    "        self.c4 = self.ConvLayer(in_channels=num_filters,out_channels=num_filters)\n",
    "        \n",
    "        self.c5 = self.ConvLayer(in_channels=2*num_filters,out_channels=num_filters)\n",
    "        self.c6 = self.ConvLayer(in_channels=num_filters,out_channels=num_filters)\n",
    "        self.c7 = self.ConvLayer(in_channels=num_filters,out_channels=1)\n",
    "    \n",
    "    def forward(self, x_in, evaluating=False):\n",
    "        x1 = self.c1(x_in)\n",
    "        x1 = self.c2(x1)\n",
    "        \n",
    "        x2 = F.max_pool3d(x1,kernel_size=2)\n",
    "        x2 = self.c3(x2)\n",
    "        x2 = self.c4(x2)\n",
    "        \n",
    "        x2 = F.interpolate(x2, scale_factor=2)\n",
    "        # concatenate x1,x2\n",
    "        x = torch.cat([x1,x2],dim=1)\n",
    "        x = self.c5(x)\n",
    "        x = self.c6(x)\n",
    "        x = self.c7(x)\n",
    "        \n",
    "        x_out = x.squeeze(1)\n",
    "        x_out = torch.sigmoid(x_out)\n",
    "\n",
    "        return x_out\n",
    "    \n",
    "    def ConvLayer(self, in_channels=32, out_channels=32, kernel_size=3,\n",
    "                  stride=1, padding=1, bias=True, relu=True, batchnorm=True):\n",
    "        layer = nn.Sequential()\n",
    "        conv = nn.Conv3d(in_channels=in_channels,\n",
    "                         out_channels=out_channels,\n",
    "                         kernel_size=kernel_size,\n",
    "                         padding=padding,\n",
    "                         bias=bias)\n",
    "        layer.add_module('conv',conv)\n",
    "        if relu:\n",
    "            layer.add_module('relu',nn.ReLU())\n",
    "        if batchnorm:\n",
    "            layer.add_module('batchnorm',nn.BatchNorm3d(out_channels))\n",
    "\n",
    "        return layer\n",
    "\n",
    "\n",
    "model = Small3dUcat()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "criterion = iou_module()\n",
    "simulator(model=model,criterion=criterion,score_fun=iou_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shallow 3d U-net again\n",
    "* Same as above except skip connection adds tensors instead of concatenating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input has shape (1, 4, 32, 32, 64).\n",
      "Target has shape (1, 32, 32, 64).\n",
      "Training output has shape (1, 32, 32, 64).\n",
      "Loss is 0.3324456214904785.\n",
      "Backward pass works.\n",
      "Evaluation output has shape (1, 32, 32, 64).\n",
      "Score is 1.0.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# small 3d u-net with addition skip connection\n",
    "class Small3dUadd(nn.Module):\n",
    "    def __init__(self,input_channels=4,num_filters=32):\n",
    "        super(Small3dUadd,self).__init__()\n",
    "        \n",
    "        # Structure:\n",
    "        # Conv,Conv,MaxPool,Conv,Conv,UnPool,Conv,Conv\n",
    "        \n",
    "        self.input_channels = input_channels\n",
    "        self.num_filters = num_filters\n",
    "        \n",
    "        self.c1 = self.ConvLayer(in_channels=self.input_channels,out_channels=num_filters)\n",
    "        self.c2 = self.ConvLayer(in_channels=num_filters,out_channels=num_filters)\n",
    "\n",
    "        self.c3 = self.ConvLayer(in_channels=num_filters,out_channels=num_filters)\n",
    "        self.c4 = self.ConvLayer(in_channels=num_filters,out_channels=num_filters)\n",
    "        \n",
    "        self.c5 = self.ConvLayer(in_channels=num_filters,out_channels=num_filters)\n",
    "        self.c6 = self.ConvLayer(in_channels=num_filters,out_channels=num_filters)\n",
    "        self.c7 = self.ConvLayer(in_channels=num_filters,out_channels=1)\n",
    "    \n",
    "    def forward(self, x_in, evaluating=False):\n",
    "        x1 = self.c1(x_in)\n",
    "        x1 = self.c2(x1)\n",
    "        \n",
    "        x2 = F.max_pool3d(x1,kernel_size=2)\n",
    "        x2 = self.c3(x2)\n",
    "        x2 = self.c4(x2)\n",
    "        \n",
    "        x2 = F.interpolate(x2, scale_factor=2)\n",
    "        # add x1,x2\n",
    "        x = F.relu(x1+x2)\n",
    "        x = self.c5(x)\n",
    "        x = self.c6(x)\n",
    "        x = self.c7(x)\n",
    "        \n",
    "        x_out = x.squeeze(1)\n",
    "        x_out = torch.sigmoid(x_out)\n",
    "\n",
    "        return x_out\n",
    "    \n",
    "    def ConvLayer(self, in_channels=32, out_channels=32, kernel_size=3,\n",
    "                  stride=1, padding=1, bias=True, relu=True, batchnorm=True):\n",
    "        layer = nn.Sequential()\n",
    "        conv = nn.Conv3d(in_channels=in_channels,\n",
    "                         out_channels=out_channels,\n",
    "                         kernel_size=kernel_size,\n",
    "                         padding=padding,\n",
    "                         bias=bias)\n",
    "        layer.add_module('conv',conv)\n",
    "        if relu:\n",
    "            layer.add_module('relu',nn.ReLU())\n",
    "        if batchnorm:\n",
    "            layer.add_module('batchnorm',nn.BatchNorm3d(out_channels))\n",
    "\n",
    "        return layer\n",
    "\n",
    "\n",
    "model = Small3dUadd()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "criterion = iou_module()\n",
    "simulator(model=model,criterion=criterion,score_fun=iou_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A proper U-net.\n",
    "![](extra/u.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input has shape (1, 4, 32, 32, 64).\n",
      "Target has shape (1, 32, 32, 64).\n",
      "Training output has shape (1, 32, 32, 64).\n",
      "Loss is 1.2631392564799171e-05.\n",
      "Backward pass works.\n",
      "Evaluation output has shape (1, 32, 32, 64).\n",
      "Score is 1.0.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# a proper U-net\n",
    "# include picture of architecture\n",
    "# small 3d u-net with addition skip connection\n",
    "class Unet3d(nn.Module):\n",
    "    def __init__(self,input_channels=4,num_filters=64):\n",
    "        super(Unet3d,self).__init__()\n",
    "        \n",
    "        # Structure:\n",
    "        # cccmcccmcccmcccmcccucccucccucccucccf\n",
    "        \n",
    "        self.input_channels = input_channels\n",
    "        c = self.input_channels\n",
    "        self.num_filters = num_filters\n",
    "        n = self.num_filters\n",
    "        \n",
    "        # down\n",
    "        self.c1 = self.ConvLayer(in_channels=c,out_channels=n)\n",
    "        self.c2 = self.ConvLayer(in_channels=n,out_channels=n)\n",
    "        self.c3 = self.ConvLayer(in_channels=n,out_channels=n)\n",
    "\n",
    "        self.c4 = self.ConvLayer(in_channels=n,out_channels=2*n)\n",
    "        self.c5 = self.ConvLayer(in_channels=2*n,out_channels=2*n)\n",
    "        self.c6 = self.ConvLayer(in_channels=2*n,out_channels=2*n)\n",
    "\n",
    "        self.c7 = self.ConvLayer(in_channels=2*n,out_channels=4*n)\n",
    "        self.c8 = self.ConvLayer(in_channels=4*n,out_channels=4*n)\n",
    "        self.c9 = self.ConvLayer(in_channels=4*n,out_channels=4*n)\n",
    "\n",
    "        self.c10 = self.ConvLayer(in_channels=4*n,out_channels=8*n)\n",
    "        self.c11 = self.ConvLayer(in_channels=8*n,out_channels=8*n)\n",
    "        self.c12 = self.ConvLayer(in_channels=8*n,out_channels=8*n)\n",
    "\n",
    "        self.c13 = self.ConvLayer(in_channels=8*n,out_channels=16*n)\n",
    "        self.c14 = self.ConvLayer(in_channels=16*n,out_channels=16*n)\n",
    "        self.c15 = self.ConvLayer(in_channels=16*n,out_channels=16*n)\n",
    "        \n",
    "        # up\n",
    "        self.c16 = self.ConvLayer(in_channels=16*n+8*n,out_channels=8*n)\n",
    "        self.c17 = self.ConvLayer(in_channels=8*n,out_channels=8*n)\n",
    "        self.c18 = self.ConvLayer(in_channels=8*n,out_channels=8*n)\n",
    "\n",
    "        \n",
    "        self.c19 = self.ConvLayer(in_channels=8*n+4*n,out_channels=4*n)\n",
    "        self.c20 = self.ConvLayer(in_channels=4*n,out_channels=4*n)\n",
    "        self.c21 = self.ConvLayer(in_channels=4*n,out_channels=4*n)\n",
    "\n",
    "        \n",
    "        self.c22 = self.ConvLayer(in_channels=4*n+2*n,out_channels=2*n)\n",
    "        self.c23 = self.ConvLayer(in_channels=2*n,out_channels=2*n)\n",
    "        self.c24 = self.ConvLayer(in_channels=2*n,out_channels=2*n)\n",
    "\n",
    "        self.c25 = self.ConvLayer(in_channels=2*n+n,out_channels=n)\n",
    "        self.c26 = self.ConvLayer(in_channels=n,out_channels=n)\n",
    "        self.c27 = self.ConvLayer(in_channels=n,out_channels=n)\n",
    "        \n",
    "        self.f = self.ConvLayer(in_channels=n,out_channels=1,\n",
    "                                kernel_size=1,padding=0)\n",
    "\n",
    "    def forward(self, x_in, evaluating=False):\n",
    "        x = self.c1(x_in)\n",
    "        x = self.c2(x)\n",
    "        x3 = self.c3(x)\n",
    "        \n",
    "        x = F.max_pool3d(x3,kernel_size=2)\n",
    "        x = self.c4(x)\n",
    "        x = self.c5(x)\n",
    "        x6 = self.c6(x)\n",
    "        \n",
    "        x = F.max_pool3d(x6,kernel_size=2)\n",
    "        x = self.c7(x)\n",
    "        x = self.c8(x)\n",
    "        x9 = self.c9(x)\n",
    "        \n",
    "        x = F.max_pool3d(x9,kernel_size=2)\n",
    "        x = self.c10(x)\n",
    "        x = self.c11(x)\n",
    "        x12 = self.c12(x)\n",
    "        \n",
    "        x = F.max_pool3d(x12,kernel_size=2)\n",
    "        x = self.c13(x)\n",
    "        x = self.c14(x)\n",
    "        x = self.c15(x)\n",
    "        \n",
    "        x = F.interpolate(x, scale_factor=2)\n",
    "        x = torch.cat([x,x12],dim=1)\n",
    "        x = self.c16(x)\n",
    "        x = self.c17(x)\n",
    "        x = self.c18(x)\n",
    "        \n",
    "        x = F.interpolate(x,scale_factor=2)\n",
    "        x = torch.cat([x,x9],dim=1)\n",
    "        x = self.c19(x)\n",
    "        x = self.c20(x)\n",
    "        x = self.c21(x)\n",
    "        \n",
    "        x = F.interpolate(x,scale_factor=2)\n",
    "        x = torch.cat([x,x6],dim=1)\n",
    "        x = self.c22(x)\n",
    "        x = self.c23(x)\n",
    "        x = self.c24(x)\n",
    "        \n",
    "        x = F.interpolate(x,scale_factor=2)\n",
    "        x = torch.cat([x,x3],dim=1)\n",
    "        x = self.c25(x)\n",
    "        x = self.c26(x)\n",
    "        x = self.c27(x)\n",
    "        \n",
    "        x = self.f(x)\n",
    "        x_out = x.squeeze(1)\n",
    "\n",
    "        return x_out\n",
    "        \n",
    "#         # remove the sigmoid when copying to models.py\n",
    "#         x_out = x.squeeze(1)\n",
    "#         x_out = torch.sigmoid(x_out)\n",
    "\n",
    "        return x_out\n",
    "    \n",
    "    def ConvLayer(self, in_channels=32, out_channels=32, kernel_size=3,\n",
    "                  stride=1, padding=1, bias=True, relu=True, batchnorm=True):\n",
    "        layer = nn.Sequential()\n",
    "        conv = nn.Conv3d(in_channels=in_channels,\n",
    "                         out_channels=out_channels,\n",
    "                         kernel_size=kernel_size,\n",
    "                         padding=padding,\n",
    "                         bias=bias)\n",
    "        layer.add_module('conv',conv)\n",
    "        if relu:\n",
    "            layer.add_module('relu',nn.ReLU())\n",
    "        if batchnorm:\n",
    "            layer.add_module('batchnorm',nn.BatchNorm3d(out_channels))\n",
    "\n",
    "        return layer\n",
    "\n",
    "\n",
    "model = Unet3d()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "criterion = iou_module()\n",
    "simulator(model=model,criterion=criterion,score_fun=iou_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Double U-net  \n",
    "![](extra/uu.png)  \n",
    "Taken from https://arxiv.org/pdf/1701.03056.pdf (with minor differences)  \n",
    "![](extra/myuu.png)\n",
    "* Downsampling is Conv with 2x2 kernel, stride=2.\n",
    "* Has PReLU activations.\n",
    "* Upsampling is now the transpose of downsampling.  \n",
    "![](extra/tconv.gif)\n",
    "* Uses 1d convolutions to reduce channels.   \n",
    "![](extra/1dconv.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input has shape (1, 4, 32, 32, 64).\n",
      "Target has shape (1, 32, 32, 64).\n",
      "Training output has shape (1, 32, 32, 64).\n",
      "Loss is 0.15647375583648682.\n",
      "Backward pass works.\n",
      "Evaluation output has shape (1, 32, 32, 64).\n",
      "Score is 0.6155891418457031.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 3d UU-net\n",
    "class Big3dU(nn.Module):\n",
    "    def __init__(self,input_channels=4):\n",
    "        super(Big3dU,self).__init__()\n",
    "        \n",
    "        self.c0 = self.ConvLayer(in_channels=4,out_channels=8,\n",
    "                                 kernel_size=3,stride=1,padding=1)\n",
    "\n",
    "        self.d1 = self.ConvLayer(in_channels=8,out_channels=16,\n",
    "                                 stride=2,kernel_size=2,padding=0)\n",
    "        self.c1 = self.ConvLayer(in_channels=16,out_channels=16,\n",
    "                                kernel_size=3,stride=1,padding=1)\n",
    "\n",
    "        self.d2 = self.ConvLayer(in_channels=16,out_channels=32,\n",
    "                                 stride=2,kernel_size=2,padding=0)\n",
    "        self.c2 = self.ConvLayer(in_channels=32,out_channels=32,\n",
    "                                kernel_size=3,stride=1,padding=1)\n",
    "\n",
    "        self.d3 = self.ConvLayer(in_channels=32,out_channels=64,\n",
    "                                 stride=2,kernel_size=2,padding=0)\n",
    "        self.c3 = self.ConvLayer(in_channels=64,out_channels=64,\n",
    "                                kernel_size=3,stride=1,padding=1)\n",
    "        \n",
    "        self.f1 = self.ConvLayer(in_channels=64,out_channels=32,\n",
    "                                kernel_size=1,stride=1,padding=0)\n",
    "        \n",
    "        self.u1 = self.ConvLayer(in_channels=32,out_channels=32,\n",
    "                                kernel_size=2,stride=2,padding=0,transpose=True)\n",
    "        self.c4 = self.ConvLayer(in_channels=64,out_channels=32,\n",
    "                                 kernel_size=3,stride=1,padding=1)\n",
    "        self.f2 = self.ConvLayer(in_channels=32,out_channels=16,\n",
    "                                kernel_size=1,stride=1,padding=0)\n",
    "        \n",
    "        self.u2 = self.ConvLayer(in_channels=16,out_channels=16,\n",
    "                                kernel_size=2,stride=2,padding=0,transpose=True)\n",
    "        self.c5 = self.ConvLayer(in_channels=32,out_channels=16,\n",
    "                                kernel_size=3,stride=1,padding=1)\n",
    "        self.f3 = self.ConvLayer(in_channels=16,out_channels=8,\n",
    "                                kernel_size=1,stride=1,padding=0)\n",
    "        \n",
    "        self.u3 = self.ConvLayer(in_channels=8,out_channels=8,\n",
    "                                kernel_size=2,stride=2,padding=0,transpose=True)\n",
    "        self.c6 = self.ConvLayer(in_channels=16,out_channels=16,\n",
    "                                kernel_size=3,stride=1,padding=1)\n",
    "        \n",
    "        self.f4 = self.ConvLayer(in_channels=32,out_channels=16,\n",
    "                                kernel_size=1,stride=1,padding=0)\n",
    "        self.f5 = self.ConvLayer(in_channels=16,out_channels=8,\n",
    "                                kernel_size=1,stride=1,padding=0)\n",
    "        self.f6 = self.ConvLayer(in_channels=16,out_channels=1,\n",
    "                                kernel_size=1,stride=1,padding=0)\n",
    "        \n",
    "        self.u4 = self.ConvLayer(in_channels=16,out_channels=8,\n",
    "                                kernel_size=2,stride=2,padding=0,transpose=True)\n",
    "        self.u5 = self.ConvLayer(in_channels=8,out_channels=1,\n",
    "                                kernel_size=2,stride=2,padding=0,transpose=True)\n",
    "        \n",
    "        self.act = nn.PReLU(num_parameters=1)  \n",
    "\n",
    "    def forward(self, x_in, evaluating=False):\n",
    "        x0 = self.c0(x_in)\n",
    "        x1 = self.d1(x0)\n",
    "        x2 = self.c1(x1)\n",
    "        x3 = self.d2(x2+x1)\n",
    "        x4 = self.c2(x3)\n",
    "        x5 = self.d3(x4+x3)\n",
    "        x6 = self.c3(x5)\n",
    "        x7 = self.f1(x6+x5)\n",
    "        x8 = self.u1(x7)\n",
    "        x9 = self.c4(torch.cat([x8,x4],dim=1))\n",
    "        x10 = self.f2(x9)\n",
    "        x11 = self.u2(x10)\n",
    "        x12 = self.c5(torch.cat([x11,x2],dim=1))\n",
    "        x13 = self.f3(x12)\n",
    "        x14 = self.u3(x13)\n",
    "        x15 = self.f4(x9)\n",
    "        x16 = self.u4(x15)\n",
    "        x17 = self.f5(x12)\n",
    "        # x18 is missing, the graph I sketched had a box I did not use\n",
    "        x19 = self.u5(x16+x17)\n",
    "        x20 = self.c6(torch.cat([x14,x0],dim=1))\n",
    "        x21 = self.f6(x20)\n",
    "        x22 = self.act(x21+x19)\n",
    "\n",
    "        x_out = x22.squeeze(1)\n",
    "        x_out = torch.sigmoid(x_out)\n",
    "\n",
    "        return x_out\n",
    "\n",
    "    def ConvLayer(self,in_channels,out_channels,kernel_size,\n",
    "                  stride,padding,dilation=1,bias=True,prelu=True,batchnorm=True,transpose=False):\n",
    "        layer = nn.Sequential()\n",
    "        if transpose:\n",
    "            tconv = nn.ConvTranspose3d(in_channels=in_channels,\n",
    "                             out_channels=out_channels,\n",
    "                             kernel_size=kernel_size,\n",
    "                             stride=stride,\n",
    "                             padding=padding,\n",
    "                             dilation=dilation,\n",
    "                             bias=bias)\n",
    "            layer.add_module('tconv',tconv)\n",
    "        else:\n",
    "            conv = nn.Conv3d(in_channels=in_channels,\n",
    "                             out_channels=out_channels,\n",
    "                             kernel_size=kernel_size,\n",
    "                             stride=stride,\n",
    "                             padding=padding,\n",
    "                             dilation=dilation,\n",
    "                             bias=bias)\n",
    "            layer.add_module('conv',conv)\n",
    "\n",
    "        if batchnorm:\n",
    "            layer.add_module('batchnorm',nn.BatchNorm3d(num_features=out_channels))\n",
    "\n",
    "        if prelu:\n",
    "            layer.add_module('prelu',nn.PReLU(num_parameters=out_channels))\n",
    "\n",
    "        return layer\n",
    "\n",
    "model = Big3dU()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "criterion = iou_module()\n",
    "simulator(model=model,criterion=criterion,score_fun=iou_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
