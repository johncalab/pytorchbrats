{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models\n",
    "I just ran `convert_to_np.py`. Let's make sure everything went OK first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intersection over Union\n",
    "![](extra/iou.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class iou_module(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(iou_module,self).__init__()\n",
    "\n",
    "    def forward(self, y_pred, y):\n",
    "        loss = iou_loss(y_pred, y)\n",
    "        return loss\n",
    "    \n",
    "def iou_score(y_pred, y, SMOOTH=1e-6, rounding=False):\n",
    "    \"\"\"\n",
    "    aka Jaccard\n",
    "    expect: y_pred, y to be of SAME integer type\n",
    "    \n",
    "    y_pred is output of model\n",
    "        expect: y_pred.shape = (batch_len,D,D,S)\n",
    "        (no channels!)\n",
    "    y is truth value (labels)\n",
    "        expect: y.shape = (batch_len,D,D,S)\n",
    "    \n",
    "    returns: the mean across the batch of the iou scores\n",
    "    \"\"\"\n",
    "    # sanity check\n",
    "    assert y_pred.shape == y.shape\n",
    "    # to compute scores, we sum along all axes except for batch\n",
    "    axes = tuple([i for i in range(1,len(y.shape))])\n",
    "    batch_len = y.shape[0]\n",
    "    # if y_pred hasn't been rounded\n",
    "    if rounding:\n",
    "        y_pred = y_pred.round()\n",
    "    \n",
    "    intersection = (y_pred & y).sum(dim=axes).float()\n",
    "    union = (y_pred | y).sum(dim=axes).float()\n",
    "    # sanity check\n",
    "    assert intersection.shape == union.shape\n",
    "    assert union.shape == (batch_len,)\n",
    "    \n",
    "    iou = 1 - (intersection + SMOOTH) / (union + SMOOTH)\n",
    "    \n",
    "    return iou.mean()\n",
    "\n",
    "def iou_loss(y_pred, y, SMOOTH=1e-6):\n",
    "    \"\"\"\n",
    "    essentially returns 1 - iou_score\n",
    "    but takes care of y_pred not being rounded\n",
    "    \"\"\"\n",
    "    assert y_pred.shape == y.shape\n",
    "    axes = tuple([i for i in range(1,len(y.shape))])\n",
    "    batch_len = y.shape[0]\n",
    "    \n",
    "    numerator = y*y_pred\n",
    "    numerator = numerator.sum(dim=axes)\n",
    "    a = y.sum(dim=axes)\n",
    "    b = y.sum(dim=axes)\n",
    "    denominator = a + b - numerator\n",
    "    quotient = (numerator + SMOOTH) / (denominator + SMOOTH)\n",
    "    return quotient.mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulator(model,criterion,score_fun,batch_len=1,C=4,D=32,S=64,sigm=True):\n",
    "    \"\"\"\n",
    "    Creates source data x of shape (batch_len,C,D,D,S).\n",
    "    Creates target data y of shape (batch_len,D,D,S).\n",
    "    Creates prediction data y_pred using model(x).\n",
    "    Computes loss using criterion(y_pred,y).\n",
    "        NOTE: the order of arguments may matter.\n",
    "    \"\"\"\n",
    "    # simulate input data\n",
    "    x = torch.randn(batch_len,C,D,D,S)\n",
    "    print(f\"Input has shape {tuple(x.shape)}.\")\n",
    "    # simulate output data\n",
    "    y = torch.randn(batch_len,D,D,S)\n",
    "    y = torch.sigmoid(y).round()\n",
    "    print(f\"Target has shape {tuple(y.shape)}.\")\n",
    "    # simulate prediction for training\n",
    "    y_pred = model(x)\n",
    "    print(f\"Training output has shape {tuple(y_pred.shape)}.\")\n",
    "    # compute loss\n",
    "    if sigm:\n",
    "        y_pred = torch.sigmoid(y_pred)\n",
    "    loss = criterion(y_pred,y)\n",
    "    print(f\"Loss is {loss.item()}.\")\n",
    "    \n",
    "    loss.backward()\n",
    "    print(\"Backward pass works.\")\n",
    "    \n",
    "    # simulate prediction for evaluating\n",
    "    y_pred = model(x)\n",
    "    if sigm:\n",
    "        y_pred = torch.sigmoid(y_pred)\n",
    "    y_pred = y_pred.round()\n",
    "    print(f\"Evaluation output has shape {tuple(y_pred.shape)}.\")\n",
    "    # convert to int type\n",
    "    # x.byte() is equivalent to x.to(dtype=torch.uint8)\n",
    "    # https://pytorch.org/docs/stable/tensors.html\n",
    "    y_pred = y_pred.byte()\n",
    "    y = y.byte()\n",
    "    score = score_fun(y_pred,y).item()\n",
    "    print(f\"Score is {score}.\")\n",
    "\n",
    "    # simulate segmentation comparison\n",
    "    y_pred = y_pred[0].cpu().detach().numpy()\n",
    "    y = y[0].cpu().detach().numpy()\n",
    "    \n",
    "    plt.figure(figsize=(10,10))\n",
    "    \n",
    "    plt.subplot(1,2,1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(y_pred[:,:,35])\n",
    "    \n",
    "    plt.subplot(1,2,2)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(y[:,:,35])\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pixel-by-pixel logistic regression (bad for your health)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pixelwise Logistic Regression (don't run)\n",
    "class PixLog(nn.Module):\n",
    "    def __init__(self,C=4,D=32,S=64):\n",
    "        super(PixLog,self).__init__()\n",
    "        \n",
    "        self.dimensions = (C,D,D,S)\n",
    "        self.fc = nn.Linear(in_features=C*D*D*S, out_features=D*D*S,bias=True)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x_in):\n",
    "        batch_len = x_in.shape[0]\n",
    "        x = x_in.view(batch_len,-1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        dummy_dim = (-1,) + self.dimensions[1:]\n",
    "        x_out = x.view(dummy_dim)\n",
    "        return x_out\n",
    "    \n",
    "# If you even try to initialize this, you're gonna have a bad time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive encoder-decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input has shape (1, 4, 32, 32, 64).\n",
      "Target has shape (1, 32, 32, 64).\n",
      "Training output has shape (1, 32, 32, 64).\n",
      "Loss is 0.7046499252319336.\n",
      "Backward pass works.\n",
      "Evaluation output has shape (1, 32, 32, 64).\n",
      "Score is 0.9962634444236755.\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjwAAAEMCAYAAADAnWyqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAALiklEQVR4nO3cQW7cyBkFYLWiQxjx2vv4EIJPoBxydAKjD+HsZ+3Al0h3djMbqUqj0j9/8fX3LdUgWawiqQcCfKfr9XoHAJDsvnsAAADVBB4AIJ7AAwDEE3gAgHgCDwAQT+ABAOI9jH58vH/a+5v102n8e+En99//+5/h79/++a+yYx+B+RlovG7f4nx5ngzwOC6/vixNZvV1OrtPZlbGV32Prp7bTPczpPsZtzK/s7Gtnlv32r/2DPOGBwCIJ/AAAPEEHgAgnsADAMQTeACAeAIPABBP4AEA4g17eLbX2FdS3gGxeVdLdwdF6fxM9v3954/h798+fx3vv3ntbkl1l8zR+0xW7D431dvP7Lz2O19Xb/He8XvDAwDEE3gAgHgCDwAQT+ABAOIJPABAPIEHAIgn8AAA8Y7dw1Opuwdn866W8p6dmcr5mex7fu57rx1/qu5iqd5+pU+l+h4+etfLquqendH+qzuAKsf+Ec6Xl//uDQ8AEE/gAQDiCTwAQDyBBwCIJ/AAAPEEHgAgnsADAMSr7eGZdNl8//lj+Htr18vmPTgzy/0ixT1E0/F9/lp6/FLdHU78obqPZGer51b9/N197ruvnZX57+5/2vXa84YHAIgn8AAA8QQeACCewAMAxBN4AIB4Ag8AEE/gAQDina6DTpDH+6e9C0P0ndys1g6R8OvufHmenOBxzJ5h1ddR9/5Xjr17D013j9BM5/iO2pPzVrPx3X/6/cVnmDc8AEA8gQcAiCfwAADxBB4AIJ7AAwDEE3gAgHgCDwAQ72Fl4/Zv9Q/ed8L7tfZAuO5uxs49Oh9x/ErdHUbd269aOX51R9Lq9jNVc+sNDwAQT+ABAOIJPABAPIEHAIgn8AAA8QQeACCewAMAxFvq4enuWTiy7nPvPj4cQXWfSeXxV+/hI5/7W7bf/fidutd+Zja+8+Xlv3vDAwDEE3gAgHgCDwAQT+ABAOIJPABAPIEHAIgn8AAA8U7X6/XVHx/vn17/McHp9Ppvg3m5BckdE1Oj6+LuLv7aOF+eJxNwHJdfX4aLVd21svN9tPPY3qK7B2imc35X56b63FePP9v//affX3yGecMDAMQTeACAeAIPABBP4AEA4gk8AEA8gQcAiPfQPYCR8s/6Vj4vDv90ufqT1Onafv463v7nj/H2K+OfrN3q2I9+bdyS1c9jZ6o/e6/cd/XcHN2RP2tf1X3u58vLf/eGBwCIJ/AAAPEEHgAgnsADAMQTeACAeAIPABBP4AEA4tX28Cx21ezcM8Ca+dquXRtLPQ6T63Z17BzHkbtoqnt2Vu08dx9x/NX52/n/31HvC294AIB4Ag8AEE/gAQDiCTwAQDyBBwCIJ/AAAPEEHgAg3uk66MJ5/Me/x4Uikx4daLPYAXXLzpfnyeQdx+XXl60XerVrZdRnsnOPy91dfw/QTPX4Knt+ujuMVq3O7WvPMG94AIB4Ag8AEE/gAQDiCTwAQDyBBwCIJ/AAAPEEHgAg3sPwV10lbco7Krp7aqqPP9n+yP0lfJzZWq/eh7PtK/tSKntePuL4q3M30722M7fcwTRTdV94wwMAxBN4AIB4Ag8AEE/gAQDiCTwAQDyBBwCIJ/AAAPHGPTzpRl0wzR1E5T0K3R1LzT0/R++p4G2qu1RWVV6Hq+e22lPT3WNT3bOzcwfTTHXPW3mP3Dt5wwMAxBN4AIB4Ag8AEE/gAQDiCTwAQDyBBwCIJ/AAAPHGPTyTLpNpl8rq9tW6j0+dydqOeiJ09PBRurtodt33Dqp7cLp7frr2/RGqxucNDwAQT+ABAOIJPABAPIEHAIgn8AAA8QQeACCewAMAxDtdB30lj/dPWxfVrPYYDHuCdPSwqeXrfuJ8eZ4UaB3H5deXpRu5u0ens2ulevuZ7p6cmZ3XvvrY3ffFzGvPMG94AIB4Ag8AEE/gAQDiCTwAQDyBBwCIJ/AAAPEEHgAg3rCHp7rDgver7sDgdt1SD8/ufSIzo/F1Hvstx6/u2VnV3VVz5Pmr7jia7V8PDwBwswQeACCewAMAxBN4AIB4Ag8AEE/gAQDiCTwAQLyH0Y/d39Lrmnnd7ud+6LU7TWpoBt1V7GX3LpTKLpXu5++RO4zeYnV8ndt3d/h0Hd8bHgAgnsADAMQTeACAeAIPABBP4AEA4gk8AEA8gQcAiDfs4VnV3VNAn93XbtQD0d4PNekB+v7zR+3xebPquV65Fqt7cma6e3S6e+Sqraxf97UxUzW33vAAAPEEHgAgnsADAMQTeACAeAIPABBP4AEA4gk8AEC80h4eSFTev3G99h6fv03lWq72yHR3sawev3r83fMzWr/Z2FbHXt1VVtWB5A0PABBP4AEA4gk8AEA8gQcAiCfwAADxBB4AIJ7AAwDE6+3hOZ3Gv0/6SDpV9QTE2HxtV9bH2ufoXqvqPpTKY1fvv7onaHXtdz5+V8/NRx2/ijc8AEA8gQcAiCfwAADxBB4AIJ7AAwDEE3gAgHgCDwAQr7eHR8/OcW3eszOz0mHx7fPXyd73Pnf+dPT7fOU6rrbaxVK9Nqv7P3KPz+4dR1W84QEA4gk8AEA8gQcAiCfwAADxBB4AIJ7AAwDEE3gAgHi9PTwHdvT+jmUH7tm5u1tcn83Pnbdb7SNZvc87u1yqu1iOfvyZ7h6fkd3nbnX/7z0/b3gAgHgCDwAQT+ABAOIJPABAPIEHAIgn8AAA8QQeACDe6TroFHm8f1orHDmdxr/rM+ljbV516x1L58vz5OI4jtkzrHutV48/2r66Y+joXS/VVufnyD091WZz89ozzBseACCewAMAxBN4AIB4Ag8AEE/gAQDiCTwAQDyBBwCI9zD6cbmjYvcul1EXTfPYy/tBqs/vwD0/7f0fB547/prOrpXq63y1x6e6x6a7R2j1+Efv0hmpmjtveACAeAIPABBP4AEA4gk8AEA8gQcAiCfwAADxhp+lt3+eW23jz3sPP/eTue3+ZLbU6mfli9dl9Nz+RatzcfRPo3e2Ojer13H1fdB5H3bP7arV8Z0vL//dGx4AIJ7AAwDEE3gAgHgCDwAQT+ABAOIJPABAPIEHAIg37OEh2GpXzOL+l3seqse/ornfqbtDI0l3T8+K6o4f19nY6vysrF91/1T3ffHe/XvDAwDEE3gAgHgCDwAQT+ABAOIJPABAPIEHAIgn8AAA8YY9PKvf4s9U75+BSVfMzv0id3d37V03HEN138hMdRfOSHVXSrXu8XdfOzOV+9997LPtz5eX/+4NDwAQT+ABAOIJPABAPIEHAIgn8AAA8QQeACCewAMAxDtdB30mj/dPvWUnp9P49427WLo7GuKtXhuj7Te+rj7EZO7O//ttMrnHMXuGdXfRVHa9HP3cVvffbeeOp6OvzWx8959+f/EZ5g0PABBP4AEA4gk8AEA8gQcAiCfwAADxBB4AIJ7AAwDEexj+2t2Dc+A+lN07Ig5v9do48LW13O9x4HP/q1bnqruPZGV8nT0vb1E9vt3XdueuttW56752zpeX/+4NDwAQT+ABAOIJPABAPIEHAIgn8AAA8QQeACCewAMAxBv38HT3dXT2AK0eu7vDaHfm5910PP19qvtIKnt6Vo9d3WOzcw/NRxy/ev52PfZbdK2tNzwAQDyBBwCIJ/AAAPEEHgAgnsADAMQTeACAeAIPABBv3MPTrbPrZrUHRo/MWPH8rHR87N4PMqXj6A+rfSTda70yvuqx795T0719te5rc2TXHiBveACAeAIPABBP4AEA4gk8AEA8gQcAiCfwAADxBB4AIN7pOujkeLx/qi3smPWFzKz2iYyOf0NdJS/S5XKzzpfnxRtzH6vPsO4ulp27Vo7eYTRTPf7Oa2v13FbXvvraee0Z5g0PABBP4AEA4gk8AEA8gQcAiCfwAADxBB4AIJ7AAwDEG/bwAAAk8IYHAIgn8AAA8QQeACCewAMAxBN4AIB4Ag8AEO//OPanDjzPaPIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input has shape (1, 4, 32, 32, 64).\n",
      "Target has shape (1, 32, 32, 64).\n",
      "Training output has shape (1, 32, 32, 64).\n",
      "Loss is 0.3330758213996887.\n",
      "Backward pass works.\n",
      "Evaluation output has shape (1, 32, 32, 64).\n",
      "Score is 0.6676416397094727.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# a simple encoderdecoder-type model\n",
    "class Crush(nn.Module):\n",
    "    def __init__(self,D=32,S=64,C=4,crush_size=32):\n",
    "        super(Crush,self).__init__()\n",
    "        \n",
    "        self.dimensions = (C,D,D,S)\n",
    "        self.in_features = int(D*D*S*C)\n",
    "        self.crush = crush_size\n",
    "        self.out_features = int(D*D*S)\n",
    "        self.enc = nn.Linear(in_features=self.in_features,out_features=self.crush,bias=True)\n",
    "        self.act = nn.ReLU()\n",
    "        self.dec = nn.Linear(in_features=self.crush,out_features=self.out_features)\n",
    "        self.sig = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x_in):\n",
    "        batch_len = x_in.shape[0]\n",
    "        x = x_in.view(batch_len,-1)\n",
    "        x = self.enc(x)\n",
    "        x = self.act(x)\n",
    "        x = self.dec(x)\n",
    "        \n",
    "        dummy_dim = (-1,) + self.dimensions[1:]\n",
    "        x_out = x.view(dummy_dim)\n",
    "        return x_out\n",
    "\n",
    "model = Crush()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "simulator(model=model,criterion=nn.BCEWithLogitsLoss(),score_fun=iou_score,sigm=False)\n",
    "simulator(model=model,criterion=iou_module(),score_fun=iou_score)\n",
    "# for loss = nn.BCEWithLogitsLoss()\n",
    "# should be of the form loss(y_pred,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple Sequential CNN\n",
    "* 4 Convolution layers  \n",
    "![](extra/conv.gif)  \n",
    "Image from:\n",
    "https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md\n",
    "* ReLU activation\n",
    "* Batch normalization  \n",
    "![](extra/batchnorm.png)  \n",
    "Image from:\n",
    "https://medium.com/luminovo/a-refresher-on-batch-re-normalization-5e0a1e902960"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input has shape (1, 4, 32, 32, 64).\n",
      "Target has shape (1, 32, 32, 64).\n",
      "Training output has shape (1, 32, 32, 64).\n",
      "Loss is 0.3332260251045227.\n",
      "Backward pass works.\n",
      "Evaluation output has shape (1, 32, 32, 64).\n",
      "Score is 0.6645858287811279.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# a simple sequence of convolutional layers\n",
    "class ConvSeq(nn.Module):\n",
    "    def __init__(self,input_channels=4):\n",
    "        super(ConvSeq,self).__init__()\n",
    "        \n",
    "        self.input_channels = input_channels\n",
    "        \n",
    "        self.c1 = self.ConvLayer(in_channels=self.input_channels)\n",
    "        self.c2 = self.ConvLayer()\n",
    "        self.c3 = self.ConvLayer()\n",
    "        self.cfinal = self.ConvLayer(out_channels=1, relu=False)\n",
    "    \n",
    "    def forward(self, x_in):\n",
    "        x = self.c1(x_in)\n",
    "        x = self.c2(x)\n",
    "        x = self.c3(x)\n",
    "        x_out = self.cfinal(x).squeeze(1)\n",
    "                    \n",
    "        return x_out\n",
    "    \n",
    "    def ConvLayer(self, in_channels=32, out_channels=32, kernel_size=3, stride=1, padding=1, bias=True, relu=True):\n",
    "        layer = nn.Sequential()\n",
    "        conv = nn.Conv3d(in_channels=in_channels,\n",
    "                         out_channels=out_channels,\n",
    "                         kernel_size=kernel_size,\n",
    "                         padding=padding,\n",
    "                         bias=bias)\n",
    "        layer.add_module('conv',conv)\n",
    "        if relu:\n",
    "            layer.add_module('relu',nn.ReLU())\n",
    "\n",
    "        return layer\n",
    "\n",
    "model = ConvSeq()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "criterion = iou_module()\n",
    "simulator(model=model,criterion=criterion,score_fun=iou_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shallow 3d U-net\n",
    "* Original U-net paper https://arxiv.org/abs/1505.04597\n",
    "* The skip connection concatenates tensors channel-wise.\n",
    "* Downsampling is MaxPool.  \n",
    "![](extra/maxpool.webp)\n",
    "* Upsampling is nearest neighbor. https://pytorch.org/docs/master/nn.functional.html#interpolate  \n",
    "![](extra/upsample.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input has shape (1, 4, 32, 32, 64).\n",
      "Target has shape (1, 32, 32, 64).\n",
      "Training output has shape (1, 32, 32, 64).\n",
      "Loss is 0.33302950859069824.\n",
      "Backward pass works.\n",
      "Evaluation output has shape (1, 32, 32, 64).\n",
      "Score is 0.74537593126297.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# small 3d u-net with concatenating skip connection\n",
    "class Small3dUcat(nn.Module):\n",
    "    def __init__(self,input_channels=4,num_filters=32):\n",
    "        super(Small3dUcat,self).__init__()\n",
    "        \n",
    "        # Structure:\n",
    "        # Conv,Conv,MaxPool,Conv,Conv,UnPool,Conv,Conv\n",
    "        \n",
    "        self.input_channels = input_channels\n",
    "        self.num_filters = num_filters\n",
    "        \n",
    "        self.c1 = self.ConvLayer(in_channels=self.input_channels,out_channels=num_filters)\n",
    "        self.c2 = self.ConvLayer(in_channels=num_filters,out_channels=num_filters)\n",
    "\n",
    "        self.c3 = self.ConvLayer(in_channels=num_filters,out_channels=num_filters)\n",
    "        self.c4 = self.ConvLayer(in_channels=num_filters,out_channels=num_filters)\n",
    "        \n",
    "        self.c5 = self.ConvLayer(in_channels=2*num_filters,out_channels=num_filters)\n",
    "        self.c6 = self.ConvLayer(in_channels=num_filters,out_channels=num_filters)\n",
    "        self.c7 = self.ConvLayer(in_channels=num_filters,out_channels=1)\n",
    "    \n",
    "    def forward(self, x_in):\n",
    "        x1 = self.c1(x_in)\n",
    "        x1 = self.c2(x1)\n",
    "        \n",
    "        x2 = F.max_pool3d(x1,kernel_size=2)\n",
    "        x2 = self.c3(x2)\n",
    "        x2 = self.c4(x2)\n",
    "        \n",
    "        x2 = F.interpolate(x2, scale_factor=2)\n",
    "        # concatenate x1,x2\n",
    "        x = torch.cat([x1,x2],dim=1)\n",
    "        x = self.c5(x)\n",
    "        x = self.c6(x)\n",
    "        x = self.c7(x)\n",
    "        \n",
    "        x_out = x.squeeze(1)\n",
    "\n",
    "        return x_out\n",
    "    \n",
    "    def ConvLayer(self, in_channels=32, out_channels=32, kernel_size=3,\n",
    "                  stride=1, padding=1, bias=True, relu=True, batchnorm=True):\n",
    "        layer = nn.Sequential()\n",
    "        conv = nn.Conv3d(in_channels=in_channels,\n",
    "                         out_channels=out_channels,\n",
    "                         kernel_size=kernel_size,\n",
    "                         padding=padding,\n",
    "                         bias=bias)\n",
    "        layer.add_module('conv',conv)\n",
    "        if relu:\n",
    "            layer.add_module('relu',nn.ReLU())\n",
    "        if batchnorm:\n",
    "            layer.add_module('batchnorm',nn.BatchNorm3d(out_channels))\n",
    "\n",
    "        return layer\n",
    "\n",
    "model = Small3dUcat()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "criterion = iou_module()\n",
    "simulator(model=model,criterion=criterion,score_fun=iou_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shallow 3d U-net again\n",
    "* Same as above except skip connection adds tensors instead of concatenating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input has shape (1, 4, 32, 32, 64).\n",
      "Target has shape (1, 32, 32, 64).\n",
      "Training output has shape (1, 32, 32, 64).\n",
      "Loss is 0.31877589225769043.\n",
      "Backward pass works.\n",
      "Evaluation output has shape (1, 32, 32, 64).\n",
      "Score is 0.7521443963050842.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# small 3d u-net with addition skip connection\n",
    "class Small3dUadd(nn.Module):\n",
    "    def __init__(self,input_channels=4,num_filters=32):\n",
    "        super(Small3dUadd,self).__init__()\n",
    "        \n",
    "        # Structure:\n",
    "        # Conv,Conv,MaxPool,Conv,Conv,UnPool,Conv,Conv\n",
    "        \n",
    "        self.input_channels = input_channels\n",
    "        self.num_filters = num_filters\n",
    "        \n",
    "        self.c1 = self.ConvLayer(in_channels=self.input_channels,out_channels=num_filters)\n",
    "        self.c2 = self.ConvLayer(in_channels=num_filters,out_channels=num_filters)\n",
    "\n",
    "        self.c3 = self.ConvLayer(in_channels=num_filters,out_channels=num_filters)\n",
    "        self.c4 = self.ConvLayer(in_channels=num_filters,out_channels=num_filters)\n",
    "        \n",
    "        self.c5 = self.ConvLayer(in_channels=num_filters,out_channels=num_filters)\n",
    "        self.c6 = self.ConvLayer(in_channels=num_filters,out_channels=num_filters)\n",
    "        self.c7 = self.ConvLayer(in_channels=num_filters,out_channels=1)\n",
    "    \n",
    "    def forward(self, x_in, evaluating=False):\n",
    "        x1 = self.c1(x_in)\n",
    "        x1 = self.c2(x1)\n",
    "        \n",
    "        x2 = F.max_pool3d(x1,kernel_size=2)\n",
    "        x2 = self.c3(x2)\n",
    "        x2 = self.c4(x2)\n",
    "        \n",
    "        x2 = F.interpolate(x2, scale_factor=2)\n",
    "        # add x1,x2\n",
    "        x = F.relu(x1+x2)\n",
    "        x = self.c5(x)\n",
    "        x = self.c6(x)\n",
    "        x = self.c7(x)\n",
    "        \n",
    "        x_out = x.squeeze(1)\n",
    "\n",
    "        return x_out\n",
    "    \n",
    "    def ConvLayer(self, in_channels=32, out_channels=32, kernel_size=3,\n",
    "                  stride=1, padding=1, bias=True, relu=True, batchnorm=True):\n",
    "        layer = nn.Sequential()\n",
    "        conv = nn.Conv3d(in_channels=in_channels,\n",
    "                         out_channels=out_channels,\n",
    "                         kernel_size=kernel_size,\n",
    "                         padding=padding,\n",
    "                         bias=bias)\n",
    "        layer.add_module('conv',conv)\n",
    "        if relu:\n",
    "            layer.add_module('relu',nn.ReLU())\n",
    "        if batchnorm:\n",
    "            layer.add_module('batchnorm',nn.BatchNorm3d(out_channels))\n",
    "\n",
    "        return layer\n",
    "\n",
    "model = Small3dUadd()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "criterion = iou_module()\n",
    "simulator(model=model,criterion=criterion,score_fun=iou_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A proper U-net.\n",
    "![](extra/u.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input has shape (1, 4, 32, 32, 64).\n",
      "Target has shape (1, 32, 32, 64).\n",
      "Training output has shape (1, 32, 32, 64).\n",
      "Loss is 0.32622262835502625.\n",
      "Backward pass works.\n",
      "Evaluation output has shape (1, 32, 32, 64).\n",
      "Score is 0.714391827583313.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# a proper U-net\n",
    "# include picture of architecture\n",
    "# small 3d u-net with addition skip connection\n",
    "class Unet3d(nn.Module):\n",
    "    def __init__(self,input_channels=4,num_filters=64):\n",
    "        super(Unet3d,self).__init__()\n",
    "        \n",
    "        # Structure:\n",
    "        # cccmcccmcccmcccmcccucccucccucccucccf\n",
    "        \n",
    "        self.input_channels = input_channels\n",
    "        c = self.input_channels\n",
    "        self.num_filters = num_filters\n",
    "        n = self.num_filters\n",
    "        \n",
    "        # down\n",
    "        self.c1 = self.ConvLayer(in_channels=c,out_channels=n)\n",
    "        self.c2 = self.ConvLayer(in_channels=n,out_channels=n)\n",
    "        self.c3 = self.ConvLayer(in_channels=n,out_channels=n)\n",
    "\n",
    "        self.c4 = self.ConvLayer(in_channels=n,out_channels=2*n)\n",
    "        self.c5 = self.ConvLayer(in_channels=2*n,out_channels=2*n)\n",
    "        self.c6 = self.ConvLayer(in_channels=2*n,out_channels=2*n)\n",
    "\n",
    "        self.c7 = self.ConvLayer(in_channels=2*n,out_channels=4*n)\n",
    "        self.c8 = self.ConvLayer(in_channels=4*n,out_channels=4*n)\n",
    "        self.c9 = self.ConvLayer(in_channels=4*n,out_channels=4*n)\n",
    "\n",
    "        self.c10 = self.ConvLayer(in_channels=4*n,out_channels=8*n)\n",
    "        self.c11 = self.ConvLayer(in_channels=8*n,out_channels=8*n)\n",
    "        self.c12 = self.ConvLayer(in_channels=8*n,out_channels=8*n)\n",
    "\n",
    "        self.c13 = self.ConvLayer(in_channels=8*n,out_channels=16*n)\n",
    "        self.c14 = self.ConvLayer(in_channels=16*n,out_channels=16*n)\n",
    "        self.c15 = self.ConvLayer(in_channels=16*n,out_channels=16*n)\n",
    "        \n",
    "        # up\n",
    "        self.c16 = self.ConvLayer(in_channels=16*n+8*n,out_channels=8*n)\n",
    "        self.c17 = self.ConvLayer(in_channels=8*n,out_channels=8*n)\n",
    "        self.c18 = self.ConvLayer(in_channels=8*n,out_channels=8*n)\n",
    "\n",
    "        \n",
    "        self.c19 = self.ConvLayer(in_channels=8*n+4*n,out_channels=4*n)\n",
    "        self.c20 = self.ConvLayer(in_channels=4*n,out_channels=4*n)\n",
    "        self.c21 = self.ConvLayer(in_channels=4*n,out_channels=4*n)\n",
    "\n",
    "        \n",
    "        self.c22 = self.ConvLayer(in_channels=4*n+2*n,out_channels=2*n)\n",
    "        self.c23 = self.ConvLayer(in_channels=2*n,out_channels=2*n)\n",
    "        self.c24 = self.ConvLayer(in_channels=2*n,out_channels=2*n)\n",
    "\n",
    "        self.c25 = self.ConvLayer(in_channels=2*n+n,out_channels=n)\n",
    "        self.c26 = self.ConvLayer(in_channels=n,out_channels=n)\n",
    "        self.c27 = self.ConvLayer(in_channels=n,out_channels=n)\n",
    "        \n",
    "        self.f = self.ConvLayer(in_channels=n,out_channels=1,\n",
    "                                kernel_size=1,padding=0)\n",
    "\n",
    "    def forward(self, x_in, evaluating=False):\n",
    "        x = self.c1(x_in)\n",
    "        x = self.c2(x)\n",
    "        x3 = self.c3(x)\n",
    "        \n",
    "        x = F.max_pool3d(x3,kernel_size=2)\n",
    "        x = self.c4(x)\n",
    "        x = self.c5(x)\n",
    "        x6 = self.c6(x)\n",
    "        \n",
    "        x = F.max_pool3d(x6,kernel_size=2)\n",
    "        x = self.c7(x)\n",
    "        x = self.c8(x)\n",
    "        x9 = self.c9(x)\n",
    "        \n",
    "        x = F.max_pool3d(x9,kernel_size=2)\n",
    "        x = self.c10(x)\n",
    "        x = self.c11(x)\n",
    "        x12 = self.c12(x)\n",
    "        \n",
    "        x = F.max_pool3d(x12,kernel_size=2)\n",
    "        x = self.c13(x)\n",
    "        x = self.c14(x)\n",
    "        x = self.c15(x)\n",
    "        \n",
    "        x = F.interpolate(x, scale_factor=2)\n",
    "        x = torch.cat([x,x12],dim=1)\n",
    "        x = self.c16(x)\n",
    "        x = self.c17(x)\n",
    "        x = self.c18(x)\n",
    "        \n",
    "        x = F.interpolate(x,scale_factor=2)\n",
    "        x = torch.cat([x,x9],dim=1)\n",
    "        x = self.c19(x)\n",
    "        x = self.c20(x)\n",
    "        x = self.c21(x)\n",
    "        \n",
    "        x = F.interpolate(x,scale_factor=2)\n",
    "        x = torch.cat([x,x6],dim=1)\n",
    "        x = self.c22(x)\n",
    "        x = self.c23(x)\n",
    "        x = self.c24(x)\n",
    "        \n",
    "        x = F.interpolate(x,scale_factor=2)\n",
    "        x = torch.cat([x,x3],dim=1)\n",
    "        x = self.c25(x)\n",
    "        x = self.c26(x)\n",
    "        x = self.c27(x)\n",
    "        \n",
    "        x = self.f(x)\n",
    "        x_out = x.squeeze(1)\n",
    "\n",
    "        return x_out\n",
    "            \n",
    "    def ConvLayer(self, in_channels=32, out_channels=32, kernel_size=3,\n",
    "                  stride=1, padding=1, bias=True, relu=True, batchnorm=True):\n",
    "        layer = nn.Sequential()\n",
    "        conv = nn.Conv3d(in_channels=in_channels,\n",
    "                         out_channels=out_channels,\n",
    "                         kernel_size=kernel_size,\n",
    "                         padding=padding,\n",
    "                         bias=bias)\n",
    "        layer.add_module('conv',conv)\n",
    "        if relu:\n",
    "            layer.add_module('relu',nn.ReLU())\n",
    "        if batchnorm:\n",
    "            layer.add_module('batchnorm',nn.BatchNorm3d(out_channels))\n",
    "\n",
    "        return layer\n",
    "\n",
    "model = Unet3d()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "criterion = iou_module()\n",
    "simulator(model=model,criterion=criterion,score_fun=iou_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Double U-net  \n",
    "![](extra/uu.png)  \n",
    "Taken from https://arxiv.org/pdf/1701.03056.pdf (with minor differences)  \n",
    "![](extra/myuu.png)\n",
    "* Downsampling is Conv with 2x2 kernel, stride=2.\n",
    "* Has PReLU activations.\n",
    "* Upsampling is now the transpose of downsampling.  \n",
    "![](extra/tconv.gif)\n",
    "* Uses 1d convolutions to reduce channels.   \n",
    "![](extra/1dconv.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input has shape (1, 4, 32, 32, 64).\n",
      "Target has shape (1, 32, 32, 64).\n",
      "Training output has shape (1, 32, 32, 64).\n",
      "Loss is 0.4615045487880707.\n",
      "Backward pass works.\n",
      "Evaluation output has shape (1, 32, 32, 64).\n",
      "Score is 0.5009918212890625.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 3d UU-net\n",
    "class Big3dU(nn.Module):\n",
    "    def __init__(self,input_channels=4):\n",
    "        super(Big3dU,self).__init__()\n",
    "        \n",
    "        self.c0 = self.ConvLayer(in_channels=4,out_channels=8,\n",
    "                                 kernel_size=3,stride=1,padding=1)\n",
    "\n",
    "        self.d1 = self.ConvLayer(in_channels=8,out_channels=16,\n",
    "                                 stride=2,kernel_size=2,padding=0)\n",
    "        self.c1 = self.ConvLayer(in_channels=16,out_channels=16,\n",
    "                                kernel_size=3,stride=1,padding=1)\n",
    "\n",
    "        self.d2 = self.ConvLayer(in_channels=16,out_channels=32,\n",
    "                                 stride=2,kernel_size=2,padding=0)\n",
    "        self.c2 = self.ConvLayer(in_channels=32,out_channels=32,\n",
    "                                kernel_size=3,stride=1,padding=1)\n",
    "\n",
    "        self.d3 = self.ConvLayer(in_channels=32,out_channels=64,\n",
    "                                 stride=2,kernel_size=2,padding=0)\n",
    "        self.c3 = self.ConvLayer(in_channels=64,out_channels=64,\n",
    "                                kernel_size=3,stride=1,padding=1)\n",
    "        \n",
    "        self.f1 = self.ConvLayer(in_channels=64,out_channels=32,\n",
    "                                kernel_size=1,stride=1,padding=0)\n",
    "        \n",
    "        self.u1 = self.ConvLayer(in_channels=32,out_channels=32,\n",
    "                                kernel_size=2,stride=2,padding=0,transpose=True)\n",
    "        self.c4 = self.ConvLayer(in_channels=64,out_channels=32,\n",
    "                                 kernel_size=3,stride=1,padding=1)\n",
    "        self.f2 = self.ConvLayer(in_channels=32,out_channels=16,\n",
    "                                kernel_size=1,stride=1,padding=0)\n",
    "        \n",
    "        self.u2 = self.ConvLayer(in_channels=16,out_channels=16,\n",
    "                                kernel_size=2,stride=2,padding=0,transpose=True)\n",
    "        self.c5 = self.ConvLayer(in_channels=32,out_channels=16,\n",
    "                                kernel_size=3,stride=1,padding=1)\n",
    "        self.f3 = self.ConvLayer(in_channels=16,out_channels=8,\n",
    "                                kernel_size=1,stride=1,padding=0)\n",
    "        \n",
    "        self.u3 = self.ConvLayer(in_channels=8,out_channels=8,\n",
    "                                kernel_size=2,stride=2,padding=0,transpose=True)\n",
    "        self.c6 = self.ConvLayer(in_channels=16,out_channels=16,\n",
    "                                kernel_size=3,stride=1,padding=1)\n",
    "        \n",
    "        self.f4 = self.ConvLayer(in_channels=32,out_channels=16,\n",
    "                                kernel_size=1,stride=1,padding=0)\n",
    "        self.f5 = self.ConvLayer(in_channels=16,out_channels=8,\n",
    "                                kernel_size=1,stride=1,padding=0)\n",
    "        self.f6 = self.ConvLayer(in_channels=16,out_channels=1,\n",
    "                                kernel_size=1,stride=1,padding=0)\n",
    "        \n",
    "        self.u4 = self.ConvLayer(in_channels=16,out_channels=8,\n",
    "                                kernel_size=2,stride=2,padding=0,transpose=True)\n",
    "        self.u5 = self.ConvLayer(in_channels=8,out_channels=1,\n",
    "                                kernel_size=2,stride=2,padding=0,transpose=True)\n",
    "        \n",
    "        self.act = nn.PReLU(num_parameters=1)  \n",
    "\n",
    "    def forward(self, x_in):\n",
    "        x0 = self.c0(x_in)\n",
    "        x1 = self.d1(x0)\n",
    "        x2 = self.c1(x1)\n",
    "        x3 = self.d2(x2+x1)\n",
    "        x4 = self.c2(x3)\n",
    "        x5 = self.d3(x4+x3)\n",
    "        x6 = self.c3(x5)\n",
    "        x7 = self.f1(x6+x5)\n",
    "        x8 = self.u1(x7)\n",
    "        x9 = self.c4(torch.cat([x8,x4],dim=1))\n",
    "        x10 = self.f2(x9)\n",
    "        x11 = self.u2(x10)\n",
    "        x12 = self.c5(torch.cat([x11,x2],dim=1))\n",
    "        x13 = self.f3(x12)\n",
    "        x14 = self.u3(x13)\n",
    "        x15 = self.f4(x9)\n",
    "        x16 = self.u4(x15)\n",
    "        x17 = self.f5(x12)\n",
    "        # x18 is missing, the graph I sketched had a box I did not use\n",
    "        x19 = self.u5(x16+x17)\n",
    "        x20 = self.c6(torch.cat([x14,x0],dim=1))\n",
    "        x21 = self.f6(x20)\n",
    "        x22 = self.act(x21+x19)\n",
    "\n",
    "        x_out = x22.squeeze(1)\n",
    "        x_out = torch.sigmoid(x_out)\n",
    "\n",
    "        return x_out\n",
    "\n",
    "    def ConvLayer(self,in_channels,out_channels,kernel_size,\n",
    "                  stride,padding,dilation=1,bias=True,prelu=True,batchnorm=True,transpose=False):\n",
    "        layer = nn.Sequential()\n",
    "        if transpose:\n",
    "            tconv = nn.ConvTranspose3d(in_channels=in_channels,\n",
    "                             out_channels=out_channels,\n",
    "                             kernel_size=kernel_size,\n",
    "                             stride=stride,\n",
    "                             padding=padding,\n",
    "                             dilation=dilation,\n",
    "                             bias=bias)\n",
    "            layer.add_module('tconv',tconv)\n",
    "        else:\n",
    "            conv = nn.Conv3d(in_channels=in_channels,\n",
    "                             out_channels=out_channels,\n",
    "                             kernel_size=kernel_size,\n",
    "                             stride=stride,\n",
    "                             padding=padding,\n",
    "                             dilation=dilation,\n",
    "                             bias=bias)\n",
    "            layer.add_module('conv',conv)\n",
    "\n",
    "        if batchnorm:\n",
    "            layer.add_module('batchnorm',nn.BatchNorm3d(num_features=out_channels))\n",
    "\n",
    "        if prelu:\n",
    "            layer.add_module('prelu',nn.PReLU(num_parameters=out_channels))\n",
    "\n",
    "        return layer\n",
    "\n",
    "model = Big3dU()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "criterion = iou_module()\n",
    "simulator(model=model,criterion=criterion,score_fun=iou_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
